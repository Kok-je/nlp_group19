{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciCite Distillation Process with Gemma-3-12b\n",
    "\n",
    "**Objective**: Fine-tune Gemma-3-12b-it using reasoning-enhanced data from teacher models (Llama-3.3/Gemma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets accelerate peft bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Augmented Dataset\n",
    "\n",
    "We are using the partitioned dataset from the Teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shawn Kok\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partition(partition_path: str) -> Dataset:\n",
    "    df = pd.read_csv(partition_path)\n",
    "    return Dataset.from_pandas(df[[\"id\", \"model_classification\", \"reasoning\"]])\n",
    "\n",
    "# Replace with dataset path\n",
    "train_dataset = load_partition(\"./results/Gemma2_27b/first_partition.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Model and Tokenizer Steup\n",
    "We are using Hugging Face API with Gemma-3-12b-it as the student model. The tokenizer is also from the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM, TrainerCallback\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade transformers\n",
    "%pip install --upgrade torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "hf_token = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "model_id = \"google/mobilebert-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token,\n",
    "    num_labels=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Format data with our Teacher reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1365 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1365/1365 [00:02<00:00, 496.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "label_map = {\"background\": 0, \"method\": 1, \"result\": 2}\n",
    "\n",
    "def format_for_distillation(examples):\n",
    "    tokenized_text = tokenizer(\n",
    "        f\"Text: {examples['id']}\\nTeacher Reasoning: {examples['reasoning']}\\nClassification:\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # # Create a labels tensor of the same length as input_ids\n",
    "    # labels = [label_map[examples[\"model_classification\"]]] * len(tokenized_text[\"input_ids\"])\n",
    "\n",
    "    # Create a single label for the entire sequence\n",
    "    labels = label_map[examples[\"model_classification\"]]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized_text[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_text[\"attention_mask\"],\n",
    "        \"labels\": labels  # Now same shape as input_ids / Now a single scalar value\n",
    "    }\n",
    "\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    format_for_distillation,\n",
    "    remove_columns=['id', 'model_classification', 'reasoning']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Distillation Trainer\n",
    "Aligns student model with teacher reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "class ReasoningDistiller(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.step_counter = 0\n",
    "        self.start_time = time.time()\n",
    "        self.total_steps = 0\n",
    "        self.batch_size = kwargs.get('args').per_device_train_batch_size\n",
    "        self.epochs = kwargs.get('args').num_train_epochs\n",
    "        \n",
    "        # Calculate total steps\n",
    "        if hasattr(kwargs.get('train_dataset'), '__len__'):\n",
    "            dataset_size = len(kwargs.get('train_dataset'))\n",
    "            grad_accum = kwargs.get('args').gradient_accumulation_steps\n",
    "            self.total_steps = (dataset_size // (self.batch_size * grad_accum)) * self.epochs\n",
    "            print(f\"\\n===== TRAINING INFO =====\")\n",
    "            print(f\"Dataset size: {dataset_size} examples\")\n",
    "            print(f\"Batch size: {self.batch_size}\")\n",
    "            print(f\"Gradient accumulation steps: {grad_accum}\")\n",
    "            print(f\"Epochs: {self.epochs}\")\n",
    "            print(f\"Estimated total steps: {self.total_steps}\")\n",
    "            print(f\"========================\\n\")\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # print(\"compute_loss called!\")\n",
    "        # print(f\"Input keys: {inputs.keys()}\")\n",
    "        # print(\"Model config:\", model.config)\n",
    "        # for key, value in inputs.items():\n",
    "        #     print(f\"{key}: {value.shape} {value.dtype}\")\n",
    "\n",
    "        # step = self.state.global_step if hasattr(self, 'state') else 0\n",
    "        # verbose = (step % 10 == 0)\n",
    "        # if verbose:\n",
    "        #     print(f\"\\nStep {step}: Computing loss...\")\n",
    "        #     print(f\"Input keys: {inputs.keys()}\")\n",
    "\n",
    "        self.step_counter += 1\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        if self.total_steps > 0:\n",
    "            progress = (self.step_counter / self.total_steps) * 100\n",
    "            \n",
    "            # Only print status every 10 steps to avoid cluttering\n",
    "            if self.step_counter % 10 == 0 or self.step_counter == 1:\n",
    "                # Calculate time estimates\n",
    "                if self.step_counter > 1:\n",
    "                    avg_time_per_step = elapsed_time / self.step_counter\n",
    "                    remaining_steps = self.total_steps - self.step_counter\n",
    "                    remaining_time = avg_time_per_step * remaining_steps\n",
    "                    \n",
    "                    # Format time remaining\n",
    "                    mins_remaining = int(remaining_time // 60)\n",
    "                    hrs_remaining = mins_remaining // 60\n",
    "                    mins_remaining = mins_remaining % 60\n",
    "                    \n",
    "                    elapsed_mins = int(elapsed_time // 60)\n",
    "                    elapsed_hrs = elapsed_mins // 60\n",
    "                    elapsed_mins = elapsed_mins % 60\n",
    "                    \n",
    "                    time_info = f\"Elapsed: {elapsed_hrs}h {elapsed_mins}m | Remaining: {hrs_remaining}h {mins_remaining}m\"\n",
    "                else:\n",
    "                    time_info = \"Calculating time remaining...\"\n",
    "                \n",
    "                print(f\"\\nStep {self.step_counter}/{self.total_steps} ({progress:.2f}% complete)\")\n",
    "                print(f\"{time_info}\")\n",
    "\n",
    "\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        # Extract logits\n",
    "        logits = outputs.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "        labels = inputs[\"labels\"]  # Shape: (batch_size, sequence_length)\n",
    "        \n",
    "        # Flatten logits and labels for CrossEntropyLoss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        # Get hidden states for distillation\n",
    "        # student_hidden = outputs.hidden_states[-1].mean(dim=1)  # Shape: (batch_size, hidden_dim)\n",
    "        student_hidden = outputs.hidden_states[-1][:, 0, :]  # Use [CLS] token representation\n",
    "        teacher_hidden = torch.randn_like(student_hidden)  \n",
    "\n",
    "        # Alignment loss\n",
    "        reasoning_loss = torch.nn.functional.mse_loss(student_hidden, teacher_hidden)\n",
    "\n",
    "        # Combined loss\n",
    "        total_loss = loss + 0.3 * reasoning_loss\n",
    "\n",
    "        if self.step_counter % 10 == 0 or self.step_counter == 1:\n",
    "            print(f\"Classification Loss: {loss.item():.4f} | Reasoning Loss: {reasoning_loss.item():.4f}\")\n",
    "            print(f\"Total Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "For 24FB GPUs as per documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gemma3-distilled\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING INFO =====\n",
      "Dataset size: 1365 examples\n",
      "Batch size: 4\n",
      "Gradient accumulation steps: 4\n",
      "Epochs: 1\n",
      "Estimated total steps: 85\n",
      "========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(\n",
    "#         examples[\"text\"],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=512,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "\n",
    "# tokenized_train = tokenized_dataset.map(\n",
    "#     tokenize_function,\n",
    "#     batched=True,\n",
    "#     batch_size=32\n",
    "# )\n",
    "\n",
    "trainer = ReasoningDistiller(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Input Shape: torch.Size([4, 512])\n",
      "Batch Labels Shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for batch in trainer.get_train_dataloader():\n",
    "    print(f\"Batch Input Shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"Batch Labels Shape: {batch['labels'].shape}\")\n",
    "    break  # Only print the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1/85 (1.18% complete)\n",
      "Calculating time remaining...\n",
      "Classification Loss: nan | Reasoning Loss: nan\n",
      "Total Loss: nan\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3/85 10:54 < 14:54:11, 0.00 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 10/85 (11.76% complete)\n",
      "Elapsed: 0h 23m | Remaining: 2h 57m\n",
      "Classification Loss: nan | Reasoning Loss: nan\n",
      "Total Loss: nan\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gemma3-distilled-scicite\")\n",
    "tokenizer.save_pretrained(\"gemma3-distilled-scicite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
