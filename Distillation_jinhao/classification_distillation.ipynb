{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference LLM Distillation notebook: https://github.com/simranjeet97/LLM_Distillation/blob/main/LLM_Distillation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 samples from dataset.\n"
     ]
    }
   ],
   "source": [
    "# ====== Load dataset ======\n",
    "def load_partition(path: str) -> Dataset:\n",
    "    df = pd.read_csv(path).head(1000)\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "dataset = load_partition(\"../Student_Training_Data/GPT.csv\") ## should be GPT.csv\n",
    "print(f\"Loaded {len(dataset)} samples from dataset.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ====== Tokenizer & Model Setup ======\n",
    "model_id = \"google-bert/bert-base-uncased\" #\"google/gemma-3-1b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google-bert/bert-base-uncased\",\n",
    "    num_labels=3  # background, method, result\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     token=hf_token,\n",
    "#     trust_remote_code=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# lora_config = LoraConfig(\n",
    "#     r=8,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=TaskType.CAUSAL_LM\n",
    "# )\n",
    "# model = get_peft_model(model, lora_config) # TODO Why getting PEFT model? Paper and Reference notebook did not use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2136.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ====== Data Formatting Fix ======\n",
    "def format_for_distillation(examples):\n",
    "    label_map = {\"background\": 0, \"method\": 1, \"result\": 2}\n",
    "    \n",
    "    # Process text inputs\n",
    "    texts = [\n",
    "        f\"Classify the following scientific text as one of [background, method, result].:\\nSection Name: {s}\\nText: {t}\\nClassification:\"\n",
    "        for s, t in zip(examples[\"sectionName\"], examples[\"string\"])\n",
    "    ]\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Convert labels\n",
    "    labels = torch.tensor([label_map[c] for c in examples[\"model_classification\"]])\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "        # \"rationale_ids\": tokenizer(\n",
    "        #     examples[\"reasoning\"],\n",
    "        #     padding=\"max_length\",\n",
    "        #     truncation=True,\n",
    "        #     max_length=512,\n",
    "        #     return_tensors=\"pt\"\n",
    "        # )[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# ====== Training Setup ======\n",
    "tokenized_dataset = dataset.map(\n",
    "    format_for_distillation,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Training Args ======\n",
    "training_args = TrainingArguments( ## Original Training Args\n",
    "    output_dir=\"gemma3-phase1\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    max_grad_norm=1.0,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "#### Training args in phase 1 distillation before edits\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     learning_rate=2e-5,\n",
    "#     max_steps=10,  \n",
    "#     logging_steps=1,\n",
    "#     save_strategy=\"no\",\n",
    "#     remove_unused_columns=False,\n",
    "#     max_grad_norm=1.0,\n",
    "#     report_to=\"none\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, **kwargs):\n",
    "        \n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # rationale_ids = inputs.pop(\"rationale_ids\", None)\n",
    "        # print(f\"Labels: {labels} | Rationale IDs: {rationale_ids}\")\n",
    "        \n",
    "        # outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"]\n",
    "        )\n",
    "        # print(f\"Outputs: {outputs}\")\n",
    "        # print(f\"logits: {outputs.logits}\")\n",
    "\n",
    "        # Reshape logits to [batch_size, num_classes]\n",
    "        # logits = outputs.logits[:, -1, :]  # Take last token's logits\n",
    "        # logits = logits[:, :3]  # Only take logits for the 3 classes\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        label_loss = loss_fn(outputs.logits, labels)\n",
    "        \n",
    "        # if rationale_ids is not None:\n",
    "        #     rationale_outputs = model(input_ids=rationale_ids, attention_mask=inputs[\"attention_mask\"])\n",
    "        #     rationale_loss = loss_fn(rationale_outputs.logits, rationale_ids)\n",
    "        #     loss = label_loss + 0.5 * rationale_loss  # Weighted loss\n",
    "        # else:\n",
    "        #     loss = label_loss\n",
    "        \n",
    "        return (label_loss, outputs) if return_outputs else label_loss\n",
    "\n",
    "trainer = MultiTaskTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.1419, -0.7163,  0.3923],\n",
      "        [-0.1831, -0.4173,  0.2911]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.1419, -0.7163,  0.3923],\n",
      "        [-0.1831, -0.4173,  0.2911]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 1:42:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.782900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.422600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.263800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.517900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.593200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.622100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.570600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.594300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.878400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.065500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.914200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.266700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.640300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.313200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.397300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.741100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.498700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.801000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.242900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.577100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.772800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.531400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.265700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.810500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.728300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.929100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.668800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.253300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.858100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.553000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.531900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.681300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.486200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.578400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.803100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.867900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.732200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.139400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.813100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.921400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.695100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.946900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.591800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.014900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.825600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.260300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.323100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.667500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.268300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.637800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.895300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.699700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.738100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.227400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.456700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.424300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.768700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.258500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.508800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.340900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.264400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.925400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.428200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.411100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.383300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.078300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.353900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.382400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.358200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.361900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.335100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.778300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.391200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0284, -0.7294,  0.0150],\n",
      "        [ 0.0969, -0.6257, -0.4368]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.0284, -0.7294,  0.0150],\n",
      "        [ 0.0969, -0.6257, -0.4368]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2192, -0.5671,  0.0377],\n",
      "        [ 0.4241, -0.6867,  0.0030]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.2192, -0.5671,  0.0377],\n",
      "        [ 0.4241, -0.6867,  0.0030]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2257, -0.3790, -0.2835],\n",
      "        [ 0.6466, -0.8887, -0.2860]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.2257, -0.3790, -0.2835],\n",
      "        [ 0.6466, -0.8887, -0.2860]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.7138, -0.3426, -0.6297],\n",
      "        [ 0.4826, -0.7535, -0.2861]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.7138, -0.3426, -0.6297],\n",
      "        [ 0.4826, -0.7535, -0.2861]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2602, -0.5801, -0.3474],\n",
      "        [ 0.7294, -0.4878, -0.8524]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.2602, -0.5801, -0.3474],\n",
      "        [ 0.7294, -0.4878, -0.8524]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.8951, -0.1659, -0.6157],\n",
      "        [ 0.6000, -0.4330, -0.8945]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.8951, -0.1659, -0.6157],\n",
      "        [ 0.6000, -0.4330, -0.8945]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6116, -0.3118, -0.5312],\n",
      "        [ 0.6046, -0.4864, -0.8000]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.6116, -0.3118, -0.5312],\n",
      "        [ 0.6046, -0.4864, -0.8000]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.7929, -0.1512, -0.8066],\n",
      "        [ 0.6970, -0.3714, -1.1175]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.7929, -0.1512, -0.8066],\n",
      "        [ 0.6970, -0.3714, -1.1175]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.8371, -0.3191, -1.3739],\n",
      "        [ 1.2468, -0.6387, -1.1926]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.8371, -0.3191, -1.3739],\n",
      "        [ 1.2468, -0.6387, -1.1926]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.8615, -0.4434, -1.0828],\n",
      "        [ 0.4765, -0.2021, -1.1459]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.8615, -0.4434, -1.0828],\n",
      "        [ 0.4765, -0.2021, -1.1459]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.0296, -0.3028, -1.1449],\n",
      "        [ 1.0007, -0.1473, -1.1662]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.0296, -0.3028, -1.1449],\n",
      "        [ 1.0007, -0.1473, -1.1662]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3937, -0.6840, -0.9706],\n",
      "        [ 1.1468, -0.2346, -1.3714]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.3937, -0.6840, -0.9706],\n",
      "        [ 1.1468, -0.2346, -1.3714]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.3243, -0.1745, -1.4465],\n",
      "        [ 0.5503, -0.4259, -1.4106]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.3243, -0.1745, -1.4465],\n",
      "        [ 0.5503, -0.4259, -1.4106]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.2162, -0.4634, -0.7370],\n",
      "        [ 1.2327, -0.4919, -1.1259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.2162, -0.4634, -0.7370],\n",
      "        [ 1.2327, -0.4919, -1.1259]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.3346, -0.2586, -1.0623],\n",
      "        [ 1.0786, -0.5442, -0.7055]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.3346, -0.2586, -1.0623],\n",
      "        [ 1.0786, -0.5442, -0.7055]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.2278, -0.4620, -0.8823],\n",
      "        [ 1.2160, -0.5144, -0.9390]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.2278, -0.4620, -0.8823],\n",
      "        [ 1.2160, -0.5144, -0.9390]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.3504, -0.6624, -1.0061],\n",
      "        [ 1.2749, -0.7013, -0.8338]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.3504, -0.6624, -1.0061],\n",
      "        [ 1.2749, -0.7013, -0.8338]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.0303, -0.8123, -0.5601],\n",
      "        [ 1.3357, -0.9589, -1.3375]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.0303, -0.8123, -0.5601],\n",
      "        [ 1.3357, -0.9589, -1.3375]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4281, -0.8711, -1.4686],\n",
      "        [ 1.4482, -0.9387, -1.4003]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.4281, -0.8711, -1.4686],\n",
      "        [ 1.4482, -0.9387, -1.4003]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4686, -0.8718, -1.4276],\n",
      "        [ 1.1945, -0.8653, -1.3883]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.4686, -0.8718, -1.4276],\n",
      "        [ 1.1945, -0.8653, -1.3883]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.8234, -0.8816, -1.1107],\n",
      "        [ 1.3252, -0.8525, -1.4241]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.8234, -0.8816, -1.1107],\n",
      "        [ 1.3252, -0.8525, -1.4241]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.3954, -1.1039, -1.1773],\n",
      "        [ 1.4078, -1.2284, -0.9811]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.3954, -1.1039, -1.1773],\n",
      "        [ 1.4078, -1.2284, -0.9811]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4176, -1.1992, -1.1700],\n",
      "        [ 1.5204, -1.0076, -1.3394]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.4176, -1.1992, -1.1700],\n",
      "        [ 1.5204, -1.0076, -1.3394]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4764, -1.1312, -0.9086],\n",
      "        [ 1.5942, -1.1019, -1.4075]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.4764, -1.1312, -0.9086],\n",
      "        [ 1.5942, -1.1019, -1.4075]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7731, -1.0981, -1.3259],\n",
      "        [ 1.7651, -1.0372, -1.4860]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.7731, -1.0981, -1.3259],\n",
      "        [ 1.7651, -1.0372, -1.4860]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6857, -1.1053, -1.1153],\n",
      "        [ 1.8020, -1.3560, -1.2550]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.6857, -1.1053, -1.1153],\n",
      "        [ 1.8020, -1.3560, -1.2550]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6587, -1.4854, -1.3775],\n",
      "        [ 1.8570, -1.4389, -1.4520]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.6587, -1.4854, -1.3775],\n",
      "        [ 1.8570, -1.4389, -1.4520]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0587, -1.3102, -1.6589],\n",
      "        [ 1.8171, -1.5245, -1.1183]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.0587, -1.3102, -1.6589],\n",
      "        [ 1.8171, -1.5245, -1.1183]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.8319, -1.6937, -1.4961],\n",
      "        [ 1.5605, -1.3547, -1.1526]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.8319, -1.6937, -1.4961],\n",
      "        [ 1.5605, -1.3547, -1.1526]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5273, -1.7113, -1.3215],\n",
      "        [ 1.5170, -1.6133, -1.3026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.5273, -1.7113, -1.3215],\n",
      "        [ 1.5170, -1.6133, -1.3026]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4402, -1.7986, -1.3231],\n",
      "        [ 1.3323, -1.5524, -1.2523]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.4402, -1.7986, -1.3231],\n",
      "        [ 1.3323, -1.5524, -1.2523]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.9262, -1.7529, -1.4980],\n",
      "        [ 1.6086, -1.6687, -1.5739]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.9262, -1.7529, -1.4980],\n",
      "        [ 1.6086, -1.6687, -1.5739]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.3890, -1.5403, -0.9790],\n",
      "        [ 1.4919, -1.5146, -1.3293]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.3890, -1.5403, -0.9790],\n",
      "        [ 1.4919, -1.5146, -1.3293]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.2817, -1.7598, -1.0159],\n",
      "        [ 1.2192, -1.5702, -0.9720]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.2817, -1.7598, -1.0159],\n",
      "        [ 1.2192, -1.5702, -0.9720]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.2488, -0.9452, -1.1298],\n",
      "        [ 1.3331, -1.3048, -1.2496]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.2488, -0.9452, -1.1298],\n",
      "        [ 1.3331, -1.3048, -1.2496]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6068, -1.6102, -1.5667],\n",
      "        [ 1.3455, -1.5366, -1.3554]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.6068, -1.6102, -1.5667],\n",
      "        [ 1.3455, -1.5366, -1.3554]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.8013, -1.7010, -1.3870],\n",
      "        [ 1.7014, -1.7202, -1.3459]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.8013, -1.7010, -1.3870],\n",
      "        [ 1.7014, -1.7202, -1.3459]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0656, -1.4822, -1.4247],\n",
      "        [ 2.0163, -1.5988, -1.7478]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.0656, -1.4822, -1.4247],\n",
      "        [ 2.0163, -1.5988, -1.7478]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7142, -1.5988, -1.6198],\n",
      "        [ 2.3477, -1.5865, -1.8042]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.7142, -1.5988, -1.6198],\n",
      "        [ 2.3477, -1.5865, -1.8042]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2927, -1.7220, -1.8674],\n",
      "        [ 2.4664, -1.5024, -1.7604]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.2927, -1.7220, -1.8674],\n",
      "        [ 2.4664, -1.5024, -1.7604]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.9205, -1.5530, -1.3090],\n",
      "        [ 2.4848, -1.9511, -1.9009]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.9205, -1.5530, -1.3090],\n",
      "        [ 2.4848, -1.9511, -1.9009]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4041, -1.9166, -1.5143],\n",
      "        [ 2.3940, -1.3432, -1.3732]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.4041, -1.9166, -1.5143],\n",
      "        [ 2.3940, -1.3432, -1.3732]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4007, -1.7165, -1.5129],\n",
      "        [ 2.4724, -1.7082, -1.7134]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.4007, -1.7165, -1.5129],\n",
      "        [ 2.4724, -1.7082, -1.7134]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3744, -1.5846, -1.4804],\n",
      "        [ 2.5266, -1.5706, -1.5109]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.3744, -1.5846, -1.4804],\n",
      "        [ 2.5266, -1.5706, -1.5109]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6295, -1.5579, -1.7009],\n",
      "        [ 2.8455, -1.7967, -2.0894]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6295, -1.5579, -1.7009],\n",
      "        [ 2.8455, -1.7967, -2.0894]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7336, -1.6655, -2.1336],\n",
      "        [ 2.8333, -1.5307, -2.0832]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7336, -1.6655, -2.1336],\n",
      "        [ 2.8333, -1.5307, -2.0832]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6025, -1.9508, -2.0065],\n",
      "        [ 2.7702, -1.9268, -2.3492]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6025, -1.9508, -2.0065],\n",
      "        [ 2.7702, -1.9268, -2.3492]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3710, -2.0660, -1.8318],\n",
      "        [ 2.6066, -1.5987, -2.0409]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.3710, -2.0660, -1.8318],\n",
      "        [ 2.6066, -1.5987, -2.0409]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0443, -1.7977, -1.7503],\n",
      "        [ 2.0559, -1.9987, -1.6806]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.0443, -1.7977, -1.7503],\n",
      "        [ 2.0559, -1.9987, -1.6806]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3513, -1.7036, -1.6965],\n",
      "        [ 2.2601, -1.9289, -1.6923]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.3513, -1.7036, -1.6965],\n",
      "        [ 2.2601, -1.9289, -1.6923]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2477, -1.9042, -1.7400],\n",
      "        [ 2.1793, -1.7931, -1.7475]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.2477, -1.9042, -1.7400],\n",
      "        [ 2.1793, -1.7931, -1.7475]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5328, -1.7633, -2.0251],\n",
      "        [ 2.4942, -1.9755, -1.6181]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.5328, -1.7633, -2.0251],\n",
      "        [ 2.4942, -1.9755, -1.6181]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3605, -2.1485, -1.9481],\n",
      "        [ 2.3698, -1.9072, -2.1230]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.3605, -2.1485, -1.9481],\n",
      "        [ 2.3698, -1.9072, -2.1230]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5900, -1.8933, -1.7679],\n",
      "        [ 2.5172, -2.2091, -2.1318]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.5900, -1.8933, -1.7679],\n",
      "        [ 2.5172, -2.2091, -2.1318]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0375, -2.2141, -2.3496],\n",
      "        [ 3.2682, -2.1002, -2.2476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0375, -2.2141, -2.3496],\n",
      "        [ 3.2682, -2.1002, -2.2476]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1842, -2.0458, -2.0866],\n",
      "        [ 3.2567, -2.0397, -2.4173]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1842, -2.0458, -2.0866],\n",
      "        [ 3.2567, -2.0397, -2.4173]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2909, -1.6475, -1.9636],\n",
      "        [ 2.6381, -1.9710, -1.8124]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2909, -1.6475, -1.9636],\n",
      "        [ 2.6381, -1.9710, -1.8124]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2671, -2.1963, -2.0398],\n",
      "        [ 3.1965, -2.2340, -1.8297]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2671, -2.1963, -2.0398],\n",
      "        [ 3.1965, -2.2340, -1.8297]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6407, -1.8449, -1.7082],\n",
      "        [ 3.0919, -2.1615, -1.7389]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6407, -1.8449, -1.7082],\n",
      "        [ 3.0919, -2.1615, -1.7389]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1294, -2.1894, -1.8335],\n",
      "        [ 2.7778, -1.9757, -1.7884]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1294, -2.1894, -1.8335],\n",
      "        [ 2.7778, -1.9757, -1.7884]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0949, -1.8342, -1.8027],\n",
      "        [ 2.8994, -1.6795, -1.9556]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0949, -1.8342, -1.8027],\n",
      "        [ 2.8994, -1.6795, -1.9556]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7034, -1.9306, -1.7973],\n",
      "        [ 2.8847, -1.8176, -1.7273]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7034, -1.9306, -1.7973],\n",
      "        [ 2.8847, -1.8176, -1.7273]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9328, -1.7574, -1.7758],\n",
      "        [ 2.5832, -1.6498, -1.7041]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9328, -1.7574, -1.7758],\n",
      "        [ 2.5832, -1.6498, -1.7041]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7108, -1.5653, -1.6214],\n",
      "        [ 2.6144, -1.6815, -1.7158]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7108, -1.5653, -1.6214],\n",
      "        [ 2.6144, -1.6815, -1.7158]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6369, -1.4461, -1.6763],\n",
      "        [ 2.4044, -1.3321, -1.7115]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6369, -1.4461, -1.6763],\n",
      "        [ 2.4044, -1.3321, -1.7115]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5649, -1.5439, -1.9270],\n",
      "        [ 2.6342, -1.5733, -1.8568]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.5649, -1.5439, -1.9270],\n",
      "        [ 2.6342, -1.5733, -1.8568]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4854, -1.3936, -1.7565],\n",
      "        [ 2.3406, -1.1621, -1.6121]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.4854, -1.3936, -1.7565],\n",
      "        [ 2.3406, -1.1621, -1.6121]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5182, -1.2178, -1.7669],\n",
      "        [ 2.5944, -1.3585, -1.7685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.5182, -1.2178, -1.7669],\n",
      "        [ 2.5944, -1.3585, -1.7685]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4432, -1.2306, -1.6910],\n",
      "        [ 2.6782, -1.5026, -1.9555]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.4432, -1.2306, -1.6910],\n",
      "        [ 2.6782, -1.5026, -1.9555]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4444, -1.1056, -1.8437],\n",
      "        [ 2.5656, -1.4943, -1.7957]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.4444, -1.1056, -1.8437],\n",
      "        [ 2.5656, -1.4943, -1.7957]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8226, -1.5973, -2.0941],\n",
      "        [ 3.0321, -1.7264, -2.0250]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8226, -1.5973, -2.0941],\n",
      "        [ 3.0321, -1.7264, -2.0250]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8041, -1.8488, -2.2405],\n",
      "        [ 2.9688, -1.6873, -2.0596]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8041, -1.8488, -2.2405],\n",
      "        [ 2.9688, -1.6873, -2.0596]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0210, -1.8721, -2.1316],\n",
      "        [ 2.9811, -1.6903, -2.1763]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0210, -1.8721, -2.1316],\n",
      "        [ 2.9811, -1.6903, -2.1763]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2368, -1.8861, -2.1899],\n",
      "        [ 3.3023, -1.8668, -2.5205]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2368, -1.8861, -2.1899],\n",
      "        [ 3.3023, -1.8668, -2.5205]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2916, -2.0384, -2.6115],\n",
      "        [ 3.1693, -2.0042, -2.4728]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2916, -2.0384, -2.6115],\n",
      "        [ 3.1693, -2.0042, -2.4728]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4572, -1.9039, -2.3598],\n",
      "        [ 3.2480, -1.8194, -2.5220]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4572, -1.9039, -2.3598],\n",
      "        [ 3.2480, -1.8194, -2.5220]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2976, -1.9706, -2.4625],\n",
      "        [ 3.2214, -1.7665, -2.4520]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2976, -1.9706, -2.4625],\n",
      "        [ 3.2214, -1.7665, -2.4520]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5216, -1.7265, -2.9075],\n",
      "        [ 3.5009, -1.7673, -2.6395]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5216, -1.7265, -2.9075],\n",
      "        [ 3.5009, -1.7673, -2.6395]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4732, -2.0426, -2.8227],\n",
      "        [ 3.6549, -1.9588, -2.9234]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4732, -2.0426, -2.8227],\n",
      "        [ 3.6549, -1.9588, -2.9234]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3281, -1.6749, -2.6692],\n",
      "        [ 3.4863, -1.6332, -2.6536]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3281, -1.6749, -2.6692],\n",
      "        [ 3.4863, -1.6332, -2.6536]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4407, -1.4562, -2.6224],\n",
      "        [ 3.4158, -1.1920, -2.7361]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4407, -1.4562, -2.6224],\n",
      "        [ 3.4158, -1.1920, -2.7361]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1673, -1.1143, -2.5744],\n",
      "        [ 3.3006, -1.4082, -2.7069]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1673, -1.1143, -2.5744],\n",
      "        [ 3.3006, -1.4082, -2.7069]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2304, -1.3371, -2.7668],\n",
      "        [ 3.1387, -1.1990, -2.6680]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2304, -1.3371, -2.7668],\n",
      "        [ 3.1387, -1.1990, -2.6680]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3001, -1.3076, -2.6773],\n",
      "        [ 3.3765, -1.5861, -2.6097]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3001, -1.3076, -2.6773],\n",
      "        [ 3.3765, -1.5861, -2.6097]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5457, -1.7035, -3.0354],\n",
      "        [ 3.3726, -1.4212, -3.0982]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5457, -1.7035, -3.0354],\n",
      "        [ 3.3726, -1.4212, -3.0982]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5127, -1.5353, -2.7661],\n",
      "        [ 3.5955, -1.8474, -2.9153]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5127, -1.5353, -2.7661],\n",
      "        [ 3.5955, -1.8474, -2.9153]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7550, -1.9197, -3.0199],\n",
      "        [ 3.3798, -1.4305, -2.5933]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7550, -1.9197, -3.0199],\n",
      "        [ 3.3798, -1.4305, -2.5933]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6296, -1.6243, -2.7124],\n",
      "        [ 3.6697, -1.8881, -2.9619]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6296, -1.6243, -2.7124],\n",
      "        [ 3.6697, -1.8881, -2.9619]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2865, -1.7692, -2.7573],\n",
      "        [ 3.5689, -1.5520, -3.0169]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2865, -1.7692, -2.7573],\n",
      "        [ 3.5689, -1.5520, -3.0169]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4622, -1.6699, -2.9779],\n",
      "        [ 3.5121, -1.7763, -2.7777]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4622, -1.6699, -2.9779],\n",
      "        [ 3.5121, -1.7763, -2.7777]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4064, -1.9568, -2.8140],\n",
      "        [ 3.5330, -1.6975, -2.9986]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4064, -1.9568, -2.8140],\n",
      "        [ 3.5330, -1.6975, -2.9986]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3997, -1.8856, -2.9251],\n",
      "        [ 3.4664, -1.7626, -2.8683]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3997, -1.8856, -2.9251],\n",
      "        [ 3.4664, -1.7626, -2.8683]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6010, -1.7642, -3.0497],\n",
      "        [ 3.6904, -1.9004, -2.8990]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6010, -1.7642, -3.0497],\n",
      "        [ 3.6904, -1.9004, -2.8990]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8582, -1.5738, -2.9831],\n",
      "        [ 3.5562, -1.8703, -2.9075]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8582, -1.5738, -2.9831],\n",
      "        [ 3.5562, -1.8703, -2.9075]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6109, -1.9189, -2.8390],\n",
      "        [ 3.6212, -1.6766, -2.9650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6109, -1.9189, -2.8390],\n",
      "        [ 3.6212, -1.6766, -2.9650]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6163, -2.1409, -2.7523],\n",
      "        [ 3.7134, -1.7242, -3.0376]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6163, -2.1409, -2.7523],\n",
      "        [ 3.7134, -1.7242, -3.0376]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6765, -1.8044, -2.8597],\n",
      "        [ 3.5452, -2.1031, -2.8402]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6765, -1.8044, -2.8597],\n",
      "        [ 3.5452, -2.1031, -2.8402]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8728, -1.9890, -2.9606],\n",
      "        [ 3.5531, -1.9760, -2.9683]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8728, -1.9890, -2.9606],\n",
      "        [ 3.5531, -1.9760, -2.9683]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3956, -1.8004, -2.9889],\n",
      "        [ 3.5327, -1.9409, -2.9236]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3956, -1.8004, -2.9889],\n",
      "        [ 3.5327, -1.9409, -2.9236]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5767, -1.7661, -2.8203],\n",
      "        [ 3.4123, -1.5738, -2.8678]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5767, -1.7661, -2.8203],\n",
      "        [ 3.4123, -1.5738, -2.8678]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5453, -1.7641, -2.8274],\n",
      "        [ 3.4936, -1.6040, -2.5573]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5453, -1.7641, -2.8274],\n",
      "        [ 3.4936, -1.6040, -2.5573]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5557, -1.7302, -2.8145],\n",
      "        [ 3.5362, -1.9546, -2.7261]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5557, -1.7302, -2.8145],\n",
      "        [ 3.5362, -1.9546, -2.7261]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2204, -1.4918, -2.4660],\n",
      "        [ 3.3050, -1.7345, -2.7078]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2204, -1.4918, -2.4660],\n",
      "        [ 3.3050, -1.7345, -2.7078]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5148, -1.8846, -2.7496],\n",
      "        [ 3.4295, -1.5533, -2.5474]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5148, -1.8846, -2.7496],\n",
      "        [ 3.4295, -1.5533, -2.5474]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4937, -1.4571, -2.6117],\n",
      "        [ 3.3522, -1.7100, -2.7615]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4937, -1.4571, -2.6117],\n",
      "        [ 3.3522, -1.7100, -2.7615]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3730, -1.7554, -2.5040],\n",
      "        [ 3.3924, -2.0809, -2.6088]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3730, -1.7554, -2.5040],\n",
      "        [ 3.3924, -2.0809, -2.6088]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3091, -1.7996, -2.7048],\n",
      "        [ 3.2473, -1.7159, -2.6518]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3091, -1.7996, -2.7048],\n",
      "        [ 3.2473, -1.7159, -2.6518]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4025, -1.9335, -2.5735],\n",
      "        [ 3.4751, -1.9666, -2.5890]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4025, -1.9335, -2.5735],\n",
      "        [ 3.4751, -1.9666, -2.5890]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3960, -1.7378, -2.4687],\n",
      "        [ 3.1550, -1.8092, -2.4782]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3960, -1.7378, -2.4687],\n",
      "        [ 3.1550, -1.8092, -2.4782]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2122, -1.7071, -2.4186],\n",
      "        [ 3.5009, -2.1075, -2.4644]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2122, -1.7071, -2.4186],\n",
      "        [ 3.5009, -2.1075, -2.4644]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4191, -2.2000, -2.3515],\n",
      "        [ 3.2585, -1.8302, -2.6523]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4191, -2.2000, -2.3515],\n",
      "        [ 3.2585, -1.8302, -2.6523]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0512, -1.7321, -2.1217],\n",
      "        [ 3.1883, -2.1561, -2.6175]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0512, -1.7321, -2.1217],\n",
      "        [ 3.1883, -2.1561, -2.6175]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3572, -1.9576, -2.6534],\n",
      "        [ 3.3459, -1.9862, -2.3789]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3572, -1.9576, -2.6534],\n",
      "        [ 3.3459, -1.9862, -2.3789]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1972, -1.9873, -2.1779],\n",
      "        [ 3.3505, -2.0563, -2.4064]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1972, -1.9873, -2.1779],\n",
      "        [ 3.3505, -2.0563, -2.4064]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3697, -1.9984, -2.5078],\n",
      "        [ 3.0595, -1.6587, -2.1908]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3697, -1.9984, -2.5078],\n",
      "        [ 3.0595, -1.6587, -2.1908]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2832, -1.8793, -2.2782],\n",
      "        [ 3.1082, -2.0668, -2.3250]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2832, -1.8793, -2.2782],\n",
      "        [ 3.1082, -2.0668, -2.3250]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1868, -1.7205, -2.3705],\n",
      "        [ 3.1515, -1.9341, -2.0544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1868, -1.7205, -2.3705],\n",
      "        [ 3.1515, -1.9341, -2.0544]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4614, -2.0910, -2.3146],\n",
      "        [ 3.0827, -1.9030, -2.2065]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4614, -2.0910, -2.3146],\n",
      "        [ 3.0827, -1.9030, -2.2065]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5116, -2.1177, -2.2465],\n",
      "        [ 3.2574, -2.0688, -2.1818]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5116, -2.1177, -2.2465],\n",
      "        [ 3.2574, -2.0688, -2.1818]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1325, -2.0387, -1.9603],\n",
      "        [ 3.4631, -1.9932, -2.4082]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1325, -2.0387, -1.9603],\n",
      "        [ 3.4631, -1.9932, -2.4082]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4810, -1.8834, -2.4808],\n",
      "        [ 3.3053, -1.9478, -2.1669]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4810, -1.8834, -2.4808],\n",
      "        [ 3.3053, -1.9478, -2.1669]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5121, -1.8040, -2.4046],\n",
      "        [ 3.2357, -2.1378, -2.3342]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5121, -1.8040, -2.4046],\n",
      "        [ 3.2357, -2.1378, -2.3342]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3205, -1.9996, -2.3050],\n",
      "        [ 3.3429, -2.0748, -2.2926]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3205, -1.9996, -2.3050],\n",
      "        [ 3.3429, -2.0748, -2.2926]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3584, -2.1442, -2.2570],\n",
      "        [ 3.4994, -2.1367, -2.5557]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3584, -2.1442, -2.2570],\n",
      "        [ 3.4994, -2.1367, -2.5557]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7099, -2.1523, -2.3921],\n",
      "        [ 3.4298, -1.8635, -2.4336]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7099, -2.1523, -2.3921],\n",
      "        [ 3.4298, -1.8635, -2.4336]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6286, -2.3194, -2.4059],\n",
      "        [ 3.6086, -2.3944, -2.4121]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6286, -2.3194, -2.4059],\n",
      "        [ 3.6086, -2.3944, -2.4121]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6516, -2.1968, -2.3833],\n",
      "        [ 3.5755, -2.3045, -2.3784]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6516, -2.1968, -2.3833],\n",
      "        [ 3.5755, -2.3045, -2.3784]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6804, -2.2594, -2.3689],\n",
      "        [ 3.4352, -2.2810, -2.4288]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6804, -2.2594, -2.3689],\n",
      "        [ 3.4352, -2.2810, -2.4288]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8346, -2.2837, -2.6895],\n",
      "        [ 3.6086, -2.1534, -2.5006]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8346, -2.2837, -2.6895],\n",
      "        [ 3.6086, -2.1534, -2.5006]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7863, -2.2247, -2.3458],\n",
      "        [ 3.5459, -2.2180, -2.4778]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7863, -2.2247, -2.3458],\n",
      "        [ 3.5459, -2.2180, -2.4778]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7778, -2.1326, -2.5065],\n",
      "        [ 3.5415, -1.8661, -2.4286]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7778, -2.1326, -2.5065],\n",
      "        [ 3.5415, -1.8661, -2.4286]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3712, -1.9853, -2.4915],\n",
      "        [ 3.6537, -1.7278, -2.5902]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3712, -1.9853, -2.4915],\n",
      "        [ 3.6537, -1.7278, -2.5902]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4017, -1.8711, -2.5058],\n",
      "        [ 3.5621, -2.0312, -2.4663]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4017, -1.8711, -2.5058],\n",
      "        [ 3.5621, -2.0312, -2.4663]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2960, -1.7869, -2.4418],\n",
      "        [ 3.3135, -1.7928, -2.3026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2960, -1.7869, -2.4418],\n",
      "        [ 3.3135, -1.7928, -2.3026]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2386, -2.0571, -2.2119],\n",
      "        [ 3.2739, -1.9573, -2.2896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2386, -2.0571, -2.2119],\n",
      "        [ 3.2739, -1.9573, -2.2896]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4137, -1.7490, -2.2469],\n",
      "        [ 3.6488, -1.5843, -2.4984]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4137, -1.7490, -2.2469],\n",
      "        [ 3.6488, -1.5843, -2.4984]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8729, -1.1276, -2.0690],\n",
      "        [ 3.0540, -1.3716, -2.1397]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8729, -1.1276, -2.0690],\n",
      "        [ 3.0540, -1.3716, -2.1397]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6634, -1.1984, -1.8314],\n",
      "        [ 2.9900, -1.2088, -2.1803]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6634, -1.1984, -1.8314],\n",
      "        [ 2.9900, -1.2088, -2.1803]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6826, -1.1729, -2.3451],\n",
      "        [ 2.4864, -1.0102, -2.0893]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6826, -1.1729, -2.3451],\n",
      "        [ 2.4864, -1.0102, -2.0893]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3576, -0.7482, -2.0510],\n",
      "        [ 2.7566, -0.7397, -1.9001]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.3576, -0.7482, -2.0510],\n",
      "        [ 2.7566, -0.7397, -1.9001]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1968, -1.0328, -1.8570],\n",
      "        [ 2.4074, -0.7710, -1.7262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.1968, -1.0328, -1.8570],\n",
      "        [ 2.4074, -0.7710, -1.7262]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5646, -0.4476, -1.9054],\n",
      "        [ 2.3190, -0.4873, -1.8955]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.5646, -0.4476, -1.9054],\n",
      "        [ 2.3190, -0.4873, -1.8955]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8290, -0.8810, -1.7574],\n",
      "        [ 2.7590, -0.9946, -1.9673]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8290, -0.8810, -1.7574],\n",
      "        [ 2.7590, -0.9946, -1.9673]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6963, -1.0056, -2.3016],\n",
      "        [ 2.4004, -0.6857, -2.1180]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6963, -1.0056, -2.3016],\n",
      "        [ 2.4004, -0.6857, -2.1180]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5995, -0.8905, -2.1171],\n",
      "        [ 2.8139, -0.7037, -2.2803]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.5995, -0.8905, -2.1171],\n",
      "        [ 2.8139, -0.7037, -2.2803]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7460, -1.0740, -1.9456],\n",
      "        [ 2.9727, -1.1125, -2.1833]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7460, -1.0740, -1.9456],\n",
      "        [ 2.9727, -1.1125, -2.1833]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8662, -1.0267, -2.1523],\n",
      "        [ 3.0298, -1.1339, -2.0586]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8662, -1.0267, -2.1523],\n",
      "        [ 3.0298, -1.1339, -2.0586]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0410, -1.3379, -2.4153],\n",
      "        [ 3.0138, -1.2084, -2.4499]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0410, -1.3379, -2.4153],\n",
      "        [ 3.0138, -1.2084, -2.4499]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9765, -0.9701, -2.2782],\n",
      "        [ 2.9185, -1.6272, -2.2765]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9765, -0.9701, -2.2782],\n",
      "        [ 2.9185, -1.6272, -2.2765]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2156, -1.5760, -2.3511],\n",
      "        [ 2.8623, -1.3339, -2.2016]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2156, -1.5760, -2.3511],\n",
      "        [ 2.8623, -1.3339, -2.2016]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0399, -1.3507, -2.1991],\n",
      "        [ 2.8506, -1.4055, -2.3536]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0399, -1.3507, -2.1991],\n",
      "        [ 2.8506, -1.4055, -2.3536]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2225, -1.4896, -2.3965],\n",
      "        [ 2.6742, -1.5983, -1.9819]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2225, -1.4896, -2.3965],\n",
      "        [ 2.6742, -1.5983, -1.9819]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2481, -1.5467, -2.2588],\n",
      "        [ 2.7313, -1.3237, -2.1232]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2481, -1.5467, -2.2588],\n",
      "        [ 2.7313, -1.3237, -2.1232]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7109, -1.5126, -1.8506],\n",
      "        [ 2.9873, -1.6246, -2.5869]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7109, -1.5126, -1.8506],\n",
      "        [ 2.9873, -1.6246, -2.5869]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6992, -1.5577, -2.2236],\n",
      "        [ 3.1423, -1.8467, -2.1801]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6992, -1.5577, -2.2236],\n",
      "        [ 3.1423, -1.8467, -2.1801]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0920, -1.6273, -2.3442],\n",
      "        [ 3.6844, -1.9184, -2.2983]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0920, -1.6273, -2.3442],\n",
      "        [ 3.6844, -1.9184, -2.2983]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6031, -1.7919, -2.2693],\n",
      "        [ 3.6887, -1.7804, -2.4505]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6031, -1.7919, -2.2693],\n",
      "        [ 3.6887, -1.7804, -2.4505]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6346, -1.9130, -2.3907],\n",
      "        [ 3.6556, -1.6576, -2.3854]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6346, -1.9130, -2.3907],\n",
      "        [ 3.6556, -1.6576, -2.3854]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4383, -1.7594, -2.3080],\n",
      "        [ 3.9821, -1.8484, -2.4553]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4383, -1.7594, -2.3080],\n",
      "        [ 3.9821, -1.8484, -2.4553]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7013, -1.7867, -2.3694],\n",
      "        [ 3.5544, -1.8523, -2.4923]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7013, -1.7867, -2.3694],\n",
      "        [ 3.5544, -1.8523, -2.4923]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8547, -1.8151, -2.4381],\n",
      "        [ 3.8899, -1.8299, -2.4750]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8547, -1.8151, -2.4381],\n",
      "        [ 3.8899, -1.8299, -2.4750]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8843, -1.7541, -2.4730],\n",
      "        [ 3.2010, -1.8377, -2.0611]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8843, -1.7541, -2.4730],\n",
      "        [ 3.2010, -1.8377, -2.0611]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4577, -1.8925, -1.8783],\n",
      "        [ 3.5843, -1.4749, -1.9655]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4577, -1.8925, -1.8783],\n",
      "        [ 3.5843, -1.4749, -1.9655]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7291, -1.7816, -2.1575],\n",
      "        [ 3.3769, -1.8725, -2.1898]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7291, -1.7816, -2.1575],\n",
      "        [ 3.3769, -1.8725, -2.1898]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2884, -1.9371, -1.8231],\n",
      "        [ 3.5763, -1.8300, -2.2468]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2884, -1.9371, -1.8231],\n",
      "        [ 3.5763, -1.8300, -2.2468]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3195, -1.9737, -2.0458],\n",
      "        [ 3.2723, -1.7762, -1.9915]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3195, -1.9737, -2.0458],\n",
      "        [ 3.2723, -1.7762, -1.9915]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9640, -1.7843, -1.8056],\n",
      "        [ 3.2640, -2.0404, -2.0398]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9640, -1.7843, -1.8056],\n",
      "        [ 3.2640, -2.0404, -2.0398]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3159, -1.9139, -2.0247],\n",
      "        [ 3.2025, -2.3196, -2.1286]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3159, -1.9139, -2.0247],\n",
      "        [ 3.2025, -2.3196, -2.1286]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1854, -1.7145, -1.7128],\n",
      "        [ 3.2842, -2.1043, -1.7515]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1854, -1.7145, -1.7128],\n",
      "        [ 3.2842, -2.1043, -1.7515]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8769, -1.9353, -1.8009],\n",
      "        [ 3.3342, -1.8514, -1.8303]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8769, -1.9353, -1.8009],\n",
      "        [ 3.3342, -1.8514, -1.8303]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3838, -2.0536, -1.8791],\n",
      "        [ 3.2791, -1.9848, -1.7803]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3838, -2.0536, -1.8791],\n",
      "        [ 3.2791, -1.9848, -1.7803]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3661, -2.1705, -1.7831],\n",
      "        [ 3.2920, -2.1820, -1.8986]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3661, -2.1705, -1.7831],\n",
      "        [ 3.2920, -2.1820, -1.8986]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3054, -1.9866, -2.1451],\n",
      "        [ 3.2753, -2.0135, -2.1549]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3054, -1.9866, -2.1451],\n",
      "        [ 3.2753, -2.0135, -2.1549]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6568, -2.0411, -2.2162],\n",
      "        [ 3.3895, -1.9345, -2.0991]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6568, -2.0411, -2.2162],\n",
      "        [ 3.3895, -1.9345, -2.0991]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4023, -2.1894, -2.0824],\n",
      "        [ 3.4204, -1.7172, -2.0964]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4023, -2.1894, -2.0824],\n",
      "        [ 3.4204, -1.7172, -2.0964]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2330, -2.0094, -2.1749],\n",
      "        [ 3.1104, -1.9576, -2.1727]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2330, -2.0094, -2.1749],\n",
      "        [ 3.1104, -1.9576, -2.1727]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4552, -2.3513, -2.0933],\n",
      "        [ 3.1825, -1.9323, -2.1128]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4552, -2.3513, -2.0933],\n",
      "        [ 3.1825, -1.9323, -2.1128]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3234, -1.9027, -2.0196],\n",
      "        [ 3.1957, -2.1898, -2.0217]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3234, -1.9027, -2.0196],\n",
      "        [ 3.1957, -2.1898, -2.0217]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3302, -2.0073, -1.8698],\n",
      "        [ 2.9612, -1.9454, -2.0625]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3302, -2.0073, -1.8698],\n",
      "        [ 2.9612, -1.9454, -2.0625]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3695, -1.8307, -2.1224],\n",
      "        [ 3.6132, -1.8359, -1.9943]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3695, -1.8307, -2.1224],\n",
      "        [ 3.6132, -1.8359, -1.9943]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0497, -1.6262, -2.0224],\n",
      "        [ 3.1398, -1.7049, -1.9944]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0497, -1.6262, -2.0224],\n",
      "        [ 3.1398, -1.7049, -1.9944]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0163, -1.9334, -1.8466],\n",
      "        [ 2.8815, -1.5865, -1.4775]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0163, -1.9334, -1.8466],\n",
      "        [ 2.8815, -1.5865, -1.4775]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1089, -1.8539, -2.0231],\n",
      "        [ 3.1003, -1.9808, -2.0114]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1089, -1.8539, -2.0231],\n",
      "        [ 3.1003, -1.9808, -2.0114]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8994, -1.3387, -1.6976],\n",
      "        [ 2.8218, -1.4849, -2.0585]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8994, -1.3387, -1.6976],\n",
      "        [ 2.8218, -1.4849, -2.0585]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0635, -1.5470, -2.0145],\n",
      "        [ 2.7784, -1.3021, -1.8943]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0635, -1.5470, -2.0145],\n",
      "        [ 2.7784, -1.3021, -1.8943]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6954, -1.4425, -1.9587],\n",
      "        [ 2.7779, -1.4249, -1.7768]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6954, -1.4425, -1.9587],\n",
      "        [ 2.7779, -1.4249, -1.7768]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5950, -1.2307, -2.0963],\n",
      "        [ 2.9705, -1.4583, -2.1386]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.5950, -1.2307, -2.0963],\n",
      "        [ 2.9705, -1.4583, -2.1386]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0111, -1.5451, -1.9836],\n",
      "        [ 2.8737, -1.4152, -1.8890]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0111, -1.5451, -1.9836],\n",
      "        [ 2.8737, -1.4152, -1.8890]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0523, -1.3585, -2.0813],\n",
      "        [ 2.6584, -1.4237, -2.0323]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0523, -1.3585, -2.0813],\n",
      "        [ 2.6584, -1.4237, -2.0323]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9530, -1.3222, -2.2569],\n",
      "        [ 3.0760, -1.6131, -2.0654]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9530, -1.3222, -2.2569],\n",
      "        [ 3.0760, -1.6131, -2.0654]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7330, -1.2872, -2.0620],\n",
      "        [ 2.8334, -1.4193, -1.9648]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7330, -1.2872, -2.0620],\n",
      "        [ 2.8334, -1.4193, -1.9648]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6817, -1.2480, -1.9160],\n",
      "        [ 2.6365, -1.2753, -1.8045]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6817, -1.2480, -1.9160],\n",
      "        [ 2.6365, -1.2753, -1.8045]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9964, -1.1012, -2.1452],\n",
      "        [ 3.0163, -1.2862, -2.2452]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9964, -1.1012, -2.1452],\n",
      "        [ 3.0163, -1.2862, -2.2452]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2920, -1.5791, -2.1937],\n",
      "        [ 3.0114, -1.0334, -2.2126]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2920, -1.5791, -2.1937],\n",
      "        [ 3.0114, -1.0334, -2.2126]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9491, -1.2143, -2.1420],\n",
      "        [ 3.0892, -1.4500, -2.1271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9491, -1.2143, -2.1420],\n",
      "        [ 3.0892, -1.4500, -2.1271]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1698, -1.3460, -2.2732],\n",
      "        [ 3.3786, -1.3654, -2.2215]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1698, -1.3460, -2.2732],\n",
      "        [ 3.3786, -1.3654, -2.2215]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0016, -1.2106, -2.2571],\n",
      "        [ 3.0703, -1.5948, -2.1378]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0016, -1.2106, -2.2571],\n",
      "        [ 3.0703, -1.5948, -2.1378]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1003, -1.2706, -2.1442],\n",
      "        [ 2.8361, -1.5067, -2.4996]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1003, -1.2706, -2.1442],\n",
      "        [ 2.8361, -1.5067, -2.4996]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2410, -1.5900, -2.4702],\n",
      "        [ 3.0691, -1.5983, -2.0841]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2410, -1.5900, -2.4702],\n",
      "        [ 3.0691, -1.5983, -2.0841]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2398, -1.3942, -2.4439],\n",
      "        [ 3.3520, -1.5356, -2.2958]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2398, -1.3942, -2.4439],\n",
      "        [ 3.3520, -1.5356, -2.2958]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3285, -1.5258, -2.5740],\n",
      "        [ 3.4245, -1.6554, -2.5183]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3285, -1.5258, -2.5740],\n",
      "        [ 3.4245, -1.6554, -2.5183]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2069, -1.5957, -2.2702],\n",
      "        [ 3.1579, -1.5594, -2.5168]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2069, -1.5957, -2.2702],\n",
      "        [ 3.1579, -1.5594, -2.5168]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1035, -1.8473, -2.5342],\n",
      "        [ 3.1751, -1.5736, -2.3799]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1035, -1.8473, -2.5342],\n",
      "        [ 3.1751, -1.5736, -2.3799]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3635, -1.8912, -2.5003],\n",
      "        [ 3.2391, -1.6998, -2.2734]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3635, -1.8912, -2.5003],\n",
      "        [ 3.2391, -1.6998, -2.2734]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6451, -1.9636, -2.5063],\n",
      "        [ 3.6136, -1.6831, -2.6047]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6451, -1.9636, -2.5063],\n",
      "        [ 3.6136, -1.6831, -2.6047]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5955, -1.5570, -2.3160],\n",
      "        [ 3.4484, -1.6829, -2.5050]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5955, -1.5570, -2.3160],\n",
      "        [ 3.4484, -1.6829, -2.5050]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4862, -1.7389, -2.6140],\n",
      "        [ 3.5018, -1.9039, -2.6632]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4862, -1.7389, -2.6140],\n",
      "        [ 3.5018, -1.9039, -2.6632]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4129, -1.8530, -2.6680],\n",
      "        [ 3.6233, -1.6146, -2.5245]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4129, -1.8530, -2.6680],\n",
      "        [ 3.6233, -1.6146, -2.5245]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7470, -1.6180, -2.4008],\n",
      "        [ 3.6630, -1.9511, -2.5536]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7470, -1.6180, -2.4008],\n",
      "        [ 3.6630, -1.9511, -2.5536]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8163, -2.1212, -2.4006],\n",
      "        [ 3.4302, -1.7792, -2.4758]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8163, -2.1212, -2.4006],\n",
      "        [ 3.4302, -1.7792, -2.4758]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9266, -1.9982, -2.6737],\n",
      "        [ 3.6782, -2.0644, -2.5802]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9266, -1.9982, -2.6737],\n",
      "        [ 3.6782, -2.0644, -2.5802]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7849, -2.0961, -2.6929],\n",
      "        [ 3.7889, -2.0997, -2.4964]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7849, -2.0961, -2.6929],\n",
      "        [ 3.7889, -2.0997, -2.4964]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8197, -2.3110, -2.5174],\n",
      "        [ 3.7127, -2.0514, -2.6044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8197, -2.3110, -2.5174],\n",
      "        [ 3.7127, -2.0514, -2.6044]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8293, -1.9677, -2.6059],\n",
      "        [ 3.5283, -2.0752, -2.6486]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8293, -1.9677, -2.6059],\n",
      "        [ 3.5283, -2.0752, -2.6486]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5960, -1.8982, -2.5906],\n",
      "        [ 3.8453, -1.8558, -2.6767]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5960, -1.8982, -2.5906],\n",
      "        [ 3.8453, -1.8558, -2.6767]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8207, -2.1064, -2.7314],\n",
      "        [ 3.9537, -2.2476, -2.6141]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8207, -2.1064, -2.7314],\n",
      "        [ 3.9537, -2.2476, -2.6141]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9726, -2.4390, -2.6330],\n",
      "        [ 3.8471, -2.0114, -2.6044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9726, -2.4390, -2.6330],\n",
      "        [ 3.8471, -2.0114, -2.6044]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7863, -2.2378, -2.6183],\n",
      "        [ 3.9084, -2.1031, -2.7475]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7863, -2.2378, -2.6183],\n",
      "        [ 3.9084, -2.1031, -2.7475]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8005, -2.1686, -2.7290],\n",
      "        [ 3.8473, -2.3162, -2.7086]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8005, -2.1686, -2.7290],\n",
      "        [ 3.8473, -2.3162, -2.7086]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7572, -1.8233, -2.6905],\n",
      "        [ 3.9562, -2.0453, -2.5825]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7572, -1.8233, -2.6905],\n",
      "        [ 3.9562, -2.0453, -2.5825]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8474, -2.2848, -2.6519],\n",
      "        [ 3.7563, -2.0697, -2.5843]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8474, -2.2848, -2.6519],\n",
      "        [ 3.7563, -2.0697, -2.5843]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9345, -2.0452, -2.7578],\n",
      "        [ 3.9060, -2.0474, -2.4920]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9345, -2.0452, -2.7578],\n",
      "        [ 3.9060, -2.0474, -2.4920]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8508, -2.2018, -2.6248],\n",
      "        [ 3.9424, -2.1402, -2.6301]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8508, -2.2018, -2.6248],\n",
      "        [ 3.9424, -2.1402, -2.6301]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6014, -2.0041, -2.4576],\n",
      "        [ 3.4452, -2.0913, -2.3320]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6014, -2.0041, -2.4576],\n",
      "        [ 3.4452, -2.0913, -2.3320]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4983, -2.0864, -2.7137],\n",
      "        [ 3.9841, -2.1268, -2.6695]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4983, -2.0864, -2.7137],\n",
      "        [ 3.9841, -2.1268, -2.6695]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8664, -2.1026, -2.7349],\n",
      "        [ 3.7577, -1.9217, -2.5541]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8664, -2.1026, -2.7349],\n",
      "        [ 3.7577, -1.9217, -2.5541]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6764, -2.0725, -2.3693],\n",
      "        [ 3.5697, -2.0575, -2.4246]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6764, -2.0725, -2.3693],\n",
      "        [ 3.5697, -2.0575, -2.4246]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6103, -2.0159, -2.4054],\n",
      "        [ 3.5612, -2.0353, -2.4002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6103, -2.0159, -2.4054],\n",
      "        [ 3.5612, -2.0353, -2.4002]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6623, -2.1114, -2.5124],\n",
      "        [ 3.4883, -2.1391, -2.4510]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6623, -2.1114, -2.5124],\n",
      "        [ 3.4883, -2.1391, -2.4510]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3744, -2.1151, -2.5543],\n",
      "        [ 3.6031, -2.1164, -2.5424]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3744, -2.1151, -2.5543],\n",
      "        [ 3.6031, -2.1164, -2.5424]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6655, -2.0865, -2.5193],\n",
      "        [ 3.5430, -2.1222, -2.6091]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6655, -2.0865, -2.5193],\n",
      "        [ 3.5430, -2.1222, -2.6091]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5579, -2.1728, -2.4576],\n",
      "        [ 3.6809, -1.9237, -2.5724]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5579, -2.1728, -2.4576],\n",
      "        [ 3.6809, -1.9237, -2.5724]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7287, -2.1384, -2.5904],\n",
      "        [ 3.6036, -2.2152, -2.5264]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7287, -2.1384, -2.5904],\n",
      "        [ 3.6036, -2.2152, -2.5264]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8466, -2.2171, -2.6016],\n",
      "        [ 3.7480, -2.1596, -2.5452]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8466, -2.2171, -2.6016],\n",
      "        [ 3.7480, -2.1596, -2.5452]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3978, -1.9471, -2.5246],\n",
      "        [ 3.4520, -2.1624, -2.3945]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3978, -1.9471, -2.5246],\n",
      "        [ 3.4520, -2.1624, -2.3945]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5201, -2.2566, -2.3958],\n",
      "        [ 3.4694, -2.3640, -2.2649]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5201, -2.2566, -2.3958],\n",
      "        [ 3.4694, -2.3640, -2.2649]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6494, -2.1080, -2.5759],\n",
      "        [ 3.6692, -2.1647, -2.4985]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6494, -2.1080, -2.5759],\n",
      "        [ 3.6692, -2.1647, -2.4985]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5424, -2.1204, -2.5104],\n",
      "        [ 3.3324, -2.0555, -2.3455]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5424, -2.1204, -2.5104],\n",
      "        [ 3.3324, -2.0555, -2.3455]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5112, -2.0207, -2.6376],\n",
      "        [ 3.5343, -2.0317, -2.4659]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5112, -2.0207, -2.6376],\n",
      "        [ 3.5343, -2.0317, -2.4659]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3813, -2.0410, -2.4019],\n",
      "        [ 3.7149, -2.0219, -2.7514]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3813, -2.0410, -2.4019],\n",
      "        [ 3.7149, -2.0219, -2.7514]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4792, -2.0583, -2.5376],\n",
      "        [ 3.6272, -2.2030, -2.4816]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4792, -2.0583, -2.5376],\n",
      "        [ 3.6272, -2.2030, -2.4816]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4621, -2.0663, -2.4293],\n",
      "        [ 3.1276, -1.9574, -2.3049]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4621, -2.0663, -2.4293],\n",
      "        [ 3.1276, -1.9574, -2.3049]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6458, -1.8134, -2.5171],\n",
      "        [ 3.3273, -1.6547, -2.5183]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6458, -1.8134, -2.5171],\n",
      "        [ 3.3273, -1.6547, -2.5183]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5269, -2.1100, -2.5991],\n",
      "        [ 3.5702, -2.0031, -2.5505]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5269, -2.1100, -2.5991],\n",
      "        [ 3.5702, -2.0031, -2.5505]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2823, -1.9464, -2.4034],\n",
      "        [ 3.6347, -2.0257, -2.6401]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2823, -1.9464, -2.4034],\n",
      "        [ 3.6347, -2.0257, -2.6401]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4426, -1.9263, -2.5800],\n",
      "        [ 3.4364, -1.9457, -2.4335]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4426, -1.9263, -2.5800],\n",
      "        [ 3.4364, -1.9457, -2.4335]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5068, -1.9621, -2.3564],\n",
      "        [ 3.3915, -1.8876, -2.6732]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5068, -1.9621, -2.3564],\n",
      "        [ 3.3915, -1.8876, -2.6732]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5454, -2.0664, -2.6169],\n",
      "        [ 3.5135, -1.9573, -2.7154]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5454, -2.0664, -2.6169],\n",
      "        [ 3.5135, -1.9573, -2.7154]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5373, -1.8167, -2.4101],\n",
      "        [ 3.5064, -1.9321, -2.6139]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5373, -1.8167, -2.4101],\n",
      "        [ 3.5064, -1.9321, -2.6139]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5129, -2.0323, -2.6745],\n",
      "        [ 3.3947, -2.1861, -2.4638]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5129, -2.0323, -2.6745],\n",
      "        [ 3.3947, -2.1861, -2.4638]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2594, -2.0122, -2.6209],\n",
      "        [ 3.4064, -2.1034, -2.2160]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2594, -2.0122, -2.6209],\n",
      "        [ 3.4064, -2.1034, -2.2160]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9789, -1.9998, -2.1800],\n",
      "        [ 3.2778, -2.1577, -2.3332]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9789, -1.9998, -2.1800],\n",
      "        [ 3.2778, -2.1577, -2.3332]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6734, -2.0978, -2.4139],\n",
      "        [ 3.3215, -2.0326, -2.3617]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6734, -2.0978, -2.4139],\n",
      "        [ 3.3215, -2.0326, -2.3617]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4217, -2.0213, -2.3848],\n",
      "        [ 3.3584, -1.9034, -2.2727]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4217, -2.0213, -2.3848],\n",
      "        [ 3.3584, -1.9034, -2.2727]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1906, -1.8559, -2.3212],\n",
      "        [ 3.3664, -1.9297, -2.3890]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1906, -1.8559, -2.3212],\n",
      "        [ 3.3664, -1.9297, -2.3890]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0487, -1.8194, -2.0814],\n",
      "        [ 3.1520, -1.8831, -2.1865]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0487, -1.8194, -2.0814],\n",
      "        [ 3.1520, -1.8831, -2.1865]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0217, -2.1176, -1.9785],\n",
      "        [ 3.2544, -2.1042, -2.2075]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0217, -2.1176, -1.9785],\n",
      "        [ 3.2544, -2.1042, -2.2075]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8559, -1.9894, -2.1196],\n",
      "        [ 3.1339, -1.7405, -2.0125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8559, -1.9894, -2.1196],\n",
      "        [ 3.1339, -1.7405, -2.0125]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9096, -1.8803, -1.9799],\n",
      "        [ 2.8738, -1.9407, -1.8343]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9096, -1.8803, -1.9799],\n",
      "        [ 2.8738, -1.9407, -1.8343]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7599, -1.5654, -2.0059],\n",
      "        [ 3.0880, -1.9780, -2.0918]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7599, -1.5654, -2.0059],\n",
      "        [ 3.0880, -1.9780, -2.0918]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9277, -2.0421, -1.9848],\n",
      "        [ 3.1312, -1.7870, -2.0122]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9277, -2.0421, -1.9848],\n",
      "        [ 3.1312, -1.7870, -2.0122]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8703, -1.7270, -2.0050],\n",
      "        [ 2.8485, -1.9187, -2.0626]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8703, -1.7270, -2.0050],\n",
      "        [ 2.8485, -1.9187, -2.0626]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9842, -1.9206, -2.1770],\n",
      "        [ 3.1038, -1.8575, -2.3030]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9842, -1.9206, -2.1770],\n",
      "        [ 3.1038, -1.8575, -2.3030]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0145, -1.7738, -2.0375],\n",
      "        [ 3.0040, -1.7435, -1.9839]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0145, -1.7738, -2.0375],\n",
      "        [ 3.0040, -1.7435, -1.9839]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0057, -1.6399, -2.3235],\n",
      "        [ 2.8237, -1.8407, -2.1983]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0057, -1.6399, -2.3235],\n",
      "        [ 2.8237, -1.8407, -2.1983]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9062, -1.9213, -2.0076],\n",
      "        [ 3.0698, -1.7421, -2.1703]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9062, -1.9213, -2.0076],\n",
      "        [ 3.0698, -1.7421, -2.1703]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0095, -1.8885, -2.2189],\n",
      "        [ 2.8089, -1.6783, -2.1916]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0095, -1.8885, -2.2189],\n",
      "        [ 2.8089, -1.6783, -2.1916]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6210, -1.7004, -2.1163],\n",
      "        [ 2.6910, -1.8413, -2.1341]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6210, -1.7004, -2.1163],\n",
      "        [ 2.6910, -1.8413, -2.1341]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6918, -1.6595, -2.1217],\n",
      "        [ 2.8032, -1.8117, -1.9768]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6918, -1.6595, -2.1217],\n",
      "        [ 2.8032, -1.8117, -1.9768]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9019, -1.5464, -2.1547],\n",
      "        [ 2.6198, -1.2690, -2.2688]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9019, -1.5464, -2.1547],\n",
      "        [ 2.6198, -1.2690, -2.2688]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7382, -1.7501, -2.2668],\n",
      "        [ 2.9824, -1.7465, -2.2482]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7382, -1.7501, -2.2668],\n",
      "        [ 2.9824, -1.7465, -2.2482]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0725, -1.6137, -2.3545],\n",
      "        [ 2.8407, -1.6303, -2.2227]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0725, -1.6137, -2.3545],\n",
      "        [ 2.8407, -1.6303, -2.2227]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3395, -1.6878, -2.2140],\n",
      "        [ 3.2908, -1.8883, -2.4939]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3395, -1.6878, -2.2140],\n",
      "        [ 3.2908, -1.8883, -2.4939]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8152, -1.4542, -2.3425],\n",
      "        [ 2.8849, -1.4676, -2.4175]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8152, -1.4542, -2.3425],\n",
      "        [ 2.8849, -1.4676, -2.4175]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0656, -1.4421, -2.3609],\n",
      "        [ 2.8385, -1.6468, -2.4502]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0656, -1.4421, -2.3609],\n",
      "        [ 2.8385, -1.6468, -2.4502]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9556, -1.4511, -2.3188],\n",
      "        [ 3.0638, -1.5202, -2.0133]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9556, -1.4511, -2.3188],\n",
      "        [ 3.0638, -1.5202, -2.0133]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1520, -1.2854, -2.2285],\n",
      "        [ 2.7837, -1.4461, -2.3280]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1520, -1.2854, -2.2285],\n",
      "        [ 2.7837, -1.4461, -2.3280]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8441, -1.3680, -2.2836],\n",
      "        [ 3.0613, -1.5334, -2.1711]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8441, -1.3680, -2.2836],\n",
      "        [ 3.0613, -1.5334, -2.1711]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9174, -1.6530, -2.3604],\n",
      "        [ 2.9113, -1.6164, -2.4610]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9174, -1.6530, -2.3604],\n",
      "        [ 2.9113, -1.6164, -2.4610]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9829, -1.5969, -2.2299],\n",
      "        [ 3.2899, -1.3883, -2.5809]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9829, -1.5969, -2.2299],\n",
      "        [ 3.2899, -1.3883, -2.5809]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0026, -1.8231, -2.2620],\n",
      "        [ 2.9102, -1.7379, -2.2402]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0026, -1.8231, -2.2620],\n",
      "        [ 2.9102, -1.7379, -2.2402]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0355, -1.7279, -2.3747],\n",
      "        [ 2.9758, -1.6273, -2.1750]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0355, -1.7279, -2.3747],\n",
      "        [ 2.9758, -1.6273, -2.1750]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2565, -1.5315, -2.2564],\n",
      "        [ 3.0065, -1.8216, -2.1191]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2565, -1.5315, -2.2564],\n",
      "        [ 3.0065, -1.8216, -2.1191]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1344, -1.7459, -2.2629],\n",
      "        [ 3.0508, -1.5843, -2.3431]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1344, -1.7459, -2.2629],\n",
      "        [ 3.0508, -1.5843, -2.3431]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0408, -1.7073, -2.2958],\n",
      "        [ 2.9057, -1.6629, -2.2917]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0408, -1.7073, -2.2958],\n",
      "        [ 2.9057, -1.6629, -2.2917]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0444, -1.7131, -2.3249],\n",
      "        [ 3.1339, -1.8000, -2.6396]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0444, -1.7131, -2.3249],\n",
      "        [ 3.1339, -1.8000, -2.6396]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0986, -1.5464, -2.2341],\n",
      "        [ 3.1316, -1.5920, -2.4532]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0986, -1.5464, -2.2341],\n",
      "        [ 3.1316, -1.5920, -2.4532]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3163, -2.0663, -2.3849],\n",
      "        [ 3.2367, -1.9242, -2.4677]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3163, -2.0663, -2.3849],\n",
      "        [ 3.2367, -1.9242, -2.4677]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1918, -1.9410, -2.4427],\n",
      "        [ 3.2369, -1.9102, -2.3639]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1918, -1.9410, -2.4427],\n",
      "        [ 3.2369, -1.9102, -2.3639]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2516, -1.5890, -2.3415],\n",
      "        [ 3.3009, -1.8016, -2.4977]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2516, -1.5890, -2.3415],\n",
      "        [ 3.3009, -1.8016, -2.4977]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2201, -1.7619, -2.3047],\n",
      "        [ 3.2829, -1.8978, -2.5117]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2201, -1.7619, -2.3047],\n",
      "        [ 3.2829, -1.8978, -2.5117]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3664, -1.6898, -2.3730],\n",
      "        [ 3.1685, -1.7372, -2.3162]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3664, -1.6898, -2.3730],\n",
      "        [ 3.1685, -1.7372, -2.3162]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2920, -1.5991, -2.3266],\n",
      "        [ 3.0032, -1.8693, -2.3496]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2920, -1.5991, -2.3266],\n",
      "        [ 3.0032, -1.8693, -2.3496]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3623, -1.7302, -2.2174],\n",
      "        [ 3.1866, -1.4238, -2.3627]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3623, -1.7302, -2.2174],\n",
      "        [ 3.1866, -1.4238, -2.3627]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0429, -1.7054, -2.2599],\n",
      "        [ 2.9757, -1.5140, -2.2652]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0429, -1.7054, -2.2599],\n",
      "        [ 2.9757, -1.5140, -2.2652]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9348, -1.7416, -2.3388],\n",
      "        [ 2.9782, -1.6459, -2.4168]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9348, -1.7416, -2.3388],\n",
      "        [ 2.9782, -1.6459, -2.4168]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0525, -1.5272, -2.2675],\n",
      "        [ 2.9612, -1.7609, -2.5108]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0525, -1.5272, -2.2675],\n",
      "        [ 2.9612, -1.7609, -2.5108]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1126, -1.2786, -2.3294],\n",
      "        [ 2.9825, -1.4634, -2.3402]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1126, -1.2786, -2.3294],\n",
      "        [ 2.9825, -1.4634, -2.3402]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8638, -1.5075, -2.2410],\n",
      "        [ 2.8479, -1.4247, -2.2967]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8638, -1.5075, -2.2410],\n",
      "        [ 2.8479, -1.4247, -2.2967]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1629, -1.3806, -2.1468],\n",
      "        [ 2.9037, -1.4840, -2.0884]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1629, -1.3806, -2.1468],\n",
      "        [ 2.9037, -1.4840, -2.0884]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7992, -1.4056, -2.1328],\n",
      "        [ 2.7893, -1.4865, -2.0994]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7992, -1.4056, -2.1328],\n",
      "        [ 2.7893, -1.4865, -2.0994]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9269, -1.4416, -2.0430],\n",
      "        [ 2.9828, -1.7552, -2.2447]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9269, -1.4416, -2.0430],\n",
      "        [ 2.9828, -1.7552, -2.2447]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1975, -1.6034, -2.1800],\n",
      "        [ 2.9805, -1.2396, -2.1870]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1975, -1.6034, -2.1800],\n",
      "        [ 2.9805, -1.2396, -2.1870]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1944, -1.5900, -2.1567],\n",
      "        [ 2.7147, -1.5108, -2.1793]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1944, -1.5900, -2.1567],\n",
      "        [ 2.7147, -1.5108, -2.1793]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8513, -1.5239, -1.9598],\n",
      "        [ 3.1687, -1.2407, -2.3180]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8513, -1.5239, -1.9598],\n",
      "        [ 3.1687, -1.2407, -2.3180]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1449, -1.4552, -2.3412],\n",
      "        [ 2.9739, -1.7587, -2.0545]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1449, -1.4552, -2.3412],\n",
      "        [ 2.9739, -1.7587, -2.0545]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0577, -1.5805, -2.0525],\n",
      "        [ 3.2439, -1.6578, -2.2110]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0577, -1.5805, -2.0525],\n",
      "        [ 3.2439, -1.6578, -2.2110]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0345, -1.5756, -2.2170],\n",
      "        [ 2.9299, -1.7679, -2.2794]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0345, -1.5756, -2.2170],\n",
      "        [ 2.9299, -1.7679, -2.2794]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1016, -1.7423, -2.3495],\n",
      "        [ 3.0561, -1.6191, -2.5028]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1016, -1.7423, -2.3495],\n",
      "        [ 3.0561, -1.6191, -2.5028]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9553, -1.7066, -2.4584],\n",
      "        [ 3.2584, -1.7053, -2.4336]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9553, -1.7066, -2.4584],\n",
      "        [ 3.2584, -1.7053, -2.4336]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1959, -1.8358, -2.3180],\n",
      "        [ 3.0813, -1.6176, -2.3793]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1959, -1.8358, -2.3180],\n",
      "        [ 3.0813, -1.6176, -2.3793]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8792, -1.9827, -2.3535],\n",
      "        [ 3.3325, -1.7686, -2.1888]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8792, -1.9827, -2.3535],\n",
      "        [ 3.3325, -1.7686, -2.1888]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0907, -1.8246, -2.4287],\n",
      "        [ 3.4794, -2.1876, -2.4482]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0907, -1.8246, -2.4287],\n",
      "        [ 3.4794, -2.1876, -2.4482]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0312, -2.0869, -2.2458],\n",
      "        [ 3.2292, -1.8317, -2.0169]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0312, -2.0869, -2.2458],\n",
      "        [ 3.2292, -1.8317, -2.0169]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1381, -2.0524, -2.0995],\n",
      "        [ 3.0565, -1.7846, -2.2141]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1381, -2.0524, -2.0995],\n",
      "        [ 3.0565, -1.7846, -2.2141]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3982, -1.6545, -2.3844],\n",
      "        [ 3.1372, -2.0107, -2.2820]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3982, -1.6545, -2.3844],\n",
      "        [ 3.1372, -2.0107, -2.2820]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1644, -1.9660, -2.3495],\n",
      "        [ 3.5072, -2.1083, -2.1793]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1644, -1.9660, -2.3495],\n",
      "        [ 3.5072, -2.1083, -2.1793]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2913, -1.9172, -2.4398],\n",
      "        [ 3.2430, -2.0414, -2.2396]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2913, -1.9172, -2.4398],\n",
      "        [ 3.2430, -2.0414, -2.2396]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0041, -1.8416, -2.2932],\n",
      "        [ 3.1034, -2.0902, -2.0959]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0041, -1.8416, -2.2932],\n",
      "        [ 3.1034, -2.0902, -2.0959]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2075, -2.0966, -2.1424],\n",
      "        [ 3.2259, -1.7033, -2.2420]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2075, -2.0966, -2.1424],\n",
      "        [ 3.2259, -1.7033, -2.2420]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0722, -1.7387, -2.0562],\n",
      "        [ 3.1189, -1.6713, -2.2638]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0722, -1.7387, -2.0562],\n",
      "        [ 3.1189, -1.6713, -2.2638]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3888, -1.9446, -2.3197],\n",
      "        [ 2.9893, -2.0157, -2.1472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3888, -1.9446, -2.3197],\n",
      "        [ 2.9893, -2.0157, -2.1472]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0018, -1.8404, -2.2170],\n",
      "        [ 3.0920, -1.8246, -2.0171]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0018, -1.8404, -2.2170],\n",
      "        [ 3.0920, -1.8246, -2.0171]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0793, -2.0186, -1.8562],\n",
      "        [ 3.0637, -1.9028, -2.0052]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0793, -2.0186, -1.8562],\n",
      "        [ 3.0637, -1.9028, -2.0052]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2708, -1.9922, -1.9702],\n",
      "        [ 3.2044, -2.0795, -2.1169]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2708, -1.9922, -1.9702],\n",
      "        [ 3.2044, -2.0795, -2.1169]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0656, -1.7103, -2.0514],\n",
      "        [ 2.9823, -1.6074, -2.0006]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0656, -1.7103, -2.0514],\n",
      "        [ 2.9823, -1.6074, -2.0006]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9783, -1.6927, -2.1308],\n",
      "        [ 3.2163, -1.7998, -2.2250]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9783, -1.6927, -2.1308],\n",
      "        [ 3.2163, -1.7998, -2.2250]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9312, -1.5447, -2.2171],\n",
      "        [ 2.9262, -1.8332, -2.2483]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9312, -1.5447, -2.2171],\n",
      "        [ 2.9262, -1.8332, -2.2483]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8531, -1.8769, -2.0984],\n",
      "        [ 3.0122, -1.9445, -2.0311]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8531, -1.8769, -2.0984],\n",
      "        [ 3.0122, -1.9445, -2.0311]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9072, -2.0035, -2.4204],\n",
      "        [ 2.7951, -1.7151, -1.9033]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9072, -2.0035, -2.4204],\n",
      "        [ 2.7951, -1.7151, -1.9033]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8916, -1.8995, -2.0971],\n",
      "        [ 2.7988, -1.6345, -2.2596]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8916, -1.8995, -2.0971],\n",
      "        [ 2.7988, -1.6345, -2.2596]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9691, -1.7793, -2.2113],\n",
      "        [ 3.1422, -1.9149, -2.2950]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9691, -1.7793, -2.2113],\n",
      "        [ 3.1422, -1.9149, -2.2950]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2535, -1.7799, -2.1437],\n",
      "        [ 2.9299, -1.9783, -2.1640]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2535, -1.7799, -2.1437],\n",
      "        [ 2.9299, -1.9783, -2.1640]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9471, -1.5955, -2.3752],\n",
      "        [ 3.0963, -1.8931, -2.2919]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9471, -1.5955, -2.3752],\n",
      "        [ 3.0963, -1.8931, -2.2919]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2207, -1.8820, -2.0420],\n",
      "        [ 3.0879, -1.5760, -2.3312]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2207, -1.8820, -2.0420],\n",
      "        [ 3.0879, -1.5760, -2.3312]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0838, -2.1255, -2.3848],\n",
      "        [ 3.0823, -2.1627, -2.3425]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0838, -2.1255, -2.3848],\n",
      "        [ 3.0823, -2.1627, -2.3425]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0295, -1.9697, -2.2791],\n",
      "        [ 3.3418, -2.0377, -2.4389]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0295, -1.9697, -2.2791],\n",
      "        [ 3.3418, -2.0377, -2.4389]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0347, -1.7046, -2.0272],\n",
      "        [ 3.1464, -2.0481, -2.1925]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0347, -1.7046, -2.0272],\n",
      "        [ 3.1464, -2.0481, -2.1925]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8031, -1.7370, -2.3257],\n",
      "        [ 3.1859, -2.0355, -2.1693]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8031, -1.7370, -2.3257],\n",
      "        [ 3.1859, -2.0355, -2.1693]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4264, -1.7810, -2.4220],\n",
      "        [ 3.1292, -2.0919, -2.0664]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4264, -1.7810, -2.4220],\n",
      "        [ 3.1292, -2.0919, -2.0664]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3587, -2.0084, -2.1333],\n",
      "        [ 3.2418, -1.9070, -2.3793]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3587, -2.0084, -2.1333],\n",
      "        [ 3.2418, -1.9070, -2.3793]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1683, -2.2460, -2.2693],\n",
      "        [ 3.2599, -2.0634, -2.2903]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1683, -2.2460, -2.2693],\n",
      "        [ 3.2599, -2.0634, -2.2903]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1652, -2.0952, -2.1645],\n",
      "        [ 3.1372, -2.0439, -2.1650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1652, -2.0952, -2.1645],\n",
      "        [ 3.1372, -2.0439, -2.1650]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3653, -1.8017, -2.1528],\n",
      "        [ 3.1288, -2.0490, -2.1652]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3653, -1.8017, -2.1528],\n",
      "        [ 3.1288, -2.0490, -2.1652]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0773, -2.1623, -2.1641],\n",
      "        [ 3.1269, -1.9951, -2.0161]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0773, -2.1623, -2.1641],\n",
      "        [ 3.1269, -1.9951, -2.0161]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3521, -2.1191, -1.9923],\n",
      "        [ 3.1038, -1.8965, -2.1964]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3521, -2.1191, -1.9923],\n",
      "        [ 3.1038, -1.8965, -2.1964]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1630, -1.9431, -2.2809],\n",
      "        [ 3.5997, -2.3439, -2.3595]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1630, -1.9431, -2.2809],\n",
      "        [ 3.5997, -2.3439, -2.3595]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0893, -2.0674, -2.1513],\n",
      "        [ 3.3284, -2.2398, -2.1375]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0893, -2.0674, -2.1513],\n",
      "        [ 3.3284, -2.2398, -2.1375]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1438, -1.9780, -1.9913],\n",
      "        [ 3.4440, -2.0655, -2.1214]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1438, -1.9780, -1.9913],\n",
      "        [ 3.4440, -2.0655, -2.1214]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2537, -1.8550, -2.1704],\n",
      "        [ 3.3886, -1.8700, -2.1659]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2537, -1.8550, -2.1704],\n",
      "        [ 3.3886, -1.8700, -2.1659]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3962, -1.9786, -2.0852],\n",
      "        [ 3.3735, -2.2051, -2.3389]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3962, -1.9786, -2.0852],\n",
      "        [ 3.3735, -2.2051, -2.3389]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1105, -1.8242, -2.1591],\n",
      "        [ 3.3101, -1.9918, -2.0872]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1105, -1.8242, -2.1591],\n",
      "        [ 3.3101, -1.9918, -2.0872]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0518, -2.0103, -2.0557],\n",
      "        [ 3.1678, -2.0912, -2.4079]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0518, -2.0103, -2.0557],\n",
      "        [ 3.1678, -2.0912, -2.4079]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3172, -1.9626, -2.1099],\n",
      "        [ 3.2283, -1.7645, -2.3229]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3172, -1.9626, -2.1099],\n",
      "        [ 3.2283, -1.7645, -2.3229]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3943, -2.0416, -2.3660],\n",
      "        [ 3.4230, -1.9961, -2.1630]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3943, -2.0416, -2.3660],\n",
      "        [ 3.4230, -1.9961, -2.1630]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4966, -2.0176, -2.1575],\n",
      "        [ 3.2964, -1.8369, -2.4114]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4966, -2.0176, -2.1575],\n",
      "        [ 3.2964, -1.8369, -2.4114]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3179, -2.0962, -2.4460],\n",
      "        [ 3.1848, -1.9288, -2.3846]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3179, -2.0962, -2.4460],\n",
      "        [ 3.1848, -1.9288, -2.3846]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3166, -1.8550, -2.3012],\n",
      "        [ 3.2342, -1.6937, -2.4363]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3166, -1.8550, -2.3012],\n",
      "        [ 3.2342, -1.6937, -2.4363]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8678, -1.8435, -2.3056],\n",
      "        [ 3.5630, -2.2667, -2.3327]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8678, -1.8435, -2.3056],\n",
      "        [ 3.5630, -2.2667, -2.3327]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2065, -1.8791, -2.0671],\n",
      "        [ 3.3663, -1.7401, -2.3820]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2065, -1.8791, -2.0671],\n",
      "        [ 3.3663, -1.7401, -2.3820]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1105, -2.0027, -2.3227],\n",
      "        [ 3.0077, -1.8575, -2.0876]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1105, -2.0027, -2.3227],\n",
      "        [ 3.0077, -1.8575, -2.0876]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2960, -1.7848, -2.3701],\n",
      "        [ 3.3126, -2.0049, -2.3687]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2960, -1.7848, -2.3701],\n",
      "        [ 3.3126, -2.0049, -2.3687]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1382, -1.9682, -2.3352],\n",
      "        [ 3.4276, -1.9241, -2.2078]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1382, -1.9682, -2.3352],\n",
      "        [ 3.4276, -1.9241, -2.2078]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5549, -1.8730, -2.4012],\n",
      "        [ 3.0754, -1.6523, -2.2152]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5549, -1.8730, -2.4012],\n",
      "        [ 3.0754, -1.6523, -2.2152]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9987, -1.6935, -2.2708],\n",
      "        [ 3.0359, -1.6823, -2.3949]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9987, -1.6935, -2.2708],\n",
      "        [ 3.0359, -1.6823, -2.3949]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0479, -1.5884, -2.1662],\n",
      "        [ 3.0236, -1.4034, -2.1294]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0479, -1.5884, -2.1662],\n",
      "        [ 3.0236, -1.4034, -2.1294]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2391, -1.9899, -1.9898],\n",
      "        [ 3.0449, -1.3199, -2.1181]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2391, -1.9899, -1.9898],\n",
      "        [ 3.0449, -1.3199, -2.1181]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0604, -1.7524, -2.1983],\n",
      "        [ 2.9329, -1.6772, -2.0696]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0604, -1.7524, -2.1983],\n",
      "        [ 2.9329, -1.6772, -2.0696]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6993, -1.1401, -1.9708],\n",
      "        [ 2.9906, -1.6568, -2.1708]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6993, -1.1401, -1.9708],\n",
      "        [ 2.9906, -1.6568, -2.1708]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1649, -1.7082, -2.2240],\n",
      "        [ 3.1413, -1.6732, -2.0984]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1649, -1.7082, -2.2240],\n",
      "        [ 3.1413, -1.6732, -2.0984]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7771, -1.4685, -2.2246],\n",
      "        [ 2.9138, -1.5504, -2.0678]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7771, -1.4685, -2.2246],\n",
      "        [ 2.9138, -1.5504, -2.0678]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7908, -1.3573, -2.4533],\n",
      "        [ 2.7292, -1.5194, -2.2644]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7908, -1.3573, -2.4533],\n",
      "        [ 2.7292, -1.5194, -2.2644]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9914, -1.2746, -2.2471],\n",
      "        [ 2.8741, -1.1392, -2.1170]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9914, -1.2746, -2.2471],\n",
      "        [ 2.8741, -1.1392, -2.1170]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6836, -1.5069, -2.0753],\n",
      "        [ 2.4812, -0.9898, -2.1274]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6836, -1.5069, -2.0753],\n",
      "        [ 2.4812, -0.9898, -2.1274]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8343, -1.0665, -2.1465],\n",
      "        [ 2.6580, -1.5285, -2.0166]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8343, -1.0665, -2.1465],\n",
      "        [ 2.6580, -1.5285, -2.0166]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8937, -1.5862, -2.0241],\n",
      "        [ 2.8243, -1.3488, -1.8130]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8937, -1.5862, -2.0241],\n",
      "        [ 2.8243, -1.3488, -1.8130]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7788, -1.5316, -2.2537],\n",
      "        [ 2.7734, -1.4973, -2.1741]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7788, -1.5316, -2.2537],\n",
      "        [ 2.7734, -1.4973, -2.1741]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7095, -1.1413, -1.9855],\n",
      "        [ 2.9479, -1.3760, -2.2273]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7095, -1.1413, -1.9855],\n",
      "        [ 2.9479, -1.3760, -2.2273]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8329, -1.1529, -1.9770],\n",
      "        [ 2.9638, -1.6085, -2.3999]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8329, -1.1529, -1.9770],\n",
      "        [ 2.9638, -1.6085, -2.3999]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5734, -1.3189, -2.0530],\n",
      "        [ 2.6176, -1.0346, -2.1673]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.5734, -1.3189, -2.0530],\n",
      "        [ 2.6176, -1.0346, -2.1673]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6742, -0.9199, -1.8402],\n",
      "        [ 2.5326, -0.9690, -2.1167]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6742, -0.9199, -1.8402],\n",
      "        [ 2.5326, -0.9690, -2.1167]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0085, -1.2240, -2.0940],\n",
      "        [ 2.6287, -1.0252, -1.9315]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0085, -1.2240, -2.0940],\n",
      "        [ 2.6287, -1.0252, -1.9315]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8105, -1.5074, -1.9564],\n",
      "        [ 2.9121, -1.4485, -2.3725]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8105, -1.5074, -1.9564],\n",
      "        [ 2.9121, -1.4485, -2.3725]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8938, -1.5490, -2.2432],\n",
      "        [ 2.6026, -0.6965, -1.9499]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8938, -1.5490, -2.2432],\n",
      "        [ 2.6026, -0.6965, -1.9499]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9099, -1.4972, -2.3279],\n",
      "        [ 2.4370, -1.4670, -1.9847]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9099, -1.4972, -2.3279],\n",
      "        [ 2.4370, -1.4670, -1.9847]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7861, -1.1764, -2.2665],\n",
      "        [ 2.7897, -1.3636, -2.0424]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7861, -1.1764, -2.2665],\n",
      "        [ 2.7897, -1.3636, -2.0424]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6055, -1.5443, -1.7425],\n",
      "        [ 2.6533, -1.4774, -1.6599]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6055, -1.5443, -1.7425],\n",
      "        [ 2.6533, -1.4774, -1.6599]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7517, -1.8855, -2.0192],\n",
      "        [ 2.7158, -1.4144, -2.2228]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7517, -1.8855, -2.0192],\n",
      "        [ 2.7158, -1.4144, -2.2228]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5545, -1.2929, -1.7076],\n",
      "        [ 2.6151, -1.5961, -1.9147]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.5545, -1.2929, -1.7076],\n",
      "        [ 2.6151, -1.5961, -1.9147]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4123, -0.9221, -2.1115],\n",
      "        [ 2.5141, -1.1243, -1.7595]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.4123, -0.9221, -2.1115],\n",
      "        [ 2.5141, -1.1243, -1.7595]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4982, -0.9750, -1.8251],\n",
      "        [ 2.8271, -1.1305, -1.8841]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.4982, -0.9750, -1.8251],\n",
      "        [ 2.8271, -1.1305, -1.8841]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9625, -1.8727, -2.1366],\n",
      "        [ 3.0721, -1.9025, -2.0645]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9625, -1.8727, -2.1366],\n",
      "        [ 3.0721, -1.9025, -2.0645]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8538, -1.2345, -1.8625],\n",
      "        [ 2.8459, -1.4971, -1.9332]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8538, -1.2345, -1.8625],\n",
      "        [ 2.8459, -1.4971, -1.9332]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6029, -1.4758, -1.9264],\n",
      "        [ 2.6690, -1.6092, -2.0667]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6029, -1.4758, -1.9264],\n",
      "        [ 2.6690, -1.6092, -2.0667]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0576, -1.7743, -2.2715],\n",
      "        [ 2.9226, -1.6267, -2.1692]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0576, -1.7743, -2.2715],\n",
      "        [ 2.9226, -1.6267, -2.1692]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7942, -1.4726, -2.1235],\n",
      "        [ 2.8297, -1.5047, -2.0612]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7942, -1.4726, -2.1235],\n",
      "        [ 2.8297, -1.5047, -2.0612]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1640, -2.0005, -1.8826],\n",
      "        [ 2.7938, -1.4354, -2.1720]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1640, -2.0005, -1.8826],\n",
      "        [ 2.7938, -1.4354, -2.1720]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0409, -1.6560, -1.9718],\n",
      "        [ 3.0359, -2.3033, -2.0681]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0409, -1.6560, -1.9718],\n",
      "        [ 3.0359, -2.3033, -2.0681]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2624, -1.9151, -1.8952],\n",
      "        [ 3.1729, -1.8678, -2.0799]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2624, -1.9151, -1.8952],\n",
      "        [ 3.1729, -1.8678, -2.0799]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0405, -2.2679, -1.9648],\n",
      "        [ 2.9911, -1.5791, -2.1242]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0405, -2.2679, -1.9648],\n",
      "        [ 2.9911, -1.5791, -2.1242]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1825, -2.2763, -2.4221],\n",
      "        [ 2.9121, -1.7861, -2.1408]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1825, -2.2763, -2.4221],\n",
      "        [ 2.9121, -1.7861, -2.1408]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1149, -1.9023, -2.3035],\n",
      "        [ 3.2174, -2.2174, -2.1457]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1149, -1.9023, -2.3035],\n",
      "        [ 3.2174, -2.2174, -2.1457]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5270, -2.5428, -2.0432],\n",
      "        [ 3.0910, -1.7932, -2.4339]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5270, -2.5428, -2.0432],\n",
      "        [ 3.0910, -1.7932, -2.4339]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3726, -2.3326, -2.0417],\n",
      "        [ 3.5257, -2.3199, -2.1632]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3726, -2.3326, -2.0417],\n",
      "        [ 3.5257, -2.3199, -2.1632]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1878, -2.2000, -2.0178],\n",
      "        [ 3.5206, -2.3787, -2.0700]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1878, -2.2000, -2.0178],\n",
      "        [ 3.5206, -2.3787, -2.0700]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3243, -2.1694, -2.0416],\n",
      "        [ 3.0556, -2.0865, -2.0853]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3243, -2.1694, -2.0416],\n",
      "        [ 3.0556, -2.0865, -2.0853]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1432, -2.2727, -2.2169],\n",
      "        [ 3.5403, -2.2773, -2.3943]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1432, -2.2727, -2.2169],\n",
      "        [ 3.5403, -2.2773, -2.3943]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3497, -2.4512, -1.9121],\n",
      "        [ 3.3966, -2.5578, -2.3312]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3497, -2.4512, -1.9121],\n",
      "        [ 3.3966, -2.5578, -2.3312]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4461, -2.1604, -2.3511],\n",
      "        [ 3.5015, -2.2062, -2.3598]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4461, -2.1604, -2.3511],\n",
      "        [ 3.5015, -2.2062, -2.3598]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5376, -2.4190, -2.3265],\n",
      "        [ 3.3772, -2.2316, -2.1469]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5376, -2.4190, -2.3265],\n",
      "        [ 3.3772, -2.2316, -2.1469]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5956, -2.6104, -2.4054],\n",
      "        [ 3.4220, -2.4414, -2.3675]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5956, -2.6104, -2.4054],\n",
      "        [ 3.4220, -2.4414, -2.3675]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5295, -2.1258, -2.3073],\n",
      "        [ 3.4739, -2.3507, -2.2933]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5295, -2.1258, -2.3073],\n",
      "        [ 3.4739, -2.3507, -2.2933]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6385, -2.2074, -2.1334],\n",
      "        [ 3.6787, -2.2477, -2.2685]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6385, -2.2074, -2.1334],\n",
      "        [ 3.6787, -2.2477, -2.2685]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5013, -2.1519, -2.0988],\n",
      "        [ 3.6694, -2.5166, -2.2225]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5013, -2.1519, -2.0988],\n",
      "        [ 3.6694, -2.5166, -2.2225]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4928, -1.9978, -2.5611],\n",
      "        [ 3.4073, -2.3469, -2.4859]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4928, -1.9978, -2.5611],\n",
      "        [ 3.4073, -2.3469, -2.4859]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4652, -2.1385, -2.4894],\n",
      "        [ 3.4711, -2.2900, -2.4525]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4652, -2.1385, -2.4894],\n",
      "        [ 3.4711, -2.2900, -2.4525]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7606, -2.3944, -2.5551],\n",
      "        [ 3.3862, -2.1423, -2.2716]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7606, -2.3944, -2.5551],\n",
      "        [ 3.3862, -2.1423, -2.2716]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5029, -2.3393, -2.5444],\n",
      "        [ 3.5053, -2.2064, -2.1617]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5029, -2.3393, -2.5444],\n",
      "        [ 3.5053, -2.2064, -2.1617]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6379, -2.5559, -2.3696],\n",
      "        [ 3.4333, -2.2947, -2.2277]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6379, -2.5559, -2.3696],\n",
      "        [ 3.4333, -2.2947, -2.2277]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1868, -2.1034, -1.9483],\n",
      "        [ 3.5441, -2.3643, -2.1599]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1868, -2.1034, -1.9483],\n",
      "        [ 3.5441, -2.3643, -2.1599]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4501, -2.2466, -1.9369],\n",
      "        [ 3.5222, -2.4191, -2.2251]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4501, -2.2466, -1.9369],\n",
      "        [ 3.5222, -2.4191, -2.2251]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4948, -2.4409, -2.2119],\n",
      "        [ 3.5768, -2.3433, -2.1180]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4948, -2.4409, -2.2119],\n",
      "        [ 3.5768, -2.3433, -2.1180]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5293, -2.4013, -2.2123],\n",
      "        [ 3.3013, -2.4551, -2.0125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5293, -2.4013, -2.2123],\n",
      "        [ 3.3013, -2.4551, -2.0125]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4154, -2.3062, -2.2987],\n",
      "        [ 3.4313, -2.4641, -2.0514]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4154, -2.3062, -2.2987],\n",
      "        [ 3.4313, -2.4641, -2.0514]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0157, -2.5465, -1.6576],\n",
      "        [ 3.3967, -2.5106, -2.5338]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0157, -2.5465, -1.6576],\n",
      "        [ 3.3967, -2.5106, -2.5338]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0420, -2.4984, -1.8517],\n",
      "        [ 3.2068, -2.3883, -1.6767]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0420, -2.4984, -1.8517],\n",
      "        [ 3.2068, -2.3883, -1.6767]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8644, -2.3527, -1.6430],\n",
      "        [ 3.1730, -2.2689, -1.5624]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8644, -2.3527, -1.6430],\n",
      "        [ 3.1730, -2.2689, -1.5624]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6573, -2.1777, -1.4886],\n",
      "        [ 3.0095, -2.4315, -1.5616]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6573, -2.1777, -1.4886],\n",
      "        [ 3.0095, -2.4315, -1.5616]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4114, -2.2953, -1.1895],\n",
      "        [ 2.9858, -2.1583, -1.6754]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.4114, -2.2953, -1.1895],\n",
      "        [ 2.9858, -2.1583, -1.6754]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5907, -2.4804, -1.1645],\n",
      "        [ 2.8321, -2.2286, -1.0679]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.5907, -2.4804, -1.1645],\n",
      "        [ 2.8321, -2.2286, -1.0679]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1847, -2.2839, -1.9150],\n",
      "        [ 2.8181, -2.3751, -1.4313]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1847, -2.2839, -1.9150],\n",
      "        [ 2.8181, -2.3751, -1.4313]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0841, -2.3242, -1.8513],\n",
      "        [ 2.5450, -2.2480, -1.1847]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0841, -2.3242, -1.8513],\n",
      "        [ 2.5450, -2.2480, -1.1847]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8152, -2.5379, -1.5073],\n",
      "        [ 2.7428, -2.4138, -1.3312]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8152, -2.5379, -1.5073],\n",
      "        [ 2.7428, -2.4138, -1.3312]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8932, -2.5126, -1.6066],\n",
      "        [ 2.5746, -2.2013, -1.3480]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8932, -2.5126, -1.6066],\n",
      "        [ 2.5746, -2.2013, -1.3480]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9272, -2.3408, -1.1897],\n",
      "        [ 3.2074, -2.4822, -1.9569]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9272, -2.3408, -1.1897],\n",
      "        [ 3.2074, -2.4822, -1.9569]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4328, -2.0529, -1.9600],\n",
      "        [ 3.1966, -2.4604, -1.4924]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4328, -2.0529, -1.9600],\n",
      "        [ 3.1966, -2.4604, -1.4924]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1925, -2.4747, -1.4593],\n",
      "        [ 3.2208, -2.5830, -1.7417]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1925, -2.4747, -1.4593],\n",
      "        [ 3.2208, -2.5830, -1.7417]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1737, -2.3600, -1.6949],\n",
      "        [ 3.1976, -2.4747, -1.6137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1737, -2.3600, -1.6949],\n",
      "        [ 3.1976, -2.4747, -1.6137]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4369, -2.5076, -2.0043],\n",
      "        [ 3.6775, -2.7206, -2.0805]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4369, -2.5076, -2.0043],\n",
      "        [ 3.6775, -2.7206, -2.0805]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3363, -2.2631, -2.2164],\n",
      "        [ 3.3055, -2.4429, -2.0509]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3363, -2.2631, -2.2164],\n",
      "        [ 3.3055, -2.4429, -2.0509]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4157, -2.7303, -2.2391],\n",
      "        [ 3.7313, -2.3898, -2.2213]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4157, -2.7303, -2.2391],\n",
      "        [ 3.7313, -2.3898, -2.2213]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7170, -2.4681, -2.2264],\n",
      "        [ 3.7390, -2.5615, -2.1268]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7170, -2.4681, -2.2264],\n",
      "        [ 3.7390, -2.5615, -2.1268]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4240, -2.1077, -2.0361],\n",
      "        [ 3.8403, -2.6992, -2.2466]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4240, -2.1077, -2.0361],\n",
      "        [ 3.8403, -2.6992, -2.2466]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4389, -2.5568, -2.1330],\n",
      "        [ 3.1750, -1.9313, -2.2006]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4389, -2.5568, -2.1330],\n",
      "        [ 3.1750, -1.9313, -2.2006]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6272, -2.2750, -2.2941],\n",
      "        [ 3.5521, -2.6022, -2.1100]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6272, -2.2750, -2.2941],\n",
      "        [ 3.5521, -2.6022, -2.1100]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2899, -2.2421, -2.2933],\n",
      "        [ 3.2952, -2.5564, -2.0617]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2899, -2.2421, -2.2933],\n",
      "        [ 3.2952, -2.5564, -2.0617]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5320, -2.4681, -2.3046],\n",
      "        [ 3.5111, -2.3828, -2.2418]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5320, -2.4681, -2.3046],\n",
      "        [ 3.5111, -2.3828, -2.2418]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4911, -1.6861, -1.8737],\n",
      "        [ 3.3967, -1.9483, -2.1823]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4911, -1.6861, -1.8737],\n",
      "        [ 3.3967, -1.9483, -2.1823]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4705, -2.1631, -1.9921],\n",
      "        [ 3.2819, -2.1945, -2.2482]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4705, -2.1631, -1.9921],\n",
      "        [ 3.2819, -2.1945, -2.2482]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4338, -2.2162, -2.0694],\n",
      "        [ 3.7494, -2.2047, -2.1024]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4338, -2.2162, -2.0694],\n",
      "        [ 3.7494, -2.2047, -2.1024]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7039, -2.2289, -2.0920],\n",
      "        [ 3.8378, -2.1405, -2.2205]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7039, -2.2289, -2.0920],\n",
      "        [ 3.8378, -2.1405, -2.2205]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4555, -2.1763, -2.1183],\n",
      "        [ 3.5934, -1.9494, -2.1510]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4555, -2.1763, -2.1183],\n",
      "        [ 3.5934, -1.9494, -2.1510]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7247, -2.0265, -2.1303],\n",
      "        [ 3.3931, -1.8940, -2.0729]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7247, -2.0265, -2.1303],\n",
      "        [ 3.3931, -1.8940, -2.0729]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7820, -2.0874, -2.1785],\n",
      "        [ 3.3791, -2.1938, -2.0701]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7820, -2.0874, -2.1785],\n",
      "        [ 3.3791, -2.1938, -2.0701]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4985, -1.9466, -2.3716],\n",
      "        [ 3.3397, -1.9000, -2.2875]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4985, -1.9466, -2.3716],\n",
      "        [ 3.3397, -1.9000, -2.2875]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5412, -2.2403, -2.1061],\n",
      "        [ 3.5000, -1.9599, -2.4548]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5412, -2.2403, -2.1061],\n",
      "        [ 3.5000, -1.9599, -2.4548]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3668, -1.5852, -2.2185],\n",
      "        [ 3.2479, -2.0039, -2.2967]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3668, -1.5852, -2.2185],\n",
      "        [ 3.2479, -2.0039, -2.2967]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4226, -1.9939, -2.1770],\n",
      "        [ 3.4191, -1.9918, -1.9420]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4226, -1.9939, -2.1770],\n",
      "        [ 3.4191, -1.9918, -1.9420]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6492, -1.8969, -2.2097],\n",
      "        [ 3.5492, -1.9812, -2.0659]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6492, -1.8969, -2.2097],\n",
      "        [ 3.5492, -1.9812, -2.0659]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5470, -1.9119, -2.1054],\n",
      "        [ 3.3547, -1.9810, -2.1272]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5470, -1.9119, -2.1054],\n",
      "        [ 3.3547, -1.9810, -2.1272]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7394, -1.9632, -2.2009],\n",
      "        [ 3.3368, -1.9546, -2.2590]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7394, -1.9632, -2.2009],\n",
      "        [ 3.3368, -1.9546, -2.2590]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5680, -2.2261, -2.1105],\n",
      "        [ 3.5369, -1.8212, -2.2025]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5680, -2.2261, -2.1105],\n",
      "        [ 3.5369, -1.8212, -2.2025]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5655, -1.9176, -2.2287],\n",
      "        [ 3.6649, -1.8582, -2.2877]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5655, -1.9176, -2.2287],\n",
      "        [ 3.6649, -1.8582, -2.2877]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1358, -1.5224, -2.0762],\n",
      "        [ 3.1233, -1.4889, -2.3013]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1358, -1.5224, -2.0762],\n",
      "        [ 3.1233, -1.4889, -2.3013]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5449, -1.6024, -2.1200],\n",
      "        [ 3.4601, -1.6515, -2.2621]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5449, -1.6024, -2.1200],\n",
      "        [ 3.4601, -1.6515, -2.2621]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0881, -1.6641, -2.2147],\n",
      "        [ 3.2345, -1.6060, -1.9962]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0881, -1.6641, -2.2147],\n",
      "        [ 3.2345, -1.6060, -1.9962]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0376, -1.8308, -2.1140],\n",
      "        [ 3.3894, -1.6398, -2.0918]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0376, -1.8308, -2.1140],\n",
      "        [ 3.3894, -1.6398, -2.0918]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6092, -1.6162, -2.1627],\n",
      "        [ 2.8257, -1.6046, -2.0612]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6092, -1.6162, -2.1627],\n",
      "        [ 2.8257, -1.6046, -2.0612]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2209, -1.2685, -2.1910],\n",
      "        [ 3.2613, -1.5535, -1.8881]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2209, -1.2685, -2.1910],\n",
      "        [ 3.2613, -1.5535, -1.8881]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3436, -1.6392, -2.1137],\n",
      "        [ 3.1229, -1.4033, -1.7986]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3436, -1.6392, -2.1137],\n",
      "        [ 3.1229, -1.4033, -1.7986]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8916, -1.5005, -1.8275],\n",
      "        [ 3.1279, -1.6523, -1.8591]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8916, -1.5005, -1.8275],\n",
      "        [ 3.1279, -1.6523, -1.8591]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1162, -1.4585, -2.0494],\n",
      "        [ 2.9796, -1.5691, -1.9188]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1162, -1.4585, -2.0494],\n",
      "        [ 2.9796, -1.5691, -1.9188]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1067, -1.4891, -1.7948],\n",
      "        [ 3.0852, -1.4698, -2.1002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1067, -1.4891, -1.7948],\n",
      "        [ 3.0852, -1.4698, -2.1002]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0483, -1.5882, -1.9661],\n",
      "        [ 3.0115, -1.2755, -2.0131]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0483, -1.5882, -1.9661],\n",
      "        [ 3.0115, -1.2755, -2.0131]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1298, -1.4335, -2.0722],\n",
      "        [ 2.8389, -0.9934, -1.8075]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1298, -1.4335, -2.0722],\n",
      "        [ 2.8389, -0.9934, -1.8075]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0283, -1.3281, -2.1507],\n",
      "        [ 2.8563, -1.4829, -2.1093]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0283, -1.3281, -2.1507],\n",
      "        [ 2.8563, -1.4829, -2.1093]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7068, -1.0969, -1.9026],\n",
      "        [ 3.1712, -1.3174, -2.1042]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7068, -1.0969, -1.9026],\n",
      "        [ 3.1712, -1.3174, -2.1042]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0396, -1.3839, -1.9716],\n",
      "        [ 3.0920, -1.8776, -1.9351]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0396, -1.3839, -1.9716],\n",
      "        [ 3.0920, -1.8776, -1.9351]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4580, -1.6302, -2.0548],\n",
      "        [ 3.1070, -1.5416, -2.1672]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4580, -1.6302, -2.0548],\n",
      "        [ 3.1070, -1.5416, -2.1672]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6353, -0.9814, -2.0290],\n",
      "        [ 2.7968, -1.0749, -2.0435]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6353, -0.9814, -2.0290],\n",
      "        [ 2.7968, -1.0749, -2.0435]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3428, -1.6831, -1.9329],\n",
      "        [ 3.2427, -1.5684, -2.1721]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3428, -1.6831, -1.9329],\n",
      "        [ 3.2427, -1.5684, -2.1721]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6965, -1.1481, -2.1062],\n",
      "        [ 3.0449, -1.5432, -1.9480]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6965, -1.1481, -2.1062],\n",
      "        [ 3.0449, -1.5432, -1.9480]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0731, -1.3525, -2.1520],\n",
      "        [ 3.2561, -1.5776, -2.2431]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0731, -1.3525, -2.1520],\n",
      "        [ 3.2561, -1.5776, -2.2431]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8308, -1.2062, -2.1104],\n",
      "        [ 2.8813, -1.6433, -1.8755]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8308, -1.2062, -2.1104],\n",
      "        [ 2.8813, -1.6433, -1.8755]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1193, -1.2824, -2.1973],\n",
      "        [ 3.1465, -1.4903, -1.8503]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1193, -1.2824, -2.1973],\n",
      "        [ 3.1465, -1.4903, -1.8503]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8543, -1.6822, -1.7567],\n",
      "        [ 2.9056, -1.2216, -2.0593]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8543, -1.6822, -1.7567],\n",
      "        [ 2.9056, -1.2216, -2.0593]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0442, -1.6484, -2.0272],\n",
      "        [ 2.9760, -1.2791, -1.9953]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0442, -1.6484, -2.0272],\n",
      "        [ 2.9760, -1.2791, -1.9953]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7524, -1.5135, -1.9477],\n",
      "        [ 2.9371, -1.5053, -1.9819]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7524, -1.5135, -1.9477],\n",
      "        [ 2.9371, -1.5053, -1.9819]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0427, -1.6723, -1.8158],\n",
      "        [ 3.2402, -1.6069, -1.9657]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0427, -1.6723, -1.8158],\n",
      "        [ 3.2402, -1.6069, -1.9657]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1898, -1.8104, -2.0865],\n",
      "        [ 3.0607, -1.6807, -2.1227]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1898, -1.8104, -2.0865],\n",
      "        [ 3.0607, -1.6807, -2.1227]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9254, -1.1478, -2.0584],\n",
      "        [ 2.6394, -1.3440, -1.8895]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9254, -1.1478, -2.0584],\n",
      "        [ 2.6394, -1.3440, -1.8895]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8503, -1.4300, -1.9569],\n",
      "        [ 3.0043, -1.5997, -2.0515]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8503, -1.4300, -1.9569],\n",
      "        [ 3.0043, -1.5997, -2.0515]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1108, -1.8137, -1.7784],\n",
      "        [ 3.0303, -1.5345, -2.0808]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1108, -1.8137, -1.7784],\n",
      "        [ 3.0303, -1.5345, -2.0808]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7800, -1.4622, -1.5413],\n",
      "        [ 3.1851, -1.6837, -1.9476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7800, -1.4622, -1.5413],\n",
      "        [ 3.1851, -1.6837, -1.9476]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8794, -1.3351, -1.9392],\n",
      "        [ 3.2648, -1.6425, -2.1514]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8794, -1.3351, -1.9392],\n",
      "        [ 3.2648, -1.6425, -2.1514]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0319, -1.1660, -2.1421],\n",
      "        [ 3.2487, -1.6170, -1.8341]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0319, -1.1660, -2.1421],\n",
      "        [ 3.2487, -1.6170, -1.8341]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1588, -1.7915, -1.9496],\n",
      "        [ 3.2362, -2.0522, -2.0691]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1588, -1.7915, -1.9496],\n",
      "        [ 3.2362, -2.0522, -2.0691]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2004, -1.8715, -1.7543],\n",
      "        [ 2.9160, -1.4934, -1.5822]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2004, -1.8715, -1.7543],\n",
      "        [ 2.9160, -1.4934, -1.5822]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2217, -1.9053, -2.0882],\n",
      "        [ 3.0864, -1.5697, -2.1482]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2217, -1.9053, -2.0882],\n",
      "        [ 3.0864, -1.5697, -2.1482]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3390, -1.8547, -1.8940],\n",
      "        [ 3.3216, -1.8654, -2.0718]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3390, -1.8547, -1.8940],\n",
      "        [ 3.3216, -1.8654, -2.0718]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3415, -2.1065, -2.0533],\n",
      "        [ 3.3439, -1.5427, -1.9980]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3415, -2.1065, -2.0533],\n",
      "        [ 3.3439, -1.5427, -1.9980]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4469, -1.9185, -2.2207],\n",
      "        [ 3.3903, -1.8504, -2.0088]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4469, -1.9185, -2.2207],\n",
      "        [ 3.3903, -1.8504, -2.0088]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2945, -1.8907, -1.9317],\n",
      "        [ 2.8833, -1.7501, -2.0157]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2945, -1.8907, -1.9317],\n",
      "        [ 2.8833, -1.7501, -2.0157]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3927, -2.3247, -2.1577],\n",
      "        [ 3.6197, -2.2377, -2.2589]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3927, -2.3247, -2.1577],\n",
      "        [ 3.6197, -2.2377, -2.2589]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4120, -2.1520, -2.0266],\n",
      "        [ 3.4990, -2.1697, -2.1039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4120, -2.1520, -2.0266],\n",
      "        [ 3.4990, -2.1697, -2.1039]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3991, -1.7710, -2.2521],\n",
      "        [ 3.1543, -2.1834, -1.9329]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3991, -1.7710, -2.2521],\n",
      "        [ 3.1543, -2.1834, -1.9329]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4390, -1.9763, -2.2344],\n",
      "        [ 3.5136, -1.8777, -2.2204]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4390, -1.9763, -2.2344],\n",
      "        [ 3.5136, -1.8777, -2.2204]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4050, -2.3432, -2.2721],\n",
      "        [ 3.3826, -2.4171, -2.2423]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4050, -2.3432, -2.2721],\n",
      "        [ 3.3826, -2.4171, -2.2423]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3856, -2.0473, -2.1041],\n",
      "        [ 3.2252, -1.8789, -2.2555]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3856, -2.0473, -2.1041],\n",
      "        [ 3.2252, -1.8789, -2.2555]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3369, -2.1467, -2.0382],\n",
      "        [ 3.3863, -2.2187, -2.1002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3369, -2.1467, -2.0382],\n",
      "        [ 3.3863, -2.2187, -2.1002]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4710, -2.2337, -2.2161],\n",
      "        [ 3.3872, -2.2374, -1.9480]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4710, -2.2337, -2.2161],\n",
      "        [ 3.3872, -2.2374, -1.9480]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2042, -2.0166, -2.2345],\n",
      "        [ 3.0687, -2.2504, -2.0811]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2042, -2.0166, -2.2345],\n",
      "        [ 3.0687, -2.2504, -2.0811]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3992, -2.0366, -2.2574],\n",
      "        [ 3.6250, -2.1066, -2.2199]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3992, -2.0366, -2.2574],\n",
      "        [ 3.6250, -2.1066, -2.2199]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8907, -1.8057, -2.0548],\n",
      "        [ 3.4419, -2.3129, -2.0544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8907, -1.8057, -2.0548],\n",
      "        [ 3.4419, -2.3129, -2.0544]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7417, -2.3213, -2.4020],\n",
      "        [ 3.4266, -2.1870, -2.1802]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7417, -2.3213, -2.4020],\n",
      "        [ 3.4266, -2.1870, -2.1802]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5387, -2.0754, -2.1775],\n",
      "        [ 3.7047, -2.0925, -2.1229]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5387, -2.0754, -2.1775],\n",
      "        [ 3.7047, -2.0925, -2.1229]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5303, -1.9452, -2.1955],\n",
      "        [ 3.6306, -2.4711, -2.1532]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5303, -1.9452, -2.1955],\n",
      "        [ 3.6306, -2.4711, -2.1532]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5320, -2.0879, -2.3477],\n",
      "        [ 3.8246, -2.6020, -1.9227]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5320, -2.0879, -2.3477],\n",
      "        [ 3.8246, -2.6020, -1.9227]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3374, -2.2004, -2.1079],\n",
      "        [ 3.1906, -1.9394, -2.2797]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3374, -2.2004, -2.1079],\n",
      "        [ 3.1906, -1.9394, -2.2797]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9784, -1.6825, -2.2917],\n",
      "        [ 3.6110, -2.1064, -2.2292]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9784, -1.6825, -2.2917],\n",
      "        [ 3.6110, -2.1064, -2.2292]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6287, -2.4594, -2.2099],\n",
      "        [ 3.7813, -2.5894, -2.3547]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6287, -2.4594, -2.2099],\n",
      "        [ 3.7813, -2.5894, -2.3547]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5584, -2.2439, -2.2628],\n",
      "        [ 3.3098, -1.9236, -2.1960]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5584, -2.2439, -2.2628],\n",
      "        [ 3.3098, -1.9236, -2.1960]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8857, -2.2593, -2.2263],\n",
      "        [ 3.5099, -2.0979, -2.4054]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8857, -2.2593, -2.2263],\n",
      "        [ 3.5099, -2.0979, -2.4054]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4756, -2.3782, -2.2340],\n",
      "        [ 3.5117, -2.0556, -2.3358]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4756, -2.3782, -2.2340],\n",
      "        [ 3.5117, -2.0556, -2.3358]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7356, -2.2572, -2.2274],\n",
      "        [ 3.8211, -2.1642, -2.4506]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7356, -2.2572, -2.2274],\n",
      "        [ 3.8211, -2.1642, -2.4506]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0871, -1.4316, -2.2914],\n",
      "        [ 3.6749, -2.2964, -2.4483]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0871, -1.4316, -2.2914],\n",
      "        [ 3.6749, -2.2964, -2.4483]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3977, -2.1269, -2.5055],\n",
      "        [ 3.8450, -2.3057, -2.3394]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3977, -2.1269, -2.5055],\n",
      "        [ 3.8450, -2.3057, -2.3394]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5435, -2.3983, -2.4332],\n",
      "        [ 3.6502, -2.2077, -2.3947]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5435, -2.3983, -2.4332],\n",
      "        [ 3.6502, -2.2077, -2.3947]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8418, -1.5495, -2.3066],\n",
      "        [ 3.7715, -2.3454, -2.5279]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8418, -1.5495, -2.3066],\n",
      "        [ 3.7715, -2.3454, -2.5279]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3567, -1.9969, -2.4460],\n",
      "        [ 3.0735, -1.5036, -2.1109]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3567, -1.9969, -2.4460],\n",
      "        [ 3.0735, -1.5036, -2.1109]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3181, -1.3775, -2.3098],\n",
      "        [ 3.4365, -1.9492, -2.2279]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3181, -1.3775, -2.3098],\n",
      "        [ 3.4365, -1.9492, -2.2279]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4471, -2.0877, -2.4290],\n",
      "        [ 2.9365, -1.2391, -2.2354]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4471, -2.0877, -2.4290],\n",
      "        [ 2.9365, -1.2391, -2.2354]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9384, -1.5691, -2.2063],\n",
      "        [ 3.6474, -2.3285, -2.5087]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9384, -1.5691, -2.2063],\n",
      "        [ 3.6474, -2.3285, -2.5087]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6110, -1.9340, -2.4021],\n",
      "        [ 3.3216, -1.9677, -2.2405]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6110, -1.9340, -2.4021],\n",
      "        [ 3.3216, -1.9677, -2.2405]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4034, -1.0228, -1.8568],\n",
      "        [ 3.0706, -1.3046, -2.3287]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.4034, -1.0228, -1.8568],\n",
      "        [ 3.0706, -1.3046, -2.3287]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1737, -1.5015, -2.2665],\n",
      "        [ 2.8842, -1.0378, -2.3076]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1737, -1.5015, -2.2665],\n",
      "        [ 2.8842, -1.0378, -2.3076]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9753, -1.3626, -2.1611],\n",
      "        [ 3.4829, -1.9713, -2.4615]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9753, -1.3626, -2.1611],\n",
      "        [ 3.4829, -1.9713, -2.4615]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8284, -1.5548, -2.1357],\n",
      "        [ 3.3950, -1.5805, -2.3653]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8284, -1.5548, -2.1357],\n",
      "        [ 3.3950, -1.5805, -2.3653]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3334, -1.7301, -2.2973],\n",
      "        [ 2.8806, -0.7777, -2.1566]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3334, -1.7301, -2.2973],\n",
      "        [ 2.8806, -0.7777, -2.1566]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2705, -0.4558, -1.7481],\n",
      "        [ 2.7585, -1.2443, -2.4317]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.2705, -0.4558, -1.7481],\n",
      "        [ 2.7585, -1.2443, -2.4317]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3355, -0.7809, -2.3728],\n",
      "        [ 2.9989, -1.4902, -2.2388]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.3355, -0.7809, -2.3728],\n",
      "        [ 2.9989, -1.4902, -2.2388]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2424, -1.6007, -2.2181],\n",
      "        [ 2.5963, -0.8403, -2.1199]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2424, -1.6007, -2.2181],\n",
      "        [ 2.5963, -0.8403, -2.1199]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0337, -1.6397, -2.3072],\n",
      "        [ 2.2567, -0.3702, -1.8933]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0337, -1.6397, -2.3072],\n",
      "        [ 2.2567, -0.3702, -1.8933]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7184, -1.0679, -2.1368],\n",
      "        [ 2.5234, -0.9090, -2.1231]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7184, -1.0679, -2.1368],\n",
      "        [ 2.5234, -0.9090, -2.1231]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8348, -0.9751, -2.2638],\n",
      "        [ 2.1125, -0.4678, -1.9877]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8348, -0.9751, -2.2638],\n",
      "        [ 2.1125, -0.4678, -1.9877]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8585, -1.2168, -2.5254],\n",
      "        [ 2.7716, -1.0113, -2.1779]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8585, -1.2168, -2.5254],\n",
      "        [ 2.7716, -1.0113, -2.1779]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2431, -0.3607, -1.9543],\n",
      "        [ 3.0179, -1.3629, -2.1631]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.2431, -0.3607, -1.9543],\n",
      "        [ 3.0179, -1.3629, -2.1631]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4388, -0.7178, -2.0041],\n",
      "        [ 3.1586, -1.2252, -2.3715]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.4388, -0.7178, -2.0041],\n",
      "        [ 3.1586, -1.2252, -2.3715]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6796, -1.3209, -2.3084],\n",
      "        [ 2.7876, -1.1354, -2.1461]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6796, -1.3209, -2.3084],\n",
      "        [ 2.7876, -1.1354, -2.1461]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9121, -0.8470, -2.3956],\n",
      "        [ 3.1205, -1.6937, -2.1854]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9121, -0.8470, -2.3956],\n",
      "        [ 3.1205, -1.6937, -2.1854]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4150, -1.9610, -2.0688],\n",
      "        [ 3.2272, -1.3227, -2.3491]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4150, -1.9610, -2.0688],\n",
      "        [ 3.2272, -1.3227, -2.3491]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1460, -1.7685, -2.4039],\n",
      "        [ 2.6716, -1.1722, -2.3656]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1460, -1.7685, -2.4039],\n",
      "        [ 2.6716, -1.1722, -2.3656]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1356, -2.0352, -2.2447],\n",
      "        [ 3.4119, -2.0346, -2.4601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1356, -2.0352, -2.2447],\n",
      "        [ 3.4119, -2.0346, -2.4601]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6222, -2.2382, -2.4602],\n",
      "        [ 3.6738, -1.9090, -2.1574]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6222, -2.2382, -2.4602],\n",
      "        [ 3.6738, -1.9090, -2.1574]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6102, -2.3733, -2.5741],\n",
      "        [ 3.2869, -1.7720, -2.4624]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6102, -2.3733, -2.5741],\n",
      "        [ 3.2869, -1.7720, -2.4624]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8122, -2.1251, -2.4383],\n",
      "        [ 2.6957, -1.1954, -2.0742]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8122, -2.1251, -2.4383],\n",
      "        [ 2.6957, -1.1954, -2.0742]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9911, -1.4013, -2.2671],\n",
      "        [ 3.6312, -2.0477, -2.4898]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9911, -1.4013, -2.2671],\n",
      "        [ 3.6312, -2.0477, -2.4898]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5815, -0.6933, -2.0681],\n",
      "        [ 3.5780, -2.1564, -2.6112]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.5815, -0.6933, -2.0681],\n",
      "        [ 3.5780, -2.1564, -2.6112]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3204, -2.1293, -2.4402],\n",
      "        [ 3.7073, -2.3574, -2.3932]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3204, -2.1293, -2.4402],\n",
      "        [ 3.7073, -2.3574, -2.3932]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4437, -2.2908, -2.0031],\n",
      "        [ 2.7883, -1.0981, -2.2678]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4437, -2.2908, -2.0031],\n",
      "        [ 2.7883, -1.0981, -2.2678]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2306, -0.4737, -1.8598],\n",
      "        [ 3.4598, -2.4139, -2.4387]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.2306, -0.4737, -1.8598],\n",
      "        [ 3.4598, -2.4139, -2.4387]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3794, -2.1678, -2.3634],\n",
      "        [ 3.1194, -1.2822, -2.3900]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3794, -2.1678, -2.3634],\n",
      "        [ 3.1194, -1.2822, -2.3900]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6321, -2.3681, -1.9997],\n",
      "        [ 2.3894, -1.1180, -2.1753]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6321, -2.3681, -1.9997],\n",
      "        [ 2.3894, -1.1180, -2.1753]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5576, -2.3195, -2.5078],\n",
      "        [ 3.3504, -2.4063, -2.1042]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5576, -2.3195, -2.5078],\n",
      "        [ 3.3504, -2.4063, -2.1042]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7826, -2.5344, -2.4211],\n",
      "        [ 2.7725, -1.1625, -2.0917]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7826, -2.5344, -2.4211],\n",
      "        [ 2.7725, -1.1625, -2.0917]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9912, -1.5478, -2.4265],\n",
      "        [ 3.1011, -1.7352, -2.5129]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9912, -1.5478, -2.4265],\n",
      "        [ 3.1011, -1.7352, -2.5129]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4001, -2.2700, -2.2834],\n",
      "        [ 3.1989, -1.8830, -2.4835]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4001, -2.2700, -2.2834],\n",
      "        [ 3.1989, -1.8830, -2.4835]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3909, -1.6914, -1.8632],\n",
      "        [ 2.9066, -1.3200, -2.3576]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3909, -1.6914, -1.8632],\n",
      "        [ 2.9066, -1.3200, -2.3576]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4934, -1.6837, -2.1872],\n",
      "        [ 3.6684, -2.3903, -2.5980]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4934, -1.6837, -2.1872],\n",
      "        [ 3.6684, -2.3903, -2.5980]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5639, -2.4624, -2.4715],\n",
      "        [ 2.0667, -0.4739, -2.2000]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5639, -2.4624, -2.4715],\n",
      "        [ 2.0667, -0.4739, -2.2000]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5680, -2.4418, -2.2288],\n",
      "        [ 2.8395, -1.2163, -2.4081]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5680, -2.4418, -2.2288],\n",
      "        [ 2.8395, -1.2163, -2.4081]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6332, -2.2025, -2.1968],\n",
      "        [ 3.5351, -2.3560, -2.2778]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6332, -2.2025, -2.1968],\n",
      "        [ 3.5351, -2.3560, -2.2778]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0517, -1.3443, -2.3459],\n",
      "        [ 3.1217, -1.7741, -2.2466]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0517, -1.3443, -2.3459],\n",
      "        [ 3.1217, -1.7741, -2.2466]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7687, -2.6218, -2.3761],\n",
      "        [ 3.7348, -2.1724, -2.4242]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7687, -2.6218, -2.3761],\n",
      "        [ 3.7348, -2.1724, -2.4242]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2060, -2.1762, -2.3759],\n",
      "        [ 3.8363, -2.1755, -2.4531]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2060, -2.1762, -2.3759],\n",
      "        [ 3.8363, -2.1755, -2.4531]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5606, -2.1988, -2.4301],\n",
      "        [ 3.4658, -2.1234, -2.3808]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5606, -2.1988, -2.4301],\n",
      "        [ 3.4658, -2.1234, -2.3808]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4202, -2.0820, -2.4127],\n",
      "        [ 3.7658, -2.6215, -2.4605]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4202, -2.0820, -2.4127],\n",
      "        [ 3.7658, -2.6215, -2.4605]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4085, -1.7827, -2.3718],\n",
      "        [ 2.9188, -1.1816, -2.3990]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4085, -1.7827, -2.3718],\n",
      "        [ 2.9188, -1.1816, -2.3990]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7525, -2.5559, -2.3747],\n",
      "        [ 3.3929, -2.0872, -2.3336]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7525, -2.5559, -2.3747],\n",
      "        [ 3.3929, -2.0872, -2.3336]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7516, -2.2102, -2.4494],\n",
      "        [ 3.8133, -2.4650, -2.3776]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7516, -2.2102, -2.4494],\n",
      "        [ 3.8133, -2.4650, -2.3776]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.9003, -0.1883, -1.8027],\n",
      "        [ 2.3879, -0.6286, -2.2610]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.9003, -0.1883, -1.8027],\n",
      "        [ 2.3879, -0.6286, -2.2610]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9461, -2.5338, -2.3943],\n",
      "        [ 1.6056, -0.0881, -1.4809]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9461, -2.5338, -2.3943],\n",
      "        [ 1.6056, -0.0881, -1.4809]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8093, -2.0994, -2.4994],\n",
      "        [ 3.4903, -2.3021, -2.4480]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8093, -2.0994, -2.4994],\n",
      "        [ 3.4903, -2.3021, -2.4480]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8439, -2.5446, -2.3319],\n",
      "        [ 3.7216, -1.9612, -2.2555]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8439, -2.5446, -2.3319],\n",
      "        [ 3.7216, -1.9612, -2.2555]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9259, -2.5629, -2.2540],\n",
      "        [ 3.8069, -2.4961, -2.4192]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9259, -2.5629, -2.2540],\n",
      "        [ 3.8069, -2.4961, -2.4192]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5429, -0.1288, -1.3222],\n",
      "        [ 3.5132, -1.7252, -2.3845]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.5429, -0.1288, -1.3222],\n",
      "        [ 3.5132, -1.7252, -2.3845]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7090, -2.3241, -2.2983],\n",
      "        [ 3.7697, -2.4374, -2.4165]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7090, -2.3241, -2.2983],\n",
      "        [ 3.7697, -2.4374, -2.4165]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9620, -2.4486, -2.3359],\n",
      "        [ 3.9385, -2.4795, -2.5789]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9620, -2.4486, -2.3359],\n",
      "        [ 3.9385, -2.4795, -2.5789]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9009,  0.6485, -0.8835],\n",
      "        [ 3.8278, -2.9403, -2.2267]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.9009,  0.6485, -0.8835],\n",
      "        [ 3.8278, -2.9403, -2.2267]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8638, -0.9149, -2.2207],\n",
      "        [ 3.9339, -2.2860, -2.4880]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8638, -0.9149, -2.2207],\n",
      "        [ 3.9339, -2.2860, -2.4880]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9915, -1.8602, -2.1665],\n",
      "        [ 3.3348, -2.0328, -2.2758]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9915, -1.8602, -2.1665],\n",
      "        [ 3.3348, -2.0328, -2.2758]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3020, -2.2593, -2.1967],\n",
      "        [ 3.6576, -2.1786, -2.1458]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3020, -2.2593, -2.1967],\n",
      "        [ 3.6576, -2.1786, -2.1458]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8377, -2.2319, -2.7056],\n",
      "        [ 3.8258, -2.6442, -2.2320]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8377, -2.2319, -2.7056],\n",
      "        [ 3.8258, -2.6442, -2.2320]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5823, -2.2619, -2.3594],\n",
      "        [ 3.4478, -2.0702, -2.2724]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5823, -2.2619, -2.3594],\n",
      "        [ 3.4478, -2.0702, -2.2724]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6780, -2.4693, -2.4287],\n",
      "        [ 3.9195, -2.6319, -2.3566]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6780, -2.4693, -2.4287],\n",
      "        [ 3.9195, -2.6319, -2.3566]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3441, -2.0122, -2.3880],\n",
      "        [ 3.3903, -2.2260, -2.4294]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3441, -2.0122, -2.3880],\n",
      "        [ 3.3903, -2.2260, -2.4294]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9121, -2.5245, -2.3614],\n",
      "        [ 3.5628, -2.4806, -2.1990]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9121, -2.5245, -2.3614],\n",
      "        [ 3.5628, -2.4806, -2.1990]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5099, -1.2927, -1.8675],\n",
      "        [ 3.4622, -2.1283, -2.2123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.5099, -1.2927, -1.8675],\n",
      "        [ 3.4622, -2.1283, -2.2123]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6493, -2.4381, -2.1095],\n",
      "        [ 0.3690,  0.5998, -0.7200]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6493, -2.4381, -2.1095],\n",
      "        [ 0.3690,  0.5998, -0.7200]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6508, -2.3024, -2.2490],\n",
      "        [ 3.8401, -2.6257, -2.2798]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6508, -2.3024, -2.2490],\n",
      "        [ 3.8401, -2.6257, -2.2798]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2866, -1.5911, -2.3812],\n",
      "        [ 3.5347, -2.5006, -2.1302]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2866, -1.5911, -2.3812],\n",
      "        [ 3.5347, -2.5006, -2.1302]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6380, -2.3608, -2.2720],\n",
      "        [ 3.6986, -2.2853, -2.6583]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6380, -2.3608, -2.2720],\n",
      "        [ 3.6986, -2.2853, -2.6583]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.5908,  0.6547, -0.8573],\n",
      "        [ 3.6437, -2.5306, -2.2529]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.5908,  0.6547, -0.8573],\n",
      "        [ 3.6437, -2.5306, -2.2529]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6188, -2.6083, -2.3553],\n",
      "        [ 1.1900,  0.0404, -1.5537]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6188, -2.6083, -2.3553],\n",
      "        [ 1.1900,  0.0404, -1.5537]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9262, -2.3490, -2.4231],\n",
      "        [ 3.8271, -2.8246, -2.1417]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9262, -2.3490, -2.4231],\n",
      "        [ 3.8271, -2.8246, -2.1417]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3920, -2.3189, -2.4604],\n",
      "        [ 3.4862, -2.3851, -2.4305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3920, -2.3189, -2.4604],\n",
      "        [ 3.4862, -2.3851, -2.4305]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9159, -2.6444, -2.3179],\n",
      "        [ 4.1174, -2.5414, -2.5327]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9159, -2.6444, -2.3179],\n",
      "        [ 4.1174, -2.5414, -2.5327]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.5783,  0.7310, -0.7955],\n",
      "        [ 3.6835, -2.3559, -2.1213]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.5783,  0.7310, -0.7955],\n",
      "        [ 3.6835, -2.3559, -2.1213]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6991,  0.5810, -0.8629],\n",
      "        [ 3.7104, -2.4291, -2.4485]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.6991,  0.5810, -0.8629],\n",
      "        [ 3.7104, -2.4291, -2.4485]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6254, -2.6965, -1.8927],\n",
      "        [ 3.6355, -2.5296, -2.1473]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6254, -2.6965, -1.8927],\n",
      "        [ 3.6355, -2.5296, -2.1473]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8310, -2.6543, -2.1832],\n",
      "        [ 4.2240, -2.6604, -2.4259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8310, -2.6543, -2.1832],\n",
      "        [ 4.2240, -2.6604, -2.4259]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7431, -2.6573, -1.8066],\n",
      "        [ 3.9122, -2.8681, -2.1583]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7431, -2.6573, -1.8066],\n",
      "        [ 3.9122, -2.8681, -2.1583]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5760, -2.4265, -2.5081],\n",
      "        [ 3.4926, -2.7057, -1.9388]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5760, -2.4265, -2.5081],\n",
      "        [ 3.4926, -2.7057, -1.9388]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4214,  0.6293, -0.9225],\n",
      "        [ 3.9185, -2.4347, -2.4235]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.4214,  0.6293, -0.9225],\n",
      "        [ 3.9185, -2.4347, -2.4235]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5213, -2.6610, -1.8474],\n",
      "        [ 3.5981, -2.5249, -2.2795]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5213, -2.6610, -1.8474],\n",
      "        [ 3.5981, -2.5249, -2.2795]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7750, -2.7403, -2.0904],\n",
      "        [ 3.7360, -2.6439, -1.9464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7750, -2.7403, -2.0904],\n",
      "        [ 3.7360, -2.6439, -1.9464]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3320, -2.2470, -1.9402],\n",
      "        [ 3.3054, -2.3281, -2.3517]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3320, -2.2470, -1.9402],\n",
      "        [ 3.3054, -2.3281, -2.3517]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5053, -2.8141, -1.9285],\n",
      "        [ 3.6688, -2.6372, -1.8214]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5053, -2.8141, -1.9285],\n",
      "        [ 3.6688, -2.6372, -1.8214]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5606, -2.7154, -1.9963],\n",
      "        [ 3.5544, -2.6861, -2.2722]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5606, -2.7154, -1.9963],\n",
      "        [ 3.5544, -2.6861, -2.2722]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2139, -2.3855, -1.9044],\n",
      "        [ 3.8260, -2.3140, -2.2304]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2139, -2.3855, -1.9044],\n",
      "        [ 3.8260, -2.3140, -2.2304]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9082, -2.5546, -2.2803],\n",
      "        [ 3.6294, -2.7143, -1.8434]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9082, -2.5546, -2.2803],\n",
      "        [ 3.6294, -2.7143, -1.8434]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9929, -2.1695, -2.1779],\n",
      "        [ 3.4158, -2.6650, -1.6399]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9929, -2.1695, -2.1779],\n",
      "        [ 3.4158, -2.6650, -1.6399]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4095, -2.3462, -1.6438],\n",
      "        [ 3.3006, -2.0273, -2.5409]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4095, -2.3462, -1.6438],\n",
      "        [ 3.3006, -2.0273, -2.5409]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9028, -2.6227, -2.1805],\n",
      "        [ 3.5780, -2.6805, -1.8617]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9028, -2.6227, -2.1805],\n",
      "        [ 3.5780, -2.6805, -1.8617]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6933, -2.6410, -1.9648],\n",
      "        [ 3.6071, -2.8773, -1.9471]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6933, -2.6410, -1.9648],\n",
      "        [ 3.6071, -2.8773, -1.9471]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6345, -2.6894, -1.8371],\n",
      "        [ 3.9212, -2.9019, -2.1470]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6345, -2.6894, -1.8371],\n",
      "        [ 3.9212, -2.9019, -2.1470]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5038, -2.5650, -1.9199],\n",
      "        [ 3.7384, -2.4545, -2.2865]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5038, -2.5650, -1.9199],\n",
      "        [ 3.7384, -2.4545, -2.2865]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7460, -2.7521, -1.9495],\n",
      "        [ 3.0794, -2.2451, -2.0295]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7460, -2.7521, -1.9495],\n",
      "        [ 3.0794, -2.2451, -2.0295]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9335, -2.5367, -2.2579],\n",
      "        [ 3.7745, -2.8296, -2.1323]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9335, -2.5367, -2.2579],\n",
      "        [ 3.7745, -2.8296, -2.1323]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8454, -2.9230, -2.1072],\n",
      "        [ 3.9341, -2.8951, -1.8859]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8454, -2.9230, -2.1072],\n",
      "        [ 3.9341, -2.8951, -1.8859]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8005, -2.6196, -2.0797],\n",
      "        [ 3.7000, -3.0504, -1.9341]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8005, -2.6196, -2.0797],\n",
      "        [ 3.7000, -3.0504, -1.9341]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8863, -2.9204, -2.1816],\n",
      "        [ 2.8959, -1.6426, -2.3140]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8863, -2.9204, -2.1816],\n",
      "        [ 2.8959, -1.6426, -2.3140]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5399, -2.5028, -2.4232],\n",
      "        [ 4.0478, -2.6419, -2.3718]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5399, -2.5028, -2.4232],\n",
      "        [ 4.0478, -2.6419, -2.3718]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6429, -2.7878, -2.2720],\n",
      "        [ 3.7532, -2.7683, -1.9494]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6429, -2.7878, -2.2720],\n",
      "        [ 3.7532, -2.7683, -1.9494]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8589, -2.6978, -2.2570],\n",
      "        [ 3.7936, -2.7385, -2.2649]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8589, -2.6978, -2.2570],\n",
      "        [ 3.7936, -2.7385, -2.2649]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1498, -0.2514, -1.8994],\n",
      "        [ 3.7166, -3.0093, -1.8892]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.1498, -0.2514, -1.8994],\n",
      "        [ 3.7166, -3.0093, -1.8892]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8267, -2.6975, -2.0720],\n",
      "        [ 3.9349, -2.9490, -1.9566]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8267, -2.6975, -2.0720],\n",
      "        [ 3.9349, -2.9490, -1.9566]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8815, -2.8466, -2.0446],\n",
      "        [ 3.7549, -2.6359, -2.2821]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8815, -2.8466, -2.0446],\n",
      "        [ 3.7549, -2.6359, -2.2821]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8402, -2.7493, -2.1109],\n",
      "        [ 3.4951, -2.6350, -1.9915]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8402, -2.7493, -2.1109],\n",
      "        [ 3.4951, -2.6350, -1.9915]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5175, -2.8227, -1.9433],\n",
      "        [ 3.9834, -2.9930, -2.3195]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5175, -2.8227, -1.9433],\n",
      "        [ 3.9834, -2.9930, -2.3195]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0419, -2.8718, -2.2656],\n",
      "        [ 3.6227, -2.9168, -2.0159]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0419, -2.8718, -2.2656],\n",
      "        [ 3.6227, -2.9168, -2.0159]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7188, -2.4325, -1.9392],\n",
      "        [ 3.6119, -2.7762, -1.8028]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7188, -2.4325, -1.9392],\n",
      "        [ 3.6119, -2.7762, -1.8028]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6268, -2.6209, -2.1127],\n",
      "        [ 3.6288, -3.0270, -1.8649]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6268, -2.6209, -2.1127],\n",
      "        [ 3.6288, -3.0270, -1.8649]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6363, -2.5868, -2.1857],\n",
      "        [ 3.9472, -2.5390, -2.1409]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6363, -2.5868, -2.1857],\n",
      "        [ 3.9472, -2.5390, -2.1409]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7845, -2.4497, -2.1860],\n",
      "        [ 3.2855, -2.4059, -1.9094]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7845, -2.4497, -2.1860],\n",
      "        [ 3.2855, -2.4059, -1.9094]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9370, -1.6211, -2.2047],\n",
      "        [ 3.4037, -2.8250, -1.6874]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.9370, -1.6211, -2.2047],\n",
      "        [ 3.4037, -2.8250, -1.6874]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5107, -2.8457, -1.8035],\n",
      "        [ 3.6629, -2.7271, -1.8283]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5107, -2.8457, -1.8035],\n",
      "        [ 3.6629, -2.7271, -1.8283]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2460, -2.7067, -1.4138],\n",
      "        [ 3.7647, -2.8544, -1.8066]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2460, -2.7067, -1.4138],\n",
      "        [ 3.7647, -2.8544, -1.8066]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2496, -2.8634, -1.4115],\n",
      "        [ 3.2382, -2.7775, -1.4903]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2496, -2.8634, -1.4115],\n",
      "        [ 3.2382, -2.7775, -1.4903]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7742, -3.0438, -1.8182],\n",
      "        [ 2.9814, -2.9196, -1.5042]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7742, -3.0438, -1.8182],\n",
      "        [ 2.9814, -2.9196, -1.5042]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4125, -2.7072, -1.5532],\n",
      "        [ 2.8381, -2.8886, -1.3601]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4125, -2.7072, -1.5532],\n",
      "        [ 2.8381, -2.8886, -1.3601]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6695, -2.6841, -1.5395],\n",
      "        [ 3.1661, -2.9332, -1.3564]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6695, -2.6841, -1.5395],\n",
      "        [ 3.1661, -2.9332, -1.3564]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7766, -2.7682, -2.1445],\n",
      "        [ 3.1507, -2.9969, -1.3476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7766, -2.7682, -2.1445],\n",
      "        [ 3.1507, -2.9969, -1.3476]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5165, -2.8616, -1.6878],\n",
      "        [ 3.7877, -2.8981, -1.8642]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5165, -2.8616, -1.6878],\n",
      "        [ 3.7877, -2.8981, -1.8642]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9745, -2.6994, -2.3339],\n",
      "        [ 3.8468, -2.6152, -2.1317]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9745, -2.6994, -2.3339],\n",
      "        [ 3.8468, -2.6152, -2.1317]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5148, -2.9519, -1.4955],\n",
      "        [ 3.5457, -2.9625, -1.4898]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5148, -2.9519, -1.4955],\n",
      "        [ 3.5457, -2.9625, -1.4898]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7703, -3.0108, -1.9259],\n",
      "        [ 1.0113,  0.2109, -1.1094]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7703, -3.0108, -1.9259],\n",
      "        [ 1.0113,  0.2109, -1.1094]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4039, -2.6721, -1.7703],\n",
      "        [ 3.5953, -2.7813, -1.9452]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4039, -2.6721, -1.7703],\n",
      "        [ 3.5953, -2.7813, -1.9452]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4811,  0.8197, -0.9691],\n",
      "        [ 3.6809, -2.5768, -1.9045]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.4811,  0.8197, -0.9691],\n",
      "        [ 3.6809, -2.5768, -1.9045]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5788, -2.7652, -2.0391],\n",
      "        [ 3.6009, -2.9197, -1.6249]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5788, -2.7652, -2.0391],\n",
      "        [ 3.6009, -2.9197, -1.6249]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6351, -2.8645, -1.9627],\n",
      "        [ 3.6513, -3.1173, -1.7591]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6351, -2.8645, -1.9627],\n",
      "        [ 3.6513, -3.1173, -1.7591]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6289, -3.0082, -1.7765],\n",
      "        [ 2.9381, -1.6115, -2.0210]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6289, -3.0082, -1.7765],\n",
      "        [ 2.9381, -1.6115, -2.0210]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4785, -2.7383, -1.4493],\n",
      "        [ 3.7395, -3.0534, -2.1031]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4785, -2.7383, -1.4493],\n",
      "        [ 3.7395, -3.0534, -2.1031]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5078, -3.0001, -1.4654],\n",
      "        [ 3.3529, -2.7658, -1.2480]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5078, -3.0001, -1.4654],\n",
      "        [ 3.3529, -2.7658, -1.2480]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5531, -2.8049, -1.6723],\n",
      "        [ 3.5381, -2.9148, -1.6054]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5531, -2.8049, -1.6723],\n",
      "        [ 3.5381, -2.9148, -1.6054]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4351,  0.4072, -0.7047],\n",
      "        [ 3.3397, -2.9623, -1.6578]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.4351,  0.4072, -0.7047],\n",
      "        [ 3.3397, -2.9623, -1.6578]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4156, -2.8499, -1.7973],\n",
      "        [ 3.5075, -2.4212, -2.0212]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4156, -2.8499, -1.7973],\n",
      "        [ 3.5075, -2.4212, -2.0212]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4869, -2.6448, -1.8516],\n",
      "        [ 2.0214, -0.7146, -1.8707]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4869, -2.6448, -1.8516],\n",
      "        [ 2.0214, -0.7146, -1.8707]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1975, -2.8808, -1.6354],\n",
      "        [ 3.6267, -2.3053, -2.2020]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1975, -2.8808, -1.6354],\n",
      "        [ 3.6267, -2.3053, -2.2020]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4937, -2.9404, -1.5822],\n",
      "        [ 3.3291, -2.6043, -1.8850]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4937, -2.9404, -1.5822],\n",
      "        [ 3.3291, -2.6043, -1.8850]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3444, -2.8845, -2.0696],\n",
      "        [ 3.6876, -2.6590, -1.8541]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3444, -2.8845, -2.0696],\n",
      "        [ 3.6876, -2.6590, -1.8541]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7425, -2.8067, -1.8574],\n",
      "        [ 3.6700, -2.7146, -2.1547]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7425, -2.8067, -1.8574],\n",
      "        [ 3.6700, -2.7146, -2.1547]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7055, -2.8533, -1.8376],\n",
      "        [ 3.7600, -2.9016, -1.8701]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7055, -2.8533, -1.8376],\n",
      "        [ 3.7600, -2.9016, -1.8701]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4793, -2.8292, -1.9034],\n",
      "        [ 3.8593, -2.6552, -2.1024]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4793, -2.8292, -1.9034],\n",
      "        [ 3.8593, -2.6552, -2.1024]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5590, -2.6586, -1.6972],\n",
      "        [ 0.6536,  0.7847, -1.0947]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5590, -2.6586, -1.6972],\n",
      "        [ 0.6536,  0.7847, -1.0947]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5746, -2.9603, -1.8439],\n",
      "        [ 3.5844, -2.7386, -1.9890]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5746, -2.9603, -1.8439],\n",
      "        [ 3.5844, -2.7386, -1.9890]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6084, -2.5536, -1.8982],\n",
      "        [ 3.6679, -2.7297, -1.9747]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6084, -2.5536, -1.8982],\n",
      "        [ 3.6679, -2.7297, -1.9747]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9196, -2.6750, -2.1511],\n",
      "        [ 3.7332, -2.5571, -2.1831]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9196, -2.6750, -2.1511],\n",
      "        [ 3.7332, -2.5571, -2.1831]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3703, -2.5534, -1.8811],\n",
      "        [ 4.1045, -2.6968, -2.3058]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3703, -2.5534, -1.8811],\n",
      "        [ 4.1045, -2.6968, -2.3058]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7311, -2.9131, -2.0004],\n",
      "        [ 3.6786, -2.5407, -2.0634]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7311, -2.9131, -2.0004],\n",
      "        [ 3.6786, -2.5407, -2.0634]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1741,  1.1077, -0.4968],\n",
      "        [ 3.5924, -2.0393, -2.2657]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.1741,  1.1077, -0.4968],\n",
      "        [ 3.5924, -2.0393, -2.2657]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7695, -1.8836, -2.2417],\n",
      "        [ 3.3835, -2.2401, -2.2225]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7695, -1.8836, -2.2417],\n",
      "        [ 3.3835, -2.2401, -2.2225]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9883, -2.7329, -2.2062],\n",
      "        [ 3.5909, -2.5323, -1.9536]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9883, -2.7329, -2.2062],\n",
      "        [ 3.5909, -2.5323, -1.9536]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6176, -2.3049, -1.9461],\n",
      "        [ 3.1047, -1.7014, -2.4361]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6176, -2.3049, -1.9461],\n",
      "        [ 3.1047, -1.7014, -2.4361]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8066, -2.5128, -2.2630],\n",
      "        [ 3.8374, -2.9372, -1.8513]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8066, -2.5128, -2.2630],\n",
      "        [ 3.8374, -2.9372, -1.8513]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6108, -2.5282, -1.8980],\n",
      "        [ 3.4175, -2.1928, -1.8641]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6108, -2.5282, -1.8980],\n",
      "        [ 3.4175, -2.1928, -1.8641]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.6848,  1.3617, -0.3398],\n",
      "        [ 3.5400, -2.1762, -2.3407]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.6848,  1.3617, -0.3398],\n",
      "        [ 3.5400, -2.1762, -2.3407]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7472, -2.3627, -2.4435],\n",
      "        [ 3.4852, -2.2271, -2.0435]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7472, -2.3627, -2.4435],\n",
      "        [ 3.4852, -2.2271, -2.0435]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5958, -2.3283, -2.1713],\n",
      "        [ 3.7502, -2.4196, -2.3295]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5958, -2.3283, -2.1713],\n",
      "        [ 3.7502, -2.4196, -2.3295]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6237, -2.2332, -2.3998],\n",
      "        [ 3.6954, -2.3665, -2.3110]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6237, -2.2332, -2.3998],\n",
      "        [ 3.6954, -2.3665, -2.3110]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2933, -1.8488, -1.9458],\n",
      "        [ 3.8984, -2.4823, -2.2258]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2933, -1.8488, -1.9458],\n",
      "        [ 3.8984, -2.4823, -2.2258]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5933, -2.6863, -1.9565],\n",
      "        [ 3.7883, -2.0662, -2.2696]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5933, -2.6863, -1.9565],\n",
      "        [ 3.7883, -2.0662, -2.2696]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7974, -2.5929, -2.2543],\n",
      "        [ 3.5304, -2.3203, -1.8352]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7974, -2.5929, -2.2543],\n",
      "        [ 3.5304, -2.3203, -1.8352]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4629, -2.5427, -1.7597],\n",
      "        [ 3.4250, -2.5202, -1.8588]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4629, -2.5427, -1.7597],\n",
      "        [ 3.4250, -2.5202, -1.8588]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6510, -0.3111, -2.2729],\n",
      "        [ 3.0940, -2.3277, -2.1593]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.6510, -0.3111, -2.2729],\n",
      "        [ 3.0940, -2.3277, -2.1593]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8132, -2.9756, -1.9555],\n",
      "        [ 3.6804, -2.7537, -2.2757]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8132, -2.9756, -1.9555],\n",
      "        [ 3.6804, -2.7537, -2.2757]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1628, -2.0099, -2.1692],\n",
      "        [ 3.7536, -2.7240, -1.7105]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1628, -2.0099, -2.1692],\n",
      "        [ 3.7536, -2.7240, -1.7105]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7401, -3.1125, -1.8885],\n",
      "        [ 4.0985, -3.0922, -2.0546]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7401, -3.1125, -1.8885],\n",
      "        [ 4.0985, -3.0922, -2.0546]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5087, -2.6633, -1.7445],\n",
      "        [ 3.1682, -2.3237, -1.8532]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5087, -2.6633, -1.7445],\n",
      "        [ 3.1682, -2.3237, -1.8532]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4143, -2.3620, -1.7953],\n",
      "        [ 3.3693, -2.5259, -1.8397]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4143, -2.3620, -1.7953],\n",
      "        [ 3.3693, -2.5259, -1.8397]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4936, -2.6521, -1.7375],\n",
      "        [ 3.3699, -2.7088, -1.8368]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4936, -2.6521, -1.7375],\n",
      "        [ 3.3699, -2.7088, -1.8368]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8054, -2.9188, -2.1124],\n",
      "        [ 3.5516, -3.0296, -1.8254]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8054, -2.9188, -2.1124],\n",
      "        [ 3.5516, -3.0296, -1.8254]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9000, -2.9228, -2.1072],\n",
      "        [ 3.2798, -2.3715, -1.6482]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9000, -2.9228, -2.1072],\n",
      "        [ 3.2798, -2.3715, -1.6482]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4299, -2.5807, -1.8741],\n",
      "        [ 3.6154, -2.8307, -1.7903]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4299, -2.5807, -1.8741],\n",
      "        [ 3.6154, -2.8307, -1.7903]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3891, -2.6653, -1.5652],\n",
      "        [ 3.2156, -2.3663, -1.8804]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3891, -2.6653, -1.5652],\n",
      "        [ 3.2156, -2.3663, -1.8804]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8501, -2.5769, -2.4062],\n",
      "        [ 3.9207, -2.8495, -1.8815]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8501, -2.5769, -2.4062],\n",
      "        [ 3.9207, -2.8495, -1.8815]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6041, -2.3917, -1.9058],\n",
      "        [ 3.6650, -2.9176, -1.9079]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6041, -2.3917, -1.9058],\n",
      "        [ 3.6650, -2.9176, -1.9079]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5046, -2.6422, -2.1577],\n",
      "        [ 3.8424, -2.9833, -2.1857]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5046, -2.6422, -2.1577],\n",
      "        [ 3.8424, -2.9833, -2.1857]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8718, -2.7738, -1.8108],\n",
      "        [ 3.7207, -2.8324, -2.0043]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8718, -2.7738, -1.8108],\n",
      "        [ 3.7207, -2.8324, -2.0043]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6130, -2.7398, -2.0335],\n",
      "        [ 3.8123, -2.5606, -1.9291]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6130, -2.7398, -2.0335],\n",
      "        [ 3.8123, -2.5606, -1.9291]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8650, -2.7880, -1.9679],\n",
      "        [ 2.6064, -1.4226, -2.0705]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8650, -2.7880, -1.9679],\n",
      "        [ 2.6064, -1.4226, -2.0705]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.3419,  1.7572, -0.5497],\n",
      "        [ 3.6439, -2.6882, -1.7506]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.3419,  1.7572, -0.5497],\n",
      "        [ 3.6439, -2.6882, -1.7506]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8067, -2.7722, -1.9190],\n",
      "        [ 3.9079, -2.6659, -2.0833]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8067, -2.7722, -1.9190],\n",
      "        [ 3.9079, -2.6659, -2.0833]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8533, -2.7513, -2.1038],\n",
      "        [ 3.5360, -2.3254, -2.3283]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8533, -2.7513, -2.1038],\n",
      "        [ 3.5360, -2.3254, -2.3283]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4811, -2.6878, -1.9241],\n",
      "        [ 3.7036, -2.8349, -2.2681]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4811, -2.6878, -1.9241],\n",
      "        [ 3.7036, -2.8349, -2.2681]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3509, -2.3372, -1.8463],\n",
      "        [ 3.4205, -2.3122, -2.3559]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3509, -2.3372, -1.8463],\n",
      "        [ 3.4205, -2.3122, -2.3559]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8463, -2.5592, -2.1724],\n",
      "        [ 3.9793, -2.8415, -2.2760]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8463, -2.5592, -2.1724],\n",
      "        [ 3.9793, -2.8415, -2.2760]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.8119,  1.8177, -0.4411],\n",
      "        [ 3.6986, -2.1979, -2.4607]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.8119,  1.8177, -0.4411],\n",
      "        [ 3.6986, -2.1979, -2.4607]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1004, -3.0385, -2.1465],\n",
      "        [ 3.9787, -2.9013, -1.9235]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1004, -3.0385, -2.1465],\n",
      "        [ 3.9787, -2.9013, -1.9235]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7954, -3.0855, -1.7261],\n",
      "        [ 3.8364, -2.9567, -2.0985]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7954, -3.0855, -1.7261],\n",
      "        [ 3.8364, -2.9567, -2.0985]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5428, -2.8328, -1.6377],\n",
      "        [ 3.3891, -2.9305, -1.8444]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5428, -2.8328, -1.6377],\n",
      "        [ 3.3891, -2.9305, -1.8444]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7082, -3.2382, -1.9030],\n",
      "        [ 3.8049, -3.1370, -1.9059]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7082, -3.2382, -1.9030],\n",
      "        [ 3.8049, -3.1370, -1.9059]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8874, -3.2551, -1.8678],\n",
      "        [-0.1820,  1.6345, -0.8167]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8874, -3.2551, -1.8678],\n",
      "        [-0.1820,  1.6345, -0.8167]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0252, -2.9138, -2.0906],\n",
      "        [ 3.7852, -3.0729, -1.7842]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0252, -2.9138, -2.0906],\n",
      "        [ 3.7852, -3.0729, -1.7842]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9793, -2.9406, -2.1495],\n",
      "        [ 4.0147, -3.0074, -1.8772]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9793, -2.9406, -2.1495],\n",
      "        [ 4.0147, -3.0074, -1.8772]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9961, -2.9680, -1.9245],\n",
      "        [ 4.0625, -3.3023, -2.1810]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9961, -2.9680, -1.9245],\n",
      "        [ 4.0625, -3.3023, -2.1810]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0617, -2.5532, -2.1779],\n",
      "        [ 4.1850, -2.8584, -2.3597]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0617, -2.5532, -2.1779],\n",
      "        [ 4.1850, -2.8584, -2.3597]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7817, -2.8352, -2.3070],\n",
      "        [ 3.9640, -2.8065, -1.9604]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7817, -2.8352, -2.3070],\n",
      "        [ 3.9640, -2.8065, -1.9604]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8356, -2.9184, -1.9127],\n",
      "        [ 4.0728, -3.2553, -2.4362]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8356, -2.9184, -1.9127],\n",
      "        [ 4.0728, -3.2553, -2.4362]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5549, -2.5414, -2.5326],\n",
      "        [ 4.0758, -3.1221, -2.0949]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5549, -2.5414, -2.5326],\n",
      "        [ 4.0758, -3.1221, -2.0949]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9535, -2.9628, -1.9335],\n",
      "        [ 4.0559, -3.0627, -2.0463]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9535, -2.9628, -1.9335],\n",
      "        [ 4.0559, -3.0627, -2.0463]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2289, -3.1530, -2.2633],\n",
      "        [-1.3572,  2.0276, -0.1855]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2289, -3.1530, -2.2633],\n",
      "        [-1.3572,  2.0276, -0.1855]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0836, -3.1300, -2.0696],\n",
      "        [ 3.6886, -2.9617, -1.7019]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0836, -3.1300, -2.0696],\n",
      "        [ 3.6886, -2.9617, -1.7019]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2104, -2.6273, -1.4013],\n",
      "        [ 3.9301, -2.8033, -2.0367]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2104, -2.6273, -1.4013],\n",
      "        [ 3.9301, -2.8033, -2.0367]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9225, -3.0177, -1.8119],\n",
      "        [ 3.9555, -3.2276, -2.1781]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9225, -3.0177, -1.8119],\n",
      "        [ 3.9555, -3.2276, -2.1781]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9854,  0.0422, -1.5787],\n",
      "        [ 3.9971, -3.1448, -2.1485]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.9854,  0.0422, -1.5787],\n",
      "        [ 3.9971, -3.1448, -2.1485]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9237, -3.1957, -2.1020],\n",
      "        [ 2.6842, -2.3504, -1.3151]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9237, -3.1957, -2.1020],\n",
      "        [ 2.6842, -2.3504, -1.3151]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9797, -3.0112, -1.9186],\n",
      "        [ 3.8608, -2.9414, -2.0378]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9797, -3.0112, -1.9186],\n",
      "        [ 3.8608, -2.9414, -2.0378]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7434, -2.4596, -2.1499],\n",
      "        [ 3.9060, -3.2078, -1.9734]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7434, -2.4596, -2.1499],\n",
      "        [ 3.9060, -3.2078, -1.9734]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8903, -3.2126, -2.0024],\n",
      "        [ 3.6612, -2.7655, -1.7718]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8903, -3.2126, -2.0024],\n",
      "        [ 3.6612, -2.7655, -1.7718]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.3661,  1.9544, -0.4069],\n",
      "        [ 3.7061, -2.9004, -2.0553]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.3661,  1.9544, -0.4069],\n",
      "        [ 3.7061, -2.9004, -2.0553]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5162, -3.1324, -1.6107],\n",
      "        [ 3.6603, -3.1934, -1.6069]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5162, -3.1324, -1.6107],\n",
      "        [ 3.6603, -3.1934, -1.6069]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7792, -3.0398, -2.1033],\n",
      "        [ 3.6853, -3.1916, -1.5851]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7792, -3.0398, -2.1033],\n",
      "        [ 3.6853, -3.1916, -1.5851]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4192, -3.0199, -1.4398],\n",
      "        [ 3.7636, -2.8331, -2.0057]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4192, -3.0199, -1.4398],\n",
      "        [ 3.7636, -2.8331, -2.0057]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9279, -3.2292, -1.8829],\n",
      "        [ 3.6494, -2.7385, -2.1989]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9279, -3.2292, -1.8829],\n",
      "        [ 3.6494, -2.7385, -2.1989]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.7114,  2.0061, -0.3729],\n",
      "        [ 3.9307, -3.2527, -1.8318]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.7114,  2.0061, -0.3729],\n",
      "        [ 3.9307, -3.2527, -1.8318]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2870, -2.7438, -1.1840],\n",
      "        [ 3.6436, -3.1386, -1.6996]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2870, -2.7438, -1.1840],\n",
      "        [ 3.6436, -3.1386, -1.6996]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7553, -3.0918, -1.7121],\n",
      "        [ 3.5163, -2.9980, -1.3208]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7553, -3.0918, -1.7121],\n",
      "        [ 3.5163, -2.9980, -1.3208]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6459, -3.2216, -1.2218],\n",
      "        [ 3.6385, -3.1679, -1.3429]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6459, -3.2216, -1.2218],\n",
      "        [ 3.6385, -3.1679, -1.3429]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8177, -3.0921, -1.7439],\n",
      "        [ 3.8452, -3.1045, -1.8334]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8177, -3.0921, -1.7439],\n",
      "        [ 3.8452, -3.1045, -1.8334]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9965, -3.4103, -1.8212],\n",
      "        [ 2.9906, -2.2767, -1.4082]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9965, -3.4103, -1.8212],\n",
      "        [ 2.9906, -2.2767, -1.4082]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4232, -3.1081, -1.3291],\n",
      "        [ 2.9861, -3.0997, -1.0586]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4232, -3.1081, -1.3291],\n",
      "        [ 2.9861, -3.0997, -1.0586]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9614, -3.3085, -1.6991],\n",
      "        [ 3.2548, -2.9863, -1.4609]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9614, -3.3085, -1.6991],\n",
      "        [ 3.2548, -2.9863, -1.4609]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4717, -2.9913, -1.6879],\n",
      "        [ 4.1121, -3.4308, -2.0354]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4717, -2.9913, -1.6879],\n",
      "        [ 4.1121, -3.4308, -2.0354]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2006, -3.3363, -2.1898],\n",
      "        [ 3.5691, -2.8691, -1.6092]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2006, -3.3363, -2.1898],\n",
      "        [ 3.5691, -2.8691, -1.6092]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.7368,  2.2319, -0.9665],\n",
      "        [ 3.8706, -3.2708, -1.7595]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.7368,  2.2319, -0.9665],\n",
      "        [ 3.8706, -3.2708, -1.7595]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8203, -3.1171, -1.7705],\n",
      "        [ 3.8978, -3.1777, -1.4373]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8203, -3.1171, -1.7705],\n",
      "        [ 3.8978, -3.1777, -1.4373]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5190, -2.9579, -1.4934],\n",
      "        [ 4.0396, -3.2010, -1.8843]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5190, -2.9579, -1.4934],\n",
      "        [ 4.0396, -3.2010, -1.8843]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5172, -3.0495, -1.3857],\n",
      "        [ 3.8482, -3.1735, -1.9835]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5172, -3.0495, -1.3857],\n",
      "        [ 3.8482, -3.1735, -1.9835]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6346, -2.7713, -1.6881],\n",
      "        [ 4.2777, -2.8790, -2.2692]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6346, -2.7713, -1.6881],\n",
      "        [ 4.2777, -2.8790, -2.2692]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0348, -3.2066, -2.0130],\n",
      "        [ 4.0623, -3.1578, -2.3979]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0348, -3.2066, -2.0130],\n",
      "        [ 4.0623, -3.1578, -2.3979]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7575, -3.1057, -1.8595],\n",
      "        [ 3.4584, -3.2111, -1.5440]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7575, -3.1057, -1.8595],\n",
      "        [ 3.4584, -3.2111, -1.5440]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1525, -3.1752, -2.2149],\n",
      "        [ 4.0758, -3.3922, -2.1402]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1525, -3.1752, -2.2149],\n",
      "        [ 4.0758, -3.3922, -2.1402]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7537, -2.7373, -1.3582],\n",
      "        [ 4.0110, -3.0180, -2.1190]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7537, -2.7373, -1.3582],\n",
      "        [ 4.0110, -3.0180, -2.1190]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1365, -3.1458, -1.9752],\n",
      "        [ 4.1686, -3.1129, -2.0640]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1365, -3.1458, -1.9752],\n",
      "        [ 4.1686, -3.1129, -2.0640]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0282, -3.2532, -1.7099],\n",
      "        [ 3.9248, -2.8292, -2.0769]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0282, -3.2532, -1.7099],\n",
      "        [ 3.9248, -2.8292, -2.0769]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9092, -2.9174, -1.8168],\n",
      "        [ 4.1106, -3.0815, -1.8068]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9092, -2.9174, -1.8168],\n",
      "        [ 4.1106, -3.0815, -1.8068]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7911, -2.2307, -1.3337],\n",
      "        [ 3.9913, -2.9698, -1.8157]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7911, -2.2307, -1.3337],\n",
      "        [ 3.9913, -2.9698, -1.8157]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1707, -3.2977, -2.0584],\n",
      "        [ 3.5971, -2.8064, -1.9034]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1707, -3.2977, -2.0584],\n",
      "        [ 3.5971, -2.8064, -1.9034]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7144, -3.2565, -1.9847],\n",
      "        [ 4.0860, -3.2984, -1.9011]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7144, -3.2565, -1.9847],\n",
      "        [ 4.0860, -3.2984, -1.9011]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8440, -2.8761, -1.8227],\n",
      "        [ 3.0681, -2.7876, -1.2612]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8440, -2.8761, -1.8227],\n",
      "        [ 3.0681, -2.7876, -1.2612]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6798, -3.1878, -1.7088],\n",
      "        [ 4.2145, -3.2562, -2.1509]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6798, -3.1878, -1.7088],\n",
      "        [ 4.2145, -3.2562, -2.1509]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6980, -3.2118, -1.4775],\n",
      "        [ 3.9008, -3.2321, -1.6290]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6980, -3.2118, -1.4775],\n",
      "        [ 3.9008, -3.2321, -1.6290]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4357, -2.3903, -0.8900],\n",
      "        [ 3.3871, -3.2205, -1.4758]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.4357, -2.3903, -0.8900],\n",
      "        [ 3.3871, -3.2205, -1.4758]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6875, -3.2531, -1.5453],\n",
      "        [ 3.6822, -2.6140, -2.2566]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6875, -3.2531, -1.5453],\n",
      "        [ 3.6822, -2.6140, -2.2566]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9942, -3.3522, -1.7632],\n",
      "        [ 2.9084, -2.3067, -1.8836]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9942, -3.3522, -1.7632],\n",
      "        [ 2.9084, -2.3067, -1.8836]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0929, -3.3752, -1.9490],\n",
      "        [ 4.1997, -3.2490, -2.1321]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0929, -3.3752, -1.9490],\n",
      "        [ 4.1997, -3.2490, -2.1321]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3561, -2.7739, -1.5285],\n",
      "        [ 4.2059, -3.3623, -2.1738]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3561, -2.7739, -1.5285],\n",
      "        [ 4.2059, -3.3623, -2.1738]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2852, -3.3885, -2.0940],\n",
      "        [ 4.0008, -3.2252, -1.9443]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2852, -3.3885, -2.0940],\n",
      "        [ 4.0008, -3.2252, -1.9443]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9356, -3.2273, -1.7103],\n",
      "        [ 3.5172, -2.9438, -1.4311]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9356, -3.2273, -1.7103],\n",
      "        [ 3.5172, -2.9438, -1.4311]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3288, -2.7910, -1.2842],\n",
      "        [ 3.0013, -2.6218, -1.1531]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3288, -2.7910, -1.2842],\n",
      "        [ 3.0013, -2.6218, -1.1531]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6480, -2.1089, -1.0795],\n",
      "        [ 3.4092, -2.9456, -1.6936]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6480, -2.1089, -1.0795],\n",
      "        [ 3.4092, -2.9456, -1.6936]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.5789,  2.1514, -0.8479],\n",
      "        [ 4.0733, -3.2596, -1.8805]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.5789,  2.1514, -0.8479],\n",
      "        [ 4.0733, -3.2596, -1.8805]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3299, -3.3497, -2.1489],\n",
      "        [ 3.7550, -2.7695, -1.5454]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3299, -3.3497, -2.1489],\n",
      "        [ 3.7550, -2.7695, -1.5454]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.4795, -2.4484, -1.2571],\n",
      "        [ 3.3762, -2.5091, -1.4551]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.4795, -2.4484, -1.2571],\n",
      "        [ 3.3762, -2.5091, -1.4551]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0496, -3.1622, -2.0656],\n",
      "        [ 3.2845, -2.9424, -1.2879]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0496, -3.1622, -2.0656],\n",
      "        [ 3.2845, -2.9424, -1.2879]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9981, -3.2499, -2.0010],\n",
      "        [ 4.1530, -3.1158, -1.9592]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9981, -3.2499, -2.0010],\n",
      "        [ 4.1530, -3.1158, -1.9592]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1510, -3.2317, -2.0776],\n",
      "        [ 3.5501, -2.7721, -1.6511]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1510, -3.2317, -2.0776],\n",
      "        [ 3.5501, -2.7721, -1.6511]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1303, -3.1761, -2.0609],\n",
      "        [-1.7791,  2.4650, -0.5681]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1303, -3.1761, -2.0609],\n",
      "        [-1.7791,  2.4650, -0.5681]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9949, -3.2935, -1.8432],\n",
      "        [ 3.1053, -2.9395, -1.1104]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9949, -3.2935, -1.8432],\n",
      "        [ 3.1053, -2.9395, -1.1104]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7869, -3.3761, -1.7013],\n",
      "        [ 3.6707, -3.0598, -1.4540]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7869, -3.3761, -1.7013],\n",
      "        [ 3.6707, -3.0598, -1.4540]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8762, -3.2369, -2.1088],\n",
      "        [ 3.9043, -3.3843, -1.5945]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8762, -3.2369, -2.1088],\n",
      "        [ 3.9043, -3.3843, -1.5945]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3265, -3.3303, -2.1173],\n",
      "        [ 3.6960, -3.3081, -1.5823]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3265, -3.3303, -2.1173],\n",
      "        [ 3.6960, -3.3081, -1.5823]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9060, -3.5583, -1.8612],\n",
      "        [ 3.8196, -3.1123, -1.5128]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9060, -3.5583, -1.8612],\n",
      "        [ 3.8196, -3.1123, -1.5128]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5113, -3.2575, -1.4798],\n",
      "        [ 4.0821, -3.3140, -2.1818]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5113, -3.2575, -1.4798],\n",
      "        [ 4.0821, -3.3140, -2.1818]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6109, -3.1929, -1.6243],\n",
      "        [ 3.2933, -2.8013, -1.6118]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6109, -3.1929, -1.6243],\n",
      "        [ 3.2933, -2.8013, -1.6118]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0556, -3.2090, -1.7556],\n",
      "        [ 3.7467, -3.1122, -1.6961]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0556, -3.2090, -1.7556],\n",
      "        [ 3.7467, -3.1122, -1.6961]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9828, -3.1662, -1.5301],\n",
      "        [ 2.3894, -2.1792, -1.3285]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9828, -3.1662, -1.5301],\n",
      "        [ 2.3894, -2.1792, -1.3285]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5495, -1.6783, -0.2758],\n",
      "        [ 3.6142, -3.3076, -1.3414]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.5495, -1.6783, -0.2758],\n",
      "        [ 3.6142, -3.3076, -1.3414]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8713, -2.8487, -1.0866],\n",
      "        [ 4.1552, -3.2207, -1.6817]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8713, -2.8487, -1.0866],\n",
      "        [ 4.1552, -3.2207, -1.6817]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.1938, -3.1522, -1.0818],\n",
      "        [ 3.7798, -2.5384, -2.1238]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.1938, -3.1522, -1.0818],\n",
      "        [ 3.7798, -2.5384, -2.1238]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7911, -3.2370, -1.7039],\n",
      "        [ 4.0493, -3.3961, -1.9015]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7911, -3.2370, -1.7039],\n",
      "        [ 4.0493, -3.3961, -1.9015]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0424, -3.1961, -1.8654],\n",
      "        [-0.7932,  2.0690, -1.1322]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0424, -3.1961, -1.8654],\n",
      "        [-0.7932,  2.0690, -1.1322]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8031, -3.4447, -1.5524],\n",
      "        [ 3.6738, -2.8543, -2.2356]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8031, -3.4447, -1.5524],\n",
      "        [ 3.6738, -2.8543, -2.2356]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7651, -1.9663, -0.4387],\n",
      "        [ 3.4507, -2.9655, -1.2773]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.7651, -1.9663, -0.4387],\n",
      "        [ 3.4507, -2.9655, -1.2773]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3639, -3.1749, -0.8570],\n",
      "        [ 3.2474, -3.2128, -1.3407]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3639, -3.1749, -0.8570],\n",
      "        [ 3.2474, -3.2128, -1.3407]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6877, -2.7587, -0.9700],\n",
      "        [ 3.8758, -3.4442, -1.7059]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.6877, -2.7587, -0.9700],\n",
      "        [ 3.8758, -3.4442, -1.7059]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0646, -3.1626, -2.1623],\n",
      "        [ 3.8635, -2.9276, -1.8325]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0646, -3.1626, -2.1623],\n",
      "        [ 3.8635, -2.9276, -1.8325]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1480, -3.2519, -2.0283],\n",
      "        [ 4.1347, -3.2501, -2.2203]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1480, -3.2519, -2.0283],\n",
      "        [ 4.1347, -3.2501, -2.2203]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2475, -3.4746, -2.0397],\n",
      "        [ 4.4005, -3.5035, -2.1718]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2475, -3.4746, -2.0397],\n",
      "        [ 4.4005, -3.5035, -2.1718]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0757, -3.3944, -2.0491],\n",
      "        [ 4.1244, -3.1458, -2.1009]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0757, -3.3944, -2.0491],\n",
      "        [ 4.1244, -3.1458, -2.1009]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0165, -3.3565, -1.8104],\n",
      "        [ 4.2299, -3.3199, -2.0387]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0165, -3.3565, -1.8104],\n",
      "        [ 4.2299, -3.3199, -2.0387]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7968, -2.3580, -2.5533],\n",
      "        [-1.5655,  2.7982, -0.7570]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7968, -2.3580, -2.5533],\n",
      "        [-1.5655,  2.7982, -0.7570]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2012, -3.0949, -2.1616],\n",
      "        [-1.4743,  2.8571, -0.8136]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2012, -3.0949, -2.1616],\n",
      "        [-1.4743,  2.8571, -0.8136]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1559, -3.4145, -1.9475],\n",
      "        [ 4.3856, -3.4538, -2.1114]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1559, -3.4145, -1.9475],\n",
      "        [ 4.3856, -3.4538, -2.1114]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2407, -3.1393, -2.3235],\n",
      "        [ 4.3231, -3.4136, -2.2457]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2407, -3.1393, -2.3235],\n",
      "        [ 4.3231, -3.4136, -2.2457]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1502, -3.0255, -2.4534],\n",
      "        [ 3.8843, -3.3502, -1.8407]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1502, -3.0255, -2.4534],\n",
      "        [ 3.8843, -3.3502, -1.8407]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1350, -2.9141, -1.8852],\n",
      "        [ 4.3179, -3.2562, -2.1860]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1350, -2.9141, -1.8852],\n",
      "        [ 4.3179, -3.2562, -2.1860]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3581, -2.8265, -2.1819],\n",
      "        [-2.3080,  3.1357, -0.4770]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3581, -2.8265, -2.1819],\n",
      "        [-2.3080,  3.1357, -0.4770]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0564, -3.4119, -1.9434],\n",
      "        [ 4.2786, -3.0219, -2.2340]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0564, -3.4119, -1.9434],\n",
      "        [ 4.2786, -3.0219, -2.2340]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0030, -3.2922, -1.7899],\n",
      "        [ 4.2759, -3.2106, -2.2565]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0030, -3.2922, -1.7899],\n",
      "        [ 4.2759, -3.2106, -2.2565]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4201, -3.2574, -1.8397],\n",
      "        [ 4.3205, -3.3158, -2.0416]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4201, -3.2574, -1.8397],\n",
      "        [ 4.3205, -3.3158, -2.0416]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3840, -3.3400, -2.2447],\n",
      "        [ 4.1021, -3.1239, -2.0477]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3840, -3.3400, -2.2447],\n",
      "        [ 4.1021, -3.1239, -2.0477]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1675, -3.3556, -2.4005],\n",
      "        [ 4.1120, -3.0538, -1.9960]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1675, -3.3556, -2.4005],\n",
      "        [ 4.1120, -3.0538, -1.9960]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3077, -3.5465, -1.9935],\n",
      "        [ 4.1693, -3.2454, -2.2294]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3077, -3.5465, -1.9935],\n",
      "        [ 4.1693, -3.2454, -2.2294]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3626, -2.9541, -2.2228],\n",
      "        [ 4.2233, -3.1309, -2.0089]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3626, -2.9541, -2.2228],\n",
      "        [ 4.2233, -3.1309, -2.0089]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3237, -3.1159, -2.6243],\n",
      "        [ 4.0133, -2.8537, -2.2931]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3237, -3.1159, -2.6243],\n",
      "        [ 4.0133, -2.8537, -2.2931]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1266, -3.3180, -2.1546],\n",
      "        [ 4.1999, -2.8378, -2.4519]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1266, -3.3180, -2.1546],\n",
      "        [ 4.1999, -2.8378, -2.4519]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2086, -3.1116, -2.3195],\n",
      "        [ 4.0092, -3.1051, -2.3544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2086, -3.1116, -2.3195],\n",
      "        [ 4.0092, -3.1051, -2.3544]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8661, -2.9234, -2.1835],\n",
      "        [-2.4837,  3.0480, -0.8444]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8661, -2.9234, -2.1835],\n",
      "        [-2.4837,  3.0480, -0.8444]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2730, -3.0625, -2.1998],\n",
      "        [ 4.1687, -3.2163, -2.2402]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2730, -3.0625, -2.1998],\n",
      "        [ 4.1687, -3.2163, -2.2402]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4864, -2.6656, -1.3094],\n",
      "        [ 4.2377, -3.0770, -2.4241]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4864, -2.6656, -1.3094],\n",
      "        [ 4.2377, -3.0770, -2.4241]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3105, -3.1018, -2.3098],\n",
      "        [ 4.1929, -3.0271, -2.0957]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3105, -3.1018, -2.3098],\n",
      "        [ 4.1929, -3.0271, -2.0957]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3955, -2.7365, -2.3059],\n",
      "        [-2.0163,  3.0310, -0.8532]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3955, -2.7365, -2.3059],\n",
      "        [-2.0163,  3.0310, -0.8532]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2642, -2.8841, -2.4648],\n",
      "        [ 4.3861, -2.6084, -2.3397]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2642, -2.8841, -2.4648],\n",
      "        [ 4.3861, -2.6084, -2.3397]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8806, -2.7489, -2.2616],\n",
      "        [ 4.2862, -2.9066, -2.3756]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8806, -2.7489, -2.2616],\n",
      "        [ 4.2862, -2.9066, -2.3756]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0520, -2.3420, -2.5143],\n",
      "        [-1.9011,  2.9542, -0.9950]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0520, -2.3420, -2.5143],\n",
      "        [-1.9011,  2.9542, -0.9950]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5926, -1.6216, -2.5002],\n",
      "        [ 4.1843, -2.9629, -2.3766]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.5926, -1.6216, -2.5002],\n",
      "        [ 4.1843, -2.9629, -2.3766]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0732, -2.6351, -2.3384],\n",
      "        [ 4.1527, -3.0960, -2.4563]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0732, -2.6351, -2.3384],\n",
      "        [ 4.1527, -3.0960, -2.4563]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0925, -2.9993, -2.2322],\n",
      "        [ 4.2054, -2.7284, -2.3062]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0925, -2.9993, -2.2322],\n",
      "        [ 4.2054, -2.7284, -2.3062]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1715, -3.4073, -2.3279],\n",
      "        [ 4.3253, -2.8029, -2.5875]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1715, -3.4073, -2.3279],\n",
      "        [ 4.3253, -2.8029, -2.5875]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.7462,  3.0366, -1.0423],\n",
      "        [-2.1967,  3.3820, -0.7805]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.7462,  3.0366, -1.0423],\n",
      "        [-2.1967,  3.3820, -0.7805]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2861, -2.7082, -2.5183],\n",
      "        [ 4.0920, -2.6004, -2.5633]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2861, -2.7082, -2.5183],\n",
      "        [ 4.0920, -2.6004, -2.5633]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4975, -2.9224, -2.2397],\n",
      "        [ 4.3225, -3.1190, -2.4915]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4975, -2.9224, -2.2397],\n",
      "        [ 4.3225, -3.1190, -2.4915]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3194, -2.9296, -2.4052],\n",
      "        [ 4.2897, -3.2830, -2.2773]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3194, -2.9296, -2.4052],\n",
      "        [ 4.2897, -3.2830, -2.2773]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4898, -3.0975, -2.5758],\n",
      "        [ 4.1348, -3.2107, -2.3103]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4898, -3.0975, -2.5758],\n",
      "        [ 4.1348, -3.2107, -2.3103]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2221, -3.3480, -2.2209],\n",
      "        [ 4.3430, -3.5498, -2.2865]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2221, -3.3480, -2.2209],\n",
      "        [ 4.3430, -3.5498, -2.2865]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3596, -2.7465, -2.6757],\n",
      "        [ 4.3912, -3.3126, -2.2331]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3596, -2.7465, -2.6757],\n",
      "        [ 4.3912, -3.3126, -2.2331]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3441, -3.3288, -2.4876],\n",
      "        [ 4.2191, -3.0145, -2.6033]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3441, -3.3288, -2.4876],\n",
      "        [ 4.2191, -3.0145, -2.6033]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1980, -3.0858, -2.3487],\n",
      "        [ 4.3084, -3.3170, -1.9441]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1980, -3.0858, -2.3487],\n",
      "        [ 4.3084, -3.3170, -1.9441]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1927, -3.1288, -1.9644],\n",
      "        [ 4.4907, -3.0909, -2.4681]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1927, -3.1288, -1.9644],\n",
      "        [ 4.4907, -3.0909, -2.4681]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3909, -3.3413, -2.5464],\n",
      "        [-2.1988,  3.3902, -0.7269]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3909, -3.3413, -2.5464],\n",
      "        [-2.1988,  3.3902, -0.7269]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2534, -3.0082, -2.2850],\n",
      "        [ 4.1151, -2.9168, -2.5333]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2534, -3.0082, -2.2850],\n",
      "        [ 4.1151, -2.9168, -2.5333]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6246, -3.1506, -2.2989],\n",
      "        [ 4.4297, -2.8110, -2.7501]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6246, -3.1506, -2.2989],\n",
      "        [ 4.4297, -2.8110, -2.7501]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3352, -2.8855, -2.6327],\n",
      "        [ 4.2961, -3.3086, -2.4408]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3352, -2.8855, -2.6327],\n",
      "        [ 4.2961, -3.3086, -2.4408]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3908, -3.2065, -2.2501],\n",
      "        [ 4.2946, -3.2233, -2.3508]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3908, -3.2065, -2.2501],\n",
      "        [ 4.2946, -3.2233, -2.3508]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0945, -2.6717, -2.6766],\n",
      "        [ 4.5112, -3.2646, -2.3542]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0945, -2.6717, -2.6766],\n",
      "        [ 4.5112, -3.2646, -2.3542]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2440, -3.5362, -2.2771],\n",
      "        [ 4.4821, -3.3685, -2.2805]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2440, -3.5362, -2.2771],\n",
      "        [ 4.4821, -3.3685, -2.2805]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2113, -3.2486, -2.4226],\n",
      "        [ 4.1946, -2.9194, -2.4830]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2113, -3.2486, -2.4226],\n",
      "        [ 4.1946, -2.9194, -2.4830]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3735, -3.2358, -2.4650],\n",
      "        [ 4.5438, -2.7839, -2.6233]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3735, -3.2358, -2.4650],\n",
      "        [ 4.5438, -2.7839, -2.6233]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4587, -3.2641, -2.4309],\n",
      "        [-2.2083,  3.1588, -1.0028]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4587, -3.2641, -2.4309],\n",
      "        [-2.2083,  3.1588, -1.0028]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.1599,  3.3701, -0.8973],\n",
      "        [ 4.4911, -3.1055, -1.9578]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.1599,  3.3701, -0.8973],\n",
      "        [ 4.4911, -3.1055, -1.9578]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2228, -3.0060, -2.6940],\n",
      "        [ 4.1940, -3.2361, -2.1173]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2228, -3.0060, -2.6940],\n",
      "        [ 4.1940, -3.2361, -2.1173]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3954, -3.3725, -2.4493],\n",
      "        [ 4.4128, -3.5298, -2.3758]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3954, -3.3725, -2.4493],\n",
      "        [ 4.4128, -3.5298, -2.3758]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6503, -3.3499, -2.4901],\n",
      "        [ 3.8935, -2.9351, -2.3318]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6503, -3.3499, -2.4901],\n",
      "        [ 3.8935, -2.9351, -2.3318]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5158, -3.5385, -2.1780],\n",
      "        [ 3.9551, -3.4366, -1.6429]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5158, -3.5385, -2.1780],\n",
      "        [ 3.9551, -3.4366, -1.6429]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3815, -2.8510, -2.4185],\n",
      "        [ 4.4105, -3.2506, -2.3525]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3815, -2.8510, -2.4185],\n",
      "        [ 4.4105, -3.2506, -2.3525]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4446, -3.7074, -2.1730],\n",
      "        [ 4.5973, -3.4604, -2.1402]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4446, -3.7074, -2.1730],\n",
      "        [ 4.5973, -3.4604, -2.1402]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0777, -2.9709, -2.2983],\n",
      "        [ 4.2758, -3.5355, -2.0982]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0777, -2.9709, -2.2983],\n",
      "        [ 4.2758, -3.5355, -2.0982]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4150, -3.0311, -2.5979],\n",
      "        [ 4.4637, -2.9582, -2.5867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4150, -3.0311, -2.5979],\n",
      "        [ 4.4637, -2.9582, -2.5867]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3706, -3.3048, -2.2954],\n",
      "        [ 4.2834, -3.5377, -1.8646]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3706, -3.3048, -2.2954],\n",
      "        [ 4.2834, -3.5377, -1.8646]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1115, -3.0626, -2.4130],\n",
      "        [ 3.8917, -2.8143, -2.4811]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1115, -3.0626, -2.4130],\n",
      "        [ 3.8917, -2.8143, -2.4811]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5842, -3.5233, -2.3313],\n",
      "        [ 4.4588, -3.3999, -2.2199]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5842, -3.5233, -2.3313],\n",
      "        [ 4.4588, -3.3999, -2.2199]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3641, -3.0862, -2.1503],\n",
      "        [-2.1499,  3.2384, -0.8512]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3641, -3.0862, -2.1503],\n",
      "        [-2.1499,  3.2384, -0.8512]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2336, -3.3910, -2.1647],\n",
      "        [ 4.4269, -3.5374, -2.1373]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2336, -3.3910, -2.1647],\n",
      "        [ 4.4269, -3.5374, -2.1373]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3962, -3.3959, -2.1717],\n",
      "        [ 3.5515, -3.1150, -1.2971]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3962, -3.3959, -2.1717],\n",
      "        [ 3.5515, -3.1150, -1.2971]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4264, -3.1385, -2.4784],\n",
      "        [ 4.1284, -3.4881, -1.8400]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4264, -3.1385, -2.4784],\n",
      "        [ 4.1284, -3.4881, -1.8400]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8498, -3.2530, -1.6743],\n",
      "        [-1.9567,  3.2794, -1.0527]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8498, -3.2530, -1.6743],\n",
      "        [-1.9567,  3.2794, -1.0527]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5082, -3.5061, -2.2644],\n",
      "        [ 4.2483, -3.5454, -2.2912]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5082, -3.5061, -2.2644],\n",
      "        [ 4.2483, -3.5454, -2.2912]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4456, -3.4499, -2.2305],\n",
      "        [ 4.2242, -3.2606, -2.4349]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4456, -3.4499, -2.2305],\n",
      "        [ 4.2242, -3.2606, -2.4349]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2091, -3.6241, -2.1492],\n",
      "        [ 4.2577, -3.4854, -1.8187]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2091, -3.6241, -2.1492],\n",
      "        [ 4.2577, -3.4854, -1.8187]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2703, -3.4832, -2.2637],\n",
      "        [ 4.5351, -3.3803, -2.2039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2703, -3.4832, -2.2637],\n",
      "        [ 4.5351, -3.3803, -2.2039]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9540, -2.4972, -2.5630],\n",
      "        [ 2.5991, -2.7661, -0.8264]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9540, -2.4972, -2.5630],\n",
      "        [ 2.5991, -2.7661, -0.8264]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1756, -3.3795, -1.8100],\n",
      "        [ 4.3156, -3.4269, -2.2694]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1756, -3.3795, -1.8100],\n",
      "        [ 4.3156, -3.4269, -2.2694]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2412, -3.1199, -2.4943],\n",
      "        [-2.1581,  3.5352, -0.9627]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2412, -3.1199, -2.4943],\n",
      "        [-2.1581,  3.5352, -0.9627]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.4099,  3.5372, -1.0050],\n",
      "        [ 4.4505, -3.4740, -2.1546]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.4099,  3.5372, -1.0050],\n",
      "        [ 4.4505, -3.4740, -2.1546]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0410, -3.4605, -2.0181],\n",
      "        [ 2.8703, -0.8139, -2.6588]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0410, -3.4605, -2.0181],\n",
      "        [ 2.8703, -0.8139, -2.6588]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7036, -2.5594, -0.9816],\n",
      "        [-2.1356,  3.4008, -1.1631]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7036, -2.5594, -0.9816],\n",
      "        [-2.1356,  3.4008, -1.1631]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4755, -3.2013, -2.2963],\n",
      "        [ 4.2071, -3.1463, -2.5589]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4755, -3.2013, -2.2963],\n",
      "        [ 4.2071, -3.1463, -2.5589]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2028, -2.6961, -1.1569],\n",
      "        [ 4.2417, -3.2470, -1.9698]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2028, -2.6961, -1.1569],\n",
      "        [ 4.2417, -3.2470, -1.9698]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9755, -3.4113, -1.9863],\n",
      "        [ 4.1604, -2.9487, -2.2504]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9755, -3.4113, -1.9863],\n",
      "        [ 4.1604, -2.9487, -2.2504]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1692, -3.3419, -2.0381],\n",
      "        [ 3.2310, -3.0016, -1.2385]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1692, -3.3419, -2.0381],\n",
      "        [ 3.2310, -3.0016, -1.2385]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1181, -3.5698, -1.8145],\n",
      "        [ 3.9880, -3.1349, -2.1476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1181, -3.5698, -1.8145],\n",
      "        [ 3.9880, -3.1349, -2.1476]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4148, -3.3063, -1.0477],\n",
      "        [ 4.3082, -3.5766, -2.0846]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4148, -3.3063, -1.0477],\n",
      "        [ 4.3082, -3.5766, -2.0846]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6083, -3.0025, -1.9153],\n",
      "        [ 1.9957, -0.9909, -2.0593]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.6083, -3.0025, -1.9153],\n",
      "        [ 1.9957, -0.9909, -2.0593]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.2317, -3.1535, -1.2331],\n",
      "        [ 3.9267, -3.2983, -1.7358]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.2317, -3.1535, -1.2331],\n",
      "        [ 3.9267, -3.2983, -1.7358]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3393, -2.9145, -1.0829],\n",
      "        [ 3.1069, -3.0160, -1.0152]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3393, -2.9145, -1.0829],\n",
      "        [ 3.1069, -3.0160, -1.0152]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3113, -3.5360, -2.2082],\n",
      "        [ 4.2424, -3.4385, -1.9763]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3113, -3.5360, -2.2082],\n",
      "        [ 4.2424, -3.4385, -1.9763]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2042, -3.3097, -2.2237],\n",
      "        [ 4.3507, -3.1232, -2.1302]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2042, -3.3097, -2.2237],\n",
      "        [ 4.3507, -3.1232, -2.1302]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7186, -3.2471, -1.3767],\n",
      "        [ 4.2229, -3.4315, -1.6539]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7186, -3.2471, -1.3767],\n",
      "        [ 4.2229, -3.4315, -1.6539]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9869, -2.6588, -2.2618],\n",
      "        [ 4.3311, -3.4343, -1.9659]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9869, -2.6588, -2.2618],\n",
      "        [ 4.3311, -3.4343, -1.9659]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4445, -3.2986, -2.2263],\n",
      "        [ 4.0901, -3.1093, -2.1723]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4445, -3.2986, -2.2263],\n",
      "        [ 4.0901, -3.1093, -2.1723]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.9421, -1.9539, -0.3347],\n",
      "        [ 0.5113, -1.0982,  0.1517]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.9421, -1.9539, -0.3347],\n",
      "        [ 0.5113, -1.0982,  0.1517]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3452, -3.3031, -2.1442],\n",
      "        [ 4.5180, -3.6528, -2.2010]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3452, -3.3031, -2.1442],\n",
      "        [ 4.5180, -3.6528, -2.2010]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0632, -3.2074, -2.1545],\n",
      "        [ 4.1990, -2.7837, -2.4439]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0632, -3.2074, -2.1545],\n",
      "        [ 4.1990, -2.7837, -2.4439]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2289, -3.3774, -2.1539],\n",
      "        [ 4.3204, -3.6310, -2.0100]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2289, -3.3774, -2.1539],\n",
      "        [ 4.3204, -3.6310, -2.0100]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1456, -2.7405, -2.3452],\n",
      "        [ 4.1794, -3.3220, -2.2986]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1456, -2.7405, -2.3452],\n",
      "        [ 4.1794, -3.3220, -2.2986]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4237, -3.5085, -2.0606],\n",
      "        [ 4.0520, -2.9872, -2.2779]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4237, -3.5085, -2.0606],\n",
      "        [ 4.0520, -2.9872, -2.2779]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5169, -3.2103, -2.2423],\n",
      "        [ 4.2087, -3.4013, -1.9744]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5169, -3.2103, -2.2423],\n",
      "        [ 4.2087, -3.4013, -1.9744]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1334, -3.3344, -2.2393],\n",
      "        [ 4.3593, -3.4613, -1.7747]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1334, -3.3344, -2.2393],\n",
      "        [ 4.3593, -3.4613, -1.7747]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3021, -3.3147, -2.0236],\n",
      "        [ 3.2065, -1.2168, -2.4262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3021, -3.3147, -2.0236],\n",
      "        [ 3.2065, -1.2168, -2.4262]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.6129, -1.8657, -0.2699],\n",
      "        [ 4.1111, -2.7795, -2.3064]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.6129, -1.8657, -0.2699],\n",
      "        [ 4.1111, -2.7795, -2.3064]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3366, -3.4706, -2.1129],\n",
      "        [ 4.4150, -3.5278, -2.3991]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3366, -3.4706, -2.1129],\n",
      "        [ 4.4150, -3.5278, -2.3991]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4537, -3.6632, -2.4753],\n",
      "        [ 4.2749, -3.2201, -2.1773]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4537, -3.6632, -2.4753],\n",
      "        [ 4.2749, -3.2201, -2.1773]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7659, -3.3268, -1.5323],\n",
      "        [ 3.3822, -3.2021, -1.1718]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7659, -3.3268, -1.5323],\n",
      "        [ 3.3822, -3.2021, -1.1718]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0544, -3.0494, -2.0468],\n",
      "        [ 4.1168, -3.3751, -2.3573]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0544, -3.0494, -2.0468],\n",
      "        [ 4.1168, -3.3751, -2.3573]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0042, -2.1846, -0.4769],\n",
      "        [ 4.2465, -3.2371, -1.9748]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.0042, -2.1846, -0.4769],\n",
      "        [ 4.2465, -3.2371, -1.9748]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5875, -3.3136, -2.2414],\n",
      "        [ 4.3420, -3.3595, -2.1470]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5875, -3.3136, -2.2414],\n",
      "        [ 4.3420, -3.3595, -2.1470]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8568, -2.7009, -2.0958],\n",
      "        [ 2.8390, -2.6684, -1.3200]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8568, -2.7009, -2.0958],\n",
      "        [ 2.8390, -2.6684, -1.3200]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3292, -3.3578, -2.1137],\n",
      "        [ 0.5759, -1.1269, -0.0553]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3292, -3.3578, -2.1137],\n",
      "        [ 0.5759, -1.1269, -0.0553]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3001, -3.2074, -2.2879],\n",
      "        [ 4.3077, -3.1181, -2.2656]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3001, -3.2074, -2.2879],\n",
      "        [ 4.3077, -3.1181, -2.2656]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4088, -2.8068, -2.2814],\n",
      "        [ 4.0751, -3.2418, -2.0341]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4088, -2.8068, -2.2814],\n",
      "        [ 4.0751, -3.2418, -2.0341]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3041, -3.1329, -2.4708],\n",
      "        [ 4.6776, -3.3793, -2.2435]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3041, -3.1329, -2.4708],\n",
      "        [ 4.6776, -3.3793, -2.2435]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3152, -0.6956,  0.6672],\n",
      "        [ 4.3093, -2.7690, -2.2729]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.3152, -0.6956,  0.6672],\n",
      "        [ 4.3093, -2.7690, -2.2729]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4229, -3.0906, -2.5928],\n",
      "        [ 4.1393, -2.6651, -2.7762]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4229, -3.0906, -2.5928],\n",
      "        [ 4.1393, -2.6651, -2.7762]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1322, -0.4559,  0.6306],\n",
      "        [ 2.5703, -2.2015, -0.8033]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.1322, -0.4559,  0.6306],\n",
      "        [ 2.5703, -2.2015, -0.8033]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3630, -2.4652, -0.9018],\n",
      "        [ 4.4710, -3.4826, -2.3921]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.3630, -2.4652, -0.9018],\n",
      "        [ 4.4710, -3.4826, -2.3921]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.0769, -1.4178, -0.1651],\n",
      "        [ 4.4468, -3.1355, -2.4341]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.0769, -1.4178, -0.1651],\n",
      "        [ 4.4468, -3.1355, -2.4341]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4186, -2.9135, -2.5308],\n",
      "        [ 4.5826, -3.4109, -2.3168]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4186, -2.9135, -2.5308],\n",
      "        [ 4.5826, -3.4109, -2.3168]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5786, -3.1649, -2.5008],\n",
      "        [ 2.9028, -1.0831, -2.6279]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5786, -3.1649, -2.5008],\n",
      "        [ 2.9028, -1.0831, -2.6279]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3603, -3.1159, -2.6250],\n",
      "        [ 4.4256, -3.2478, -2.1689]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3603, -3.1159, -2.6250],\n",
      "        [ 4.4256, -3.2478, -2.1689]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5140, -3.5386, -2.2697],\n",
      "        [ 4.4562, -3.2084, -2.4498]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5140, -3.5386, -2.2697],\n",
      "        [ 4.4562, -3.2084, -2.4498]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4302, -3.3412, -2.2173],\n",
      "        [-0.0409, -0.9872,  0.1349]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4302, -3.3412, -2.2173],\n",
      "        [-0.0409, -0.9872,  0.1349]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4278, -3.6034, -2.1909],\n",
      "        [ 4.6544, -3.4814, -2.3040]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4278, -3.6034, -2.1909],\n",
      "        [ 4.6544, -3.4814, -2.3040]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4232, -3.6412, -2.0095],\n",
      "        [ 4.4637, -3.2098, -2.4146]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4232, -3.6412, -2.0095],\n",
      "        [ 4.4637, -3.2098, -2.4146]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3854, -3.2286, -2.5554],\n",
      "        [ 4.4406, -3.4922, -2.2770]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3854, -3.2286, -2.5554],\n",
      "        [ 4.4406, -3.4922, -2.2770]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3001, -3.4321, -2.0073],\n",
      "        [ 4.5810, -3.6932, -2.0958]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3001, -3.4321, -2.0073],\n",
      "        [ 4.5810, -3.6932, -2.0958]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4308, -3.6535, -2.2780],\n",
      "        [ 4.4599, -3.0244, -2.1238]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4308, -3.6535, -2.2780],\n",
      "        [ 4.4599, -3.0244, -2.1238]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3127, -1.1593,  0.2753],\n",
      "        [ 4.5910, -3.1872, -2.3724]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.3127, -1.1593,  0.2753],\n",
      "        [ 4.5910, -3.1872, -2.3724]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1574, -3.2735, -2.4591],\n",
      "        [ 4.5680, -3.1204, -2.4228]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1574, -3.2735, -2.4591],\n",
      "        [ 4.5680, -3.1204, -2.4228]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3757, -3.5951, -1.8620],\n",
      "        [ 4.0668, -3.2675, -1.9034]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3757, -3.5951, -1.8620],\n",
      "        [ 4.0668, -3.2675, -1.9034]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2607, -3.1813, -2.2617],\n",
      "        [ 4.6851, -3.5750, -2.2504]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2607, -3.1813, -2.2617],\n",
      "        [ 4.6851, -3.5750, -2.2504]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5771, -3.0739, -2.4928],\n",
      "        [ 4.2651, -3.5038, -2.0315]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5771, -3.0739, -2.4928],\n",
      "        [ 4.2651, -3.5038, -2.0315]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4178, -3.1250, -2.1772],\n",
      "        [ 4.4840, -3.5098, -2.1007]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4178, -3.1250, -2.1772],\n",
      "        [ 4.4840, -3.5098, -2.1007]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5301, -3.3682, -2.3880],\n",
      "        [ 4.3953, -3.1116, -2.3960]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5301, -3.3682, -2.3880],\n",
      "        [ 4.3953, -3.1116, -2.3960]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3301, -1.6763, -2.6123],\n",
      "        [ 4.3210, -3.4991, -2.1199]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3301, -1.6763, -2.6123],\n",
      "        [ 4.3210, -3.4991, -2.1199]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3166, -3.6797, -1.9230],\n",
      "        [ 4.3885, -2.9286, -2.5854]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3166, -3.6797, -1.9230],\n",
      "        [ 4.3885, -2.9286, -2.5854]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4484, -3.5099, -2.2077],\n",
      "        [ 4.3104, -3.2468, -2.5472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4484, -3.5099, -2.2077],\n",
      "        [ 4.3104, -3.2468, -2.5472]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4276, -3.3831, -2.1989],\n",
      "        [ 4.3795, -3.6534, -1.8069]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4276, -3.3831, -2.1989],\n",
      "        [ 4.3795, -3.6534, -1.8069]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0833, -2.7401, -2.3573],\n",
      "        [ 4.5831, -3.5467, -2.1763]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0833, -2.7401, -2.3573],\n",
      "        [ 4.5831, -3.5467, -2.1763]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5159, -3.5542, -2.1894],\n",
      "        [ 4.5301, -3.3410, -2.2050]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5159, -3.5542, -2.1894],\n",
      "        [ 4.5301, -3.3410, -2.2050]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3505, -3.3629, -2.3110],\n",
      "        [ 4.4191, -3.2936, -2.2964]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3505, -3.3629, -2.3110],\n",
      "        [ 4.4191, -3.2936, -2.2964]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0922, -3.0602, -2.1190],\n",
      "        [ 4.5546, -2.9749, -2.5514]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0922, -3.0602, -2.1190],\n",
      "        [ 4.5546, -2.9749, -2.5514]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2845, -3.1931, -2.3378],\n",
      "        [ 4.6586, -3.5389, -2.2191]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2845, -3.1931, -2.3378],\n",
      "        [ 4.6586, -3.5389, -2.2191]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.5408,  3.7871, -1.1171],\n",
      "        [ 3.6424, -3.1475, -1.5659]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.5408,  3.7871, -1.1171],\n",
      "        [ 3.6424, -3.1475, -1.5659]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5521, -3.5265, -2.2008],\n",
      "        [ 4.4736, -3.3872, -2.1691]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5521, -3.5265, -2.2008],\n",
      "        [ 4.4736, -3.3872, -2.1691]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9851, -3.6562, -1.7587],\n",
      "        [ 4.2735, -3.2969, -2.1499]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9851, -3.6562, -1.7587],\n",
      "        [ 4.2735, -3.2969, -2.1499]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0378, -2.1153, -0.5135],\n",
      "        [ 4.5217, -2.9602, -2.3485]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.0378, -2.1153, -0.5135],\n",
      "        [ 4.5217, -2.9602, -2.3485]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0831, -3.0232, -2.4545],\n",
      "        [ 4.5145, -3.3278, -2.3038]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0831, -3.0232, -2.4545],\n",
      "        [ 4.5145, -3.3278, -2.3038]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7213, -3.2256, -1.7388],\n",
      "        [ 4.4142, -3.1404, -2.2443]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7213, -3.2256, -1.7388],\n",
      "        [ 4.4142, -3.1404, -2.2443]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5646, -3.4402, -2.4259],\n",
      "        [ 4.2987, -2.9951, -2.0123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5646, -3.4402, -2.4259],\n",
      "        [ 4.2987, -2.9951, -2.0123]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2235, -3.1644, -2.3114],\n",
      "        [ 4.3124, -2.7041, -2.2156]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2235, -3.1644, -2.3114],\n",
      "        [ 4.3124, -2.7041, -2.2156]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.0917, -0.9257,  0.3410],\n",
      "        [ 4.5881, -3.1882, -2.3127]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.0917, -0.9257,  0.3410],\n",
      "        [ 4.5881, -3.1882, -2.3127]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3357, -3.2746, -2.3355],\n",
      "        [ 4.2132, -3.3420, -1.9595]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3357, -3.2746, -2.3355],\n",
      "        [ 4.2132, -3.3420, -1.9595]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0849, -2.6807, -2.5495],\n",
      "        [ 4.2533, -3.1930, -2.0867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0849, -2.6807, -2.5495],\n",
      "        [ 4.2533, -3.1930, -2.0867]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1116, -2.7498, -1.8617],\n",
      "        [ 4.3489, -3.0641, -2.0286]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1116, -2.7498, -1.8617],\n",
      "        [ 4.3489, -3.0641, -2.0286]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1169, -3.1007, -2.0310],\n",
      "        [-2.4250,  3.3968, -0.9470]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1169, -3.1007, -2.0310],\n",
      "        [-2.4250,  3.3968, -0.9470]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0095, -3.4321, -1.7361],\n",
      "        [ 4.2139, -3.2705, -1.9055]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0095, -3.4321, -1.7361],\n",
      "        [ 4.2139, -3.2705, -1.9055]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9664, -3.4126, -1.9683],\n",
      "        [-2.6029,  3.4999, -0.6858]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9664, -3.4126, -1.9683],\n",
      "        [-2.6029,  3.4999, -0.6858]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2105, -3.3610, -2.1525],\n",
      "        [ 4.3747, -3.3700, -2.1679]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2105, -3.3610, -2.1525],\n",
      "        [ 4.3747, -3.3700, -2.1679]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3521, -3.2287, -2.1378],\n",
      "        [ 4.2629, -3.3121, -2.2295]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3521, -3.2287, -2.1378],\n",
      "        [ 4.2629, -3.3121, -2.2295]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2690, -3.3759, -1.8948],\n",
      "        [ 1.9189, -2.2914, -0.5410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2690, -3.3759, -1.8948],\n",
      "        [ 1.9189, -2.2914, -0.5410]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8885, -3.3503, -1.9587],\n",
      "        [ 4.2575, -3.3389, -1.8074]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8885, -3.3503, -1.9587],\n",
      "        [ 4.2575, -3.3389, -1.8074]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3260, -2.8662, -2.3293],\n",
      "        [ 4.4773, -3.3880, -2.3346]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3260, -2.8662, -2.3293],\n",
      "        [ 4.4773, -3.3880, -2.3346]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0958, -2.4498, -2.5834],\n",
      "        [ 3.8329, -2.1656, -2.2751]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0958, -2.4498, -2.5834],\n",
      "        [ 3.8329, -2.1656, -2.2751]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5852, -3.3091, -2.4969],\n",
      "        [ 4.5599, -3.5460, -2.2040]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5852, -3.3091, -2.4969],\n",
      "        [ 4.5599, -3.5460, -2.2040]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3792, -3.4278, -2.2136],\n",
      "        [ 4.4136, -2.8026, -2.5780]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3792, -3.4278, -2.2136],\n",
      "        [ 4.4136, -2.8026, -2.5780]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2824, -3.4227, -1.8394],\n",
      "        [ 4.4503, -3.2280, -2.5201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2824, -3.4227, -1.8394],\n",
      "        [ 4.4503, -3.2280, -2.5201]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4655, -3.4095, -2.1499],\n",
      "        [-1.6509,  3.0201, -1.3338]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4655, -3.4095, -2.1499],\n",
      "        [-1.6509,  3.0201, -1.3338]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3545, -3.1829, -2.4649],\n",
      "        [ 4.0448, -3.1479, -1.9515]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3545, -3.1829, -2.4649],\n",
      "        [ 4.0448, -3.1479, -1.9515]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1358, -2.7917, -2.3621],\n",
      "        [ 4.2436, -3.2843, -2.0746]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1358, -2.7917, -2.3621],\n",
      "        [ 4.2436, -3.2843, -2.0746]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4747, -3.2179, -2.3048],\n",
      "        [ 4.4011, -3.5270, -2.0453]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4747, -3.2179, -2.3048],\n",
      "        [ 4.4011, -3.5270, -2.0453]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4512, -3.0565, -2.5120],\n",
      "        [ 4.3488, -2.9537, -2.5230]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4512, -3.0565, -2.5120],\n",
      "        [ 4.3488, -2.9537, -2.5230]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7610, -3.5331, -1.3519],\n",
      "        [ 4.3346, -3.4550, -2.2394]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7610, -3.5331, -1.3519],\n",
      "        [ 4.3346, -3.4550, -2.2394]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5276, -3.4100, -2.6104],\n",
      "        [ 4.2536, -2.6470, -2.5514]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5276, -3.4100, -2.6104],\n",
      "        [ 4.2536, -2.6470, -2.5514]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4413, -3.4347, -2.4691],\n",
      "        [ 4.4373, -2.9864, -2.4104]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4413, -3.4347, -2.4691],\n",
      "        [ 4.4373, -2.9864, -2.4104]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4367, -3.3021, -2.3287],\n",
      "        [ 4.3381, -2.7137, -2.6828]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4367, -3.3021, -2.3287],\n",
      "        [ 4.3381, -2.7137, -2.6828]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5504, -3.1512, -2.5600],\n",
      "        [-1.4241,  3.2099, -1.4611]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5504, -3.1512, -2.5600],\n",
      "        [-1.4241,  3.2099, -1.4611]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5569, -3.0908, -2.4567],\n",
      "        [ 4.5276, -3.4214, -2.3057]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5569, -3.0908, -2.4567],\n",
      "        [ 4.5276, -3.4214, -2.3057]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4024, -3.0916, -2.4380],\n",
      "        [ 4.6065, -3.2259, -2.3745]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4024, -3.0916, -2.4380],\n",
      "        [ 4.6065, -3.2259, -2.3745]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6854, -3.3750, -2.4202],\n",
      "        [-2.0379,  3.4627, -1.1792]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6854, -3.3750, -2.4202],\n",
      "        [-2.0379,  3.4627, -1.1792]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3326, -3.0637, -2.2767],\n",
      "        [ 0.3222, -1.0407,  0.6451]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3326, -3.0637, -2.2767],\n",
      "        [ 0.3222, -1.0407,  0.6451]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5525, -3.3888, -1.9994],\n",
      "        [ 4.2746, -2.9862, -2.3210]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5525, -3.3888, -1.9994],\n",
      "        [ 4.2746, -2.9862, -2.3210]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4755, -3.1287, -2.3422],\n",
      "        [ 4.5639, -3.3424, -2.3054]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4755, -3.1287, -2.3422],\n",
      "        [ 4.5639, -3.3424, -2.3054]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5063, -3.3362, -2.2761],\n",
      "        [ 4.6057, -3.4593, -2.2768]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5063, -3.3362, -2.2761],\n",
      "        [ 4.6057, -3.4593, -2.2768]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3614, -3.0878, -2.4184],\n",
      "        [-0.0196, -0.9530,  0.4731]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3614, -3.0878, -2.4184],\n",
      "        [-0.0196, -0.9530,  0.4731]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0945, -1.1060,  0.3742],\n",
      "        [ 4.6702, -3.2831, -2.5060]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.0945, -1.1060,  0.3742],\n",
      "        [ 4.6702, -3.2831, -2.5060]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4607, -3.2287, -2.5535],\n",
      "        [ 4.3187, -2.8123, -2.2456]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4607, -3.2287, -2.5535],\n",
      "        [ 4.3187, -2.8123, -2.2456]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.3199, -1.8298, -2.7682],\n",
      "        [ 4.3636, -2.7914, -2.5757]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.3199, -1.8298, -2.7682],\n",
      "        [ 4.3636, -2.7914, -2.5757]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2504, -3.1526, -1.9903],\n",
      "        [ 4.5997, -3.0264, -2.4831]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2504, -3.1526, -1.9903],\n",
      "        [ 4.5997, -3.0264, -2.4831]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4549, -3.1251, -2.2243],\n",
      "        [ 3.9802, -3.7453, -1.4589]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4549, -3.1251, -2.2243],\n",
      "        [ 3.9802, -3.7453, -1.4589]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8268, -3.2479, -2.5080],\n",
      "        [-2.0273,  3.6200, -1.4241]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8268, -3.2479, -2.5080],\n",
      "        [-2.0273,  3.6200, -1.4241]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5063, -3.3372, -2.3684],\n",
      "        [ 4.5270, -3.5146, -2.4634]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5063, -3.3372, -2.3684],\n",
      "        [ 4.5270, -3.5146, -2.4634]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3599, -2.7386, -2.6339],\n",
      "        [ 4.4393, -3.1459, -2.4698]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3599, -2.7386, -2.6339],\n",
      "        [ 4.4393, -3.1459, -2.4698]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.3628,  3.6318, -0.9609],\n",
      "        [ 4.5059, -3.0945, -2.6823]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.3628,  3.6318, -0.9609],\n",
      "        [ 4.5059, -3.0945, -2.6823]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3824, -3.6197, -2.3788],\n",
      "        [ 4.1153, -2.5919, -2.6112]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3824, -3.6197, -2.3788],\n",
      "        [ 4.1153, -2.5919, -2.6112]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1954, -3.6716, -1.6594],\n",
      "        [ 4.4009, -2.5056, -2.5468]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1954, -3.6716, -1.6594],\n",
      "        [ 4.4009, -2.5056, -2.5468]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6223, -3.6064, -2.1359],\n",
      "        [ 4.4371, -3.0691, -2.4538]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6223, -3.6064, -2.1359],\n",
      "        [ 4.4371, -3.0691, -2.4538]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5713, -2.9995, -2.4888],\n",
      "        [ 4.3396, -2.8719, -2.4512]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5713, -2.9995, -2.4888],\n",
      "        [ 4.3396, -2.8719, -2.4512]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4204, -3.5455, -2.4203],\n",
      "        [ 4.3054, -2.7992, -2.4335]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4204, -3.5455, -2.4203],\n",
      "        [ 4.3054, -2.7992, -2.4335]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.3324, -0.7011,  0.9090],\n",
      "        [ 4.6535, -3.1208, -2.4037]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.3324, -0.7011,  0.9090],\n",
      "        [ 4.6535, -3.1208, -2.4037]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.3012,  3.5599, -1.1531],\n",
      "        [ 4.7351, -3.2994, -2.3249]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.3012,  3.5599, -1.1531],\n",
      "        [ 4.7351, -3.2994, -2.3249]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7612, -3.3037, -2.4899],\n",
      "        [ 4.3744, -3.4037, -2.2576]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7612, -3.3037, -2.4899],\n",
      "        [ 4.3744, -3.4037, -2.2576]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.5392,  3.8591, -0.7592],\n",
      "        [ 4.4953, -3.1623, -2.7090]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.5392,  3.8591, -0.7592],\n",
      "        [ 4.4953, -3.1623, -2.7090]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3847, -2.9837, -2.8429],\n",
      "        [ 4.6115, -3.2706, -2.6091]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3847, -2.9837, -2.8429],\n",
      "        [ 4.6115, -3.2706, -2.6091]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9539, -3.7280, -1.5107],\n",
      "        [ 4.5537, -3.1935, -2.4287]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9539, -3.7280, -1.5107],\n",
      "        [ 4.5537, -3.1935, -2.4287]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4522, -3.4777, -2.4345],\n",
      "        [ 4.4605, -3.3787, -2.2393]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4522, -3.4777, -2.4345],\n",
      "        [ 4.4605, -3.3787, -2.2393]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3600, -3.3619, -1.8004],\n",
      "        [ 4.4962, -3.4928, -2.2290]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3600, -3.3619, -1.8004],\n",
      "        [ 4.4962, -3.4928, -2.2290]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4900, -3.4287, -2.1843],\n",
      "        [ 4.0175, -2.5146, -2.6969]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4900, -3.4287, -2.1843],\n",
      "        [ 4.0175, -2.5146, -2.6969]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5827, -3.3498, -2.2456],\n",
      "        [ 4.5526, -3.5637, -2.4979]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5827, -3.3498, -2.2456],\n",
      "        [ 4.5526, -3.5637, -2.4979]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6047, -3.1011, -2.7208],\n",
      "        [ 3.9379, -3.1953, -1.5652]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6047, -3.1011, -2.7208],\n",
      "        [ 3.9379, -3.1953, -1.5652]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1624, -2.7090, -2.6441],\n",
      "        [ 4.3809, -3.4421, -2.1640]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1624, -2.7090, -2.6441],\n",
      "        [ 4.3809, -3.4421, -2.1640]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5762, -3.2286, -2.3475],\n",
      "        [ 4.6223, -3.2285, -2.3249]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5762, -3.2286, -2.3475],\n",
      "        [ 4.6223, -3.2285, -2.3249]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3555, -3.1271, -2.1429],\n",
      "        [ 4.6689, -3.3069, -2.4333]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3555, -3.1271, -2.1429],\n",
      "        [ 4.6689, -3.3069, -2.4333]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3717, -3.3971, -2.3186],\n",
      "        [ 4.5564, -3.4952, -2.2876]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3717, -3.3971, -2.3186],\n",
      "        [ 4.5564, -3.4952, -2.2876]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4266, -3.2174, -2.3475],\n",
      "        [ 4.4521, -3.3198, -2.3082]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4266, -3.2174, -2.3475],\n",
      "        [ 4.4521, -3.3198, -2.3082]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8059, -3.0193, -2.4150],\n",
      "        [ 4.4097, -3.4778, -1.9482]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8059, -3.0193, -2.4150],\n",
      "        [ 4.4097, -3.4778, -1.9482]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2730, -3.2098, -2.4430],\n",
      "        [ 4.0846, -3.2178, -2.0248]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2730, -3.2098, -2.4430],\n",
      "        [ 4.0846, -3.2178, -2.0248]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.1069,  2.6858, -1.6517],\n",
      "        [ 4.6815, -3.2250, -2.3869]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.1069,  2.6858, -1.6517],\n",
      "        [ 4.6815, -3.2250, -2.3869]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4888, -3.2540, -2.4485],\n",
      "        [ 4.3346, -2.8701, -2.7220]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4888, -3.2540, -2.4485],\n",
      "        [ 4.3346, -2.8701, -2.7220]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4276, -3.1091, -2.5570],\n",
      "        [ 4.3187, -3.1648, -2.0532]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4276, -3.1091, -2.5570],\n",
      "        [ 4.3187, -3.1648, -2.0532]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5323, -3.4540, -2.2768],\n",
      "        [ 4.4466, -3.2328, -2.5236]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5323, -3.4540, -2.2768],\n",
      "        [ 4.4466, -3.2328, -2.5236]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.6334,  3.3171, -1.3278],\n",
      "        [ 4.5158, -3.1971, -2.5324]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.6334,  3.3171, -1.3278],\n",
      "        [ 4.5158, -3.1971, -2.5324]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5162, -3.2314, -2.3078],\n",
      "        [-2.3281,  3.7809, -1.2578]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5162, -3.2314, -2.3078],\n",
      "        [-2.3281,  3.7809, -1.2578]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2873, -2.9797, -2.5449],\n",
      "        [ 4.5367, -3.5054, -2.4609]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2873, -2.9797, -2.5449],\n",
      "        [ 4.5367, -3.5054, -2.4609]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5498, -3.0276, -2.7539],\n",
      "        [ 4.5601, -3.1323, -2.5346]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5498, -3.0276, -2.7539],\n",
      "        [ 4.5601, -3.1323, -2.5346]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4963, -2.8671, -2.5164],\n",
      "        [ 4.5645, -3.2239, -2.3783]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4963, -2.8671, -2.5164],\n",
      "        [ 4.5645, -3.2239, -2.3783]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4845, -3.2776, -2.4134],\n",
      "        [ 4.5027, -3.6161, -2.3726]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4845, -3.2776, -2.4134],\n",
      "        [ 4.5027, -3.6161, -2.3726]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4674, -2.9884, -2.4216],\n",
      "        [ 4.6140, -3.2372, -2.4354]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4674, -2.9884, -2.4216],\n",
      "        [ 4.6140, -3.2372, -2.4354]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3969, -3.2225, -2.5618],\n",
      "        [ 4.5161, -3.3745, -2.4114]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3969, -3.2225, -2.5618],\n",
      "        [ 4.5161, -3.3745, -2.4114]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7145, -3.1573, -2.4355],\n",
      "        [ 4.7213, -3.6265, -2.3477]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7145, -3.1573, -2.4355],\n",
      "        [ 4.7213, -3.6265, -2.3477]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6093, -3.4863, -2.2813],\n",
      "        [ 4.4955, -3.6165, -2.3271]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6093, -3.4863, -2.2813],\n",
      "        [ 4.4955, -3.6165, -2.3271]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2940, -3.2322, -1.9135],\n",
      "        [ 4.6608, -3.5000, -2.4039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2940, -3.2322, -1.9135],\n",
      "        [ 4.6608, -3.5000, -2.4039]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.9996,  3.2845, -1.4273],\n",
      "        [ 4.2645, -2.9759, -2.3698]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.9996,  3.2845, -1.4273],\n",
      "        [ 4.2645, -2.9759, -2.3698]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4763, -1.4176, -2.7212],\n",
      "        [ 4.4383, -3.1612, -2.2908]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4763, -1.4176, -2.7212],\n",
      "        [ 4.4383, -3.1612, -2.2908]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5666, -3.3410, -1.9750],\n",
      "        [ 4.2843, -2.8203, -2.7468]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5666, -3.3410, -1.9750],\n",
      "        [ 4.2843, -2.8203, -2.7468]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5516, -3.3466, -2.3416],\n",
      "        [ 4.6570, -3.6361, -2.2283]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5516, -3.3466, -2.3416],\n",
      "        [ 4.6570, -3.6361, -2.2283]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3002, -3.0652, -2.4995],\n",
      "        [ 4.6703, -3.5179, -2.1895]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3002, -3.0652, -2.4995],\n",
      "        [ 4.6703, -3.5179, -2.1895]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3884, -2.8917, -2.5092],\n",
      "        [ 4.4365, -3.4656, -2.2502]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3884, -2.8917, -2.5092],\n",
      "        [ 4.4365, -3.4656, -2.2502]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4610, -3.0990, -2.4599],\n",
      "        [ 4.4151, -3.5220, -2.1350]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4610, -3.0990, -2.4599],\n",
      "        [ 4.4151, -3.5220, -2.1350]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6367, -3.5537, -2.4419],\n",
      "        [ 4.6902, -3.2451, -2.5144]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6367, -3.5537, -2.4419],\n",
      "        [ 4.6902, -3.2451, -2.5144]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6502, -3.0884, -2.6639],\n",
      "        [ 4.7518, -3.1968, -2.5849]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6502, -3.0884, -2.6639],\n",
      "        [ 4.7518, -3.1968, -2.5849]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5863, -3.1951, -2.5088],\n",
      "        [ 4.5343, -3.1482, -2.3047]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5863, -3.1951, -2.5088],\n",
      "        [ 4.5343, -3.1482, -2.3047]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6363, -3.3177, -2.3947],\n",
      "        [ 4.5281, -3.4124, -2.4928]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6363, -3.3177, -2.3947],\n",
      "        [ 4.5281, -3.4124, -2.4928]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6674, -3.4649, -2.4249],\n",
      "        [ 4.4205, -3.3725, -2.4454]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6674, -3.4649, -2.4249],\n",
      "        [ 4.4205, -3.3725, -2.4454]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4884, -3.4230, -2.5367],\n",
      "        [ 4.4839, -3.2909, -2.3893]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4884, -3.4230, -2.5367],\n",
      "        [ 4.4839, -3.2909, -2.3893]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5283, -3.4870, -2.4166],\n",
      "        [-1.1696,  3.0283, -1.4377]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5283, -3.4870, -2.4166],\n",
      "        [-1.1696,  3.0283, -1.4377]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5038, -3.1566, -2.6485],\n",
      "        [ 3.8936, -3.1920, -1.4893]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5038, -3.1566, -2.6485],\n",
      "        [ 3.8936, -3.1920, -1.4893]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2360, -2.4848, -0.2962],\n",
      "        [-0.5340, -0.8370,  0.8846]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.2360, -2.4848, -0.2962],\n",
      "        [-0.5340, -0.8370,  0.8846]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3041, -2.7916, -2.6456],\n",
      "        [ 4.2400, -3.5574, -1.5792]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3041, -2.7916, -2.6456],\n",
      "        [ 4.2400, -3.5574, -1.5792]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.6560,  3.8139, -1.1680],\n",
      "        [ 4.4309, -3.4426, -2.1624]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.6560,  3.8139, -1.1680],\n",
      "        [ 4.4309, -3.4426, -2.1624]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4596, -2.9968, -2.5668],\n",
      "        [ 3.3275, -2.5209, -1.8754]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4596, -2.9968, -2.5668],\n",
      "        [ 3.3275, -2.5209, -1.8754]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.6050,  3.5199, -0.6105],\n",
      "        [ 4.3566, -2.8419, -2.6343]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.6050,  3.5199, -0.6105],\n",
      "        [ 4.3566, -2.8419, -2.6343]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5485, -3.2947, -2.2678],\n",
      "        [ 4.4997, -3.5922, -2.1987]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5485, -3.2947, -2.2678],\n",
      "        [ 4.4997, -3.5922, -2.1987]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6358, -3.4420, -2.4084],\n",
      "        [ 4.1347, -3.4543, -2.0677]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6358, -3.4420, -2.4084],\n",
      "        [ 4.1347, -3.4543, -2.0677]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.9547,  3.6783, -0.8288],\n",
      "        [ 4.5164, -2.5521, -2.7983]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.9547,  3.6783, -0.8288],\n",
      "        [ 4.5164, -2.5521, -2.7983]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4531, -2.9006, -2.4829],\n",
      "        [-2.2691,  3.9702, -1.2959]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4531, -2.9006, -2.4829],\n",
      "        [-2.2691,  3.9702, -1.2959]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6369, -3.3347, -2.4653],\n",
      "        [-2.6418,  4.0180, -1.1957]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6369, -3.3347, -2.4653],\n",
      "        [-2.6418,  4.0180, -1.1957]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4429, -3.2223, -2.2611],\n",
      "        [ 4.6241, -2.8423, -2.5534]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4429, -3.2223, -2.2611],\n",
      "        [ 4.6241, -2.8423, -2.5534]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5348, -3.1384, -2.5733],\n",
      "        [ 4.5788, -3.2459, -2.2765]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5348, -3.1384, -2.5733],\n",
      "        [ 4.5788, -3.2459, -2.2765]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9502, -3.3089, -1.4224],\n",
      "        [ 4.8029, -3.1153, -2.4974]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9502, -3.3089, -1.4224],\n",
      "        [ 4.8029, -3.1153, -2.4974]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5130, -3.0423, -2.4032],\n",
      "        [ 4.6245, -3.3882, -2.0897]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5130, -3.0423, -2.4032],\n",
      "        [ 4.6245, -3.3882, -2.0897]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.3300, -0.5628,  1.1091],\n",
      "        [ 4.5243, -3.0788, -2.6592]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.3300, -0.5628,  1.1091],\n",
      "        [ 4.5243, -3.0788, -2.6592]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4164, -2.7848, -2.6807],\n",
      "        [-2.4395,  3.7466, -1.3689]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4164, -2.7848, -2.6807],\n",
      "        [-2.4395,  3.7466, -1.3689]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.5092, -0.5345,  1.0060],\n",
      "        [ 3.7786, -3.2407, -1.8189]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.5092, -0.5345,  1.0060],\n",
      "        [ 3.7786, -3.2407, -1.8189]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.8257, -2.6224, -0.0288],\n",
      "        [ 4.7087, -3.0003, -2.5235]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.8257, -2.6224, -0.0288],\n",
      "        [ 4.7087, -3.0003, -2.5235]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3647, -3.5537, -1.9445],\n",
      "        [-0.2384, -0.8424,  1.1172]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3647, -3.5537, -1.9445],\n",
      "        [-0.2384, -0.8424,  1.1172]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.0776,  3.8645, -1.5032],\n",
      "        [ 4.1660, -2.9305, -2.7346]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.0776,  3.8645, -1.5032],\n",
      "        [ 4.1660, -2.9305, -2.7346]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6155, -3.2976, -2.4131],\n",
      "        [ 3.0833, -1.3489, -2.9666]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6155, -3.2976, -2.4131],\n",
      "        [ 3.0833, -1.3489, -2.9666]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.3167,  3.8379, -1.5109],\n",
      "        [ 4.5858, -3.2022, -2.6147]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.3167,  3.8379, -1.5109],\n",
      "        [ 4.5858, -3.2022, -2.6147]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1941, -3.3963, -2.0199],\n",
      "        [ 4.1714, -3.6961, -1.6626]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1941, -3.3963, -2.0199],\n",
      "        [ 4.1714, -3.6961, -1.6626]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6763, -3.3814, -2.4927],\n",
      "        [ 4.4089, -3.1265, -2.4347]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6763, -3.3814, -2.4927],\n",
      "        [ 4.4089, -3.1265, -2.4347]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4679, -3.1806, -2.4873],\n",
      "        [ 4.4651, -3.6426, -1.9246]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4679, -3.1806, -2.4873],\n",
      "        [ 4.4651, -3.6426, -1.9246]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.2340,  3.6812, -1.4626],\n",
      "        [ 3.8027, -3.6010, -1.6442]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.2340,  3.6812, -1.4626],\n",
      "        [ 3.8027, -3.6010, -1.6442]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4651, -3.4528, -2.1154],\n",
      "        [ 3.5012, -3.4188, -0.8752]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4651, -3.4528, -2.1154],\n",
      "        [ 3.5012, -3.4188, -0.8752]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3304, -3.0785, -2.5405],\n",
      "        [ 4.5792, -3.3254, -2.3044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3304, -3.0785, -2.5405],\n",
      "        [ 4.5792, -3.3254, -2.3044]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5277, -3.6899, -2.0926],\n",
      "        [ 4.5484, -3.5448, -2.3303]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5277, -3.6899, -2.0926],\n",
      "        [ 4.5484, -3.5448, -2.3303]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7595, -2.9734, -0.6229],\n",
      "        [-1.5299,  3.2400, -1.3857]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.7595, -2.9734, -0.6229],\n",
      "        [-1.5299,  3.2400, -1.3857]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.4404,  3.1175, -1.5307],\n",
      "        [ 4.7180, -3.2334, -2.7878]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.4404,  3.1175, -1.5307],\n",
      "        [ 4.7180, -3.2334, -2.7878]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1828, -3.6819, -2.0229],\n",
      "        [-0.0249, -1.2506,  0.9790]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1828, -3.6819, -2.0229],\n",
      "        [-0.0249, -1.2506,  0.9790]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3744, -3.6041, -2.2425],\n",
      "        [ 4.3460, -2.8951, -2.5594]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3744, -3.6041, -2.2425],\n",
      "        [ 4.3460, -2.8951, -2.5594]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0699, -2.7419, -2.6835],\n",
      "        [ 3.4430, -3.5092, -0.9220]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0699, -2.7419, -2.6835],\n",
      "        [ 3.4430, -3.5092, -0.9220]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3360, -3.7411, -1.8840],\n",
      "        [ 2.6956, -3.1031, -0.2431]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3360, -3.7411, -1.8840],\n",
      "        [ 2.6956, -3.1031, -0.2431]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2148, -3.0217, -2.3137],\n",
      "        [ 4.7220, -3.4982, -2.3095]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2148, -3.0217, -2.3137],\n",
      "        [ 4.7220, -3.4982, -2.3095]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.1864, -3.7483, -1.4670],\n",
      "        [ 4.6908, -3.5789, -2.4429]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.1864, -3.7483, -1.4670],\n",
      "        [ 4.6908, -3.5789, -2.4429]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5397, -3.7291, -2.0658],\n",
      "        [ 4.4622, -3.7571, -1.9828]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5397, -3.7291, -2.0658],\n",
      "        [ 4.4622, -3.7571, -1.9828]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.7570,  3.4759, -1.6108],\n",
      "        [ 4.5586, -3.4549, -2.4372]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.7570,  3.4759, -1.6108],\n",
      "        [ 4.5586, -3.4549, -2.4372]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5978, -2.9662, -2.1430],\n",
      "        [ 4.6762, -3.5446, -1.9096]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5978, -2.9662, -2.1430],\n",
      "        [ 4.6762, -3.5446, -1.9096]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2736, -3.3225, -2.2607],\n",
      "        [ 3.9132, -2.6277, -2.3792]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2736, -3.3225, -2.2607],\n",
      "        [ 3.9132, -2.6277, -2.3792]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6330, -2.9066, -2.5074],\n",
      "        [ 4.2306, -2.8172, -2.2749]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6330, -2.9066, -2.5074],\n",
      "        [ 4.2306, -2.8172, -2.2749]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2514, -3.2237, -2.2642],\n",
      "        [ 4.7271, -3.4318, -2.1525]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2514, -3.2237, -2.2642],\n",
      "        [ 4.7271, -3.4318, -2.1525]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5698, -3.5149, -2.2418],\n",
      "        [ 4.4290, -3.3372, -2.4568]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5698, -3.5149, -2.2418],\n",
      "        [ 4.4290, -3.3372, -2.4568]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4482, -3.3278, -2.2539],\n",
      "        [ 4.7571, -3.7085, -2.4314]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4482, -3.3278, -2.2539],\n",
      "        [ 4.7571, -3.7085, -2.4314]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6945, -3.5304, -2.3305],\n",
      "        [ 4.5551, -3.3309, -2.7092]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6945, -3.5304, -2.3305],\n",
      "        [ 4.5551, -3.3309, -2.7092]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5020, -2.9411, -2.8130],\n",
      "        [ 4.5500, -3.3816, -2.6042]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5020, -2.9411, -2.8130],\n",
      "        [ 4.5500, -3.3816, -2.6042]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5296, -3.5379, -2.6189],\n",
      "        [ 4.6219, -3.1752, -2.5197]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5296, -3.5379, -2.6189],\n",
      "        [ 4.6219, -3.1752, -2.5197]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4400, -3.2597, -2.3152],\n",
      "        [ 4.5666, -3.2828, -2.4848]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4400, -3.2597, -2.3152],\n",
      "        [ 4.5666, -3.2828, -2.4848]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9392, -3.4712, -2.4276],\n",
      "        [-0.4365, -0.8673,  0.9730]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9392, -3.4712, -2.4276],\n",
      "        [-0.4365, -0.8673,  0.9730]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7241, -3.1933, -2.5351],\n",
      "        [ 4.2433, -3.2377, -2.1649]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7241, -3.1933, -2.5351],\n",
      "        [ 4.2433, -3.2377, -2.1649]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6782, -3.0994, -2.3357],\n",
      "        [ 4.6393, -3.2315, -2.3439]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6782, -3.0994, -2.3357],\n",
      "        [ 4.6393, -3.2315, -2.3439]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4467, -3.3506, -2.3416],\n",
      "        [ 4.4060, -2.8909, -2.8834]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4467, -3.3506, -2.3416],\n",
      "        [ 4.4060, -2.8909, -2.8834]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5553, -3.7434, -2.1793],\n",
      "        [ 4.4785, -3.3277, -2.3817]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5553, -3.7434, -2.1793],\n",
      "        [ 4.4785, -3.3277, -2.3817]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2596, -1.6378,  1.1368],\n",
      "        [ 4.6042, -3.4625, -2.4620]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 0.2596, -1.6378,  1.1368],\n",
      "        [ 4.6042, -3.4625, -2.4620]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5841, -3.5134, -2.4373],\n",
      "        [ 4.7490, -3.7217, -2.1444]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5841, -3.5134, -2.4373],\n",
      "        [ 4.7490, -3.7217, -2.1444]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6831, -3.4102, -2.4811],\n",
      "        [ 4.6806, -3.4414, -2.2717]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6831, -3.4102, -2.4811],\n",
      "        [ 4.6806, -3.4414, -2.2717]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7123, -3.0485, -2.9009],\n",
      "        [ 4.8602, -3.5212, -2.1970]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7123, -3.0485, -2.9009],\n",
      "        [ 4.8602, -3.5212, -2.1970]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6070, -3.6771, -2.3766],\n",
      "        [-0.2642, -1.2720,  1.2971]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6070, -3.6771, -2.3766],\n",
      "        [-0.2642, -1.2720,  1.2971]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8064, -3.5460, -2.4196],\n",
      "        [ 4.5701, -3.3822, -2.3616]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8064, -3.5460, -2.4196],\n",
      "        [ 4.5701, -3.3822, -2.3616]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7439, -3.7687, -2.1278],\n",
      "        [ 4.7506, -3.5210, -2.2361]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7439, -3.7687, -2.1278],\n",
      "        [ 4.7506, -3.5210, -2.2361]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.0505,  3.6806, -1.5949],\n",
      "        [ 4.6376, -3.5437, -2.2442]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.0505,  3.6806, -1.5949],\n",
      "        [ 4.6376, -3.5437, -2.2442]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4669, -3.6341, -2.1188],\n",
      "        [ 4.5088, -3.2661, -2.2401]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4669, -3.6341, -2.1188],\n",
      "        [ 4.5088, -3.2661, -2.2401]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5675, -3.1537, -2.5415],\n",
      "        [ 4.5022, -3.4376, -2.1018]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5675, -3.1537, -2.5415],\n",
      "        [ 4.5022, -3.4376, -2.1018]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5390, -3.4109, -2.2397],\n",
      "        [-1.9575,  3.7667, -1.5573]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5390, -3.4109, -2.2397],\n",
      "        [-1.9575,  3.7667, -1.5573]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6272, -3.3818, -2.5788],\n",
      "        [ 4.7849, -3.1530, -2.5749]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6272, -3.3818, -2.5788],\n",
      "        [ 4.7849, -3.1530, -2.5749]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7518, -3.4907, -2.1680],\n",
      "        [ 4.4361, -3.3027, -2.4313]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7518, -3.4907, -2.1680],\n",
      "        [ 4.4361, -3.3027, -2.4313]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5898, -3.4679, -2.1763],\n",
      "        [ 4.4854, -3.5491, -1.9333]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5898, -3.4679, -2.1763],\n",
      "        [ 4.4854, -3.5491, -1.9333]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6288, -3.5339, -2.5125],\n",
      "        [ 4.2636, -3.1550, -1.9278]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6288, -3.5339, -2.5125],\n",
      "        [ 4.2636, -3.1550, -1.9278]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7470, -3.0274, -2.2497],\n",
      "        [ 4.8022, -3.4672, -2.4939]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7470, -3.0274, -2.2497],\n",
      "        [ 4.8022, -3.4672, -2.4939]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7604, -3.7685, -1.0881],\n",
      "        [ 1.5490, -2.1992,  0.2349]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7604, -3.7685, -1.0881],\n",
      "        [ 1.5490, -2.1992,  0.2349]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5198, -3.3026, -2.0554],\n",
      "        [ 4.4715, -3.1181, -2.6065]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5198, -3.3026, -2.0554],\n",
      "        [ 4.4715, -3.1181, -2.6065]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8061, -3.5297, -2.3439],\n",
      "        [-1.3005,  3.0205, -1.7982]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8061, -3.5297, -2.3439],\n",
      "        [-1.3005,  3.0205, -1.7982]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6669, -3.5419, -2.4505],\n",
      "        [ 4.5810, -3.6088, -2.1233]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6669, -3.5419, -2.4505],\n",
      "        [ 4.5810, -3.6088, -2.1233]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5067, -3.5228, -2.2549],\n",
      "        [-1.7866,  3.6880, -1.6957]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5067, -3.5228, -2.2549],\n",
      "        [-1.7866,  3.6880, -1.6957]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4910, -3.6423, -2.0473],\n",
      "        [ 4.5646, -3.1501, -2.5728]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4910, -3.6423, -2.0473],\n",
      "        [ 4.5646, -3.1501, -2.5728]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6833, -3.5316, -2.0800],\n",
      "        [ 4.5820, -3.3977, -2.2846]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6833, -3.5316, -2.0800],\n",
      "        [ 4.5820, -3.3977, -2.2846]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2055, -3.6325, -2.2478],\n",
      "        [ 3.5446, -2.5667, -2.3472]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2055, -3.6325, -2.2478],\n",
      "        [ 3.5446, -2.5667, -2.3472]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6017, -3.4906, -2.1924],\n",
      "        [ 4.2285, -3.4002, -2.3001]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6017, -3.4906, -2.1924],\n",
      "        [ 4.2285, -3.4002, -2.3001]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.2952, -0.9690,  1.6488],\n",
      "        [ 4.4310, -3.8125, -1.9750]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.2952, -0.9690,  1.6488],\n",
      "        [ 4.4310, -3.8125, -1.9750]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6204, -3.3646, -2.4129],\n",
      "        [ 4.0672, -3.3726, -1.8116]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6204, -3.3646, -2.4129],\n",
      "        [ 4.0672, -3.3726, -1.8116]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6727, -3.7533, -2.3201],\n",
      "        [ 4.5549, -3.4508, -2.4523]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6727, -3.7533, -2.3201],\n",
      "        [ 4.5549, -3.4508, -2.4523]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4375, -3.8517, -2.2249],\n",
      "        [ 4.1138, -3.3090, -2.2575]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4375, -3.8517, -2.2249],\n",
      "        [ 4.1138, -3.3090, -2.2575]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7314, -3.6762, -2.2443],\n",
      "        [-2.2059,  3.8647, -1.3185]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7314, -3.6762, -2.2443],\n",
      "        [-2.2059,  3.8647, -1.3185]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7751, -3.6021, -2.2930],\n",
      "        [ 4.4439, -3.2722, -2.0165]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7751, -3.6021, -2.2930],\n",
      "        [ 4.4439, -3.2722, -2.0165]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8971, -3.5272, -0.5065],\n",
      "        [ 4.2222, -3.6406, -1.7730]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.8971, -3.5272, -0.5065],\n",
      "        [ 4.2222, -3.6406, -1.7730]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5613, -3.6458, -2.2555],\n",
      "        [-1.8188,  3.5499, -1.4782]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5613, -3.6458, -2.2555],\n",
      "        [-1.8188,  3.5499, -1.4782]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.9544, -2.6392, -2.4336],\n",
      "        [ 4.4861, -3.7592, -1.9382]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.9544, -2.6392, -2.4336],\n",
      "        [ 4.4861, -3.7592, -1.9382]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2889, -3.5753, -2.1933],\n",
      "        [ 4.0690, -3.4529, -1.5761]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2889, -3.5753, -2.1933],\n",
      "        [ 4.0690, -3.4529, -1.5761]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4993, -3.0494, -2.3412],\n",
      "        [ 4.5926, -3.2519, -2.3903]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4993, -3.0494, -2.3412],\n",
      "        [ 4.5926, -3.2519, -2.3903]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5047, -3.7247, -1.8500],\n",
      "        [-1.0639, -0.6536,  1.6150]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5047, -3.7247, -1.8500],\n",
      "        [-1.0639, -0.6536,  1.6150]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7959, -3.8339, -2.3788],\n",
      "        [ 2.7885, -1.3826, -2.2098]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7959, -3.8339, -2.3788],\n",
      "        [ 2.7885, -1.3826, -2.2098]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8044, -3.3456, -1.4933],\n",
      "        [ 4.3205, -2.8962, -2.4612]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8044, -3.3456, -1.4933],\n",
      "        [ 4.3205, -2.8962, -2.4612]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.7401, -2.9683, -2.2141],\n",
      "        [ 4.5619, -3.6287, -2.0253]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.7401, -2.9683, -2.2141],\n",
      "        [ 4.5619, -3.6287, -2.0253]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.3026,  4.0243, -1.5776],\n",
      "        [ 4.4348, -3.7058, -2.1330]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.3026,  4.0243, -1.5776],\n",
      "        [ 4.4348, -3.7058, -2.1330]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7401, -3.6262, -2.3854],\n",
      "        [-2.1358,  3.7869, -1.5407]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7401, -3.6262, -2.3854],\n",
      "        [-2.1358,  3.7869, -1.5407]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.6955, -1.1257,  1.5344],\n",
      "        [ 4.4965, -3.7702, -1.8683]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.6955, -1.1257,  1.5344],\n",
      "        [ 4.4965, -3.7702, -1.8683]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4516, -3.3888, -2.2072],\n",
      "        [ 4.2915, -3.6338, -2.2816]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4516, -3.3888, -2.2072],\n",
      "        [ 4.2915, -3.6338, -2.2816]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7994, -3.4977, -2.2009],\n",
      "        [ 4.3911, -3.3543, -2.5328]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7994, -3.4977, -2.2009],\n",
      "        [ 4.3911, -3.3543, -2.5328]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5836, -3.7789, -1.9330],\n",
      "        [ 4.6307, -3.0816, -2.5148]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5836, -3.7789, -1.9330],\n",
      "        [ 4.6307, -3.0816, -2.5148]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5788, -3.5152, -2.1921],\n",
      "        [ 4.5143, -3.4525, -2.3121]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5788, -3.5152, -2.1921],\n",
      "        [ 4.5143, -3.4525, -2.3121]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5785, -3.3742, -2.1702],\n",
      "        [ 3.0982, -1.4714, -2.7960]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5785, -3.3742, -2.1702],\n",
      "        [ 3.0982, -1.4714, -2.7960]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6861, -3.6623, -1.9905],\n",
      "        [ 0.1112, -1.7084,  1.3406]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6861, -3.6623, -1.9905],\n",
      "        [ 0.1112, -1.7084,  1.3406]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6691, -3.5152, -2.2014],\n",
      "        [ 4.1111, -3.6187, -1.3009]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6691, -3.5152, -2.2014],\n",
      "        [ 4.1111, -3.6187, -1.3009]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6033, -3.4840, -2.3439],\n",
      "        [ 4.4735, -3.4524, -2.4101]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6033, -3.4840, -2.3439],\n",
      "        [ 4.4735, -3.4524, -2.4101]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5895, -3.7048, -1.9483],\n",
      "        [ 4.7081, -3.2204, -2.3099]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5895, -3.7048, -1.9483],\n",
      "        [ 4.7081, -3.2204, -2.3099]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5672, -3.3954, -2.2197],\n",
      "        [ 4.2335, -3.3600, -1.8793]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5672, -3.3954, -2.2197],\n",
      "        [ 4.2335, -3.3600, -1.8793]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5957, -3.4496, -2.4546],\n",
      "        [ 4.6427, -3.4104, -2.4302]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5957, -3.4496, -2.4546],\n",
      "        [ 4.6427, -3.4104, -2.4302]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4572, -3.2839, -2.2709],\n",
      "        [ 4.4667, -3.5179, -2.1384]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4572, -3.2839, -2.2709],\n",
      "        [ 4.4667, -3.5179, -2.1384]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3145, -3.1980, -2.1676],\n",
      "        [ 4.3675, -2.5407, -2.6896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3145, -3.1980, -2.1676],\n",
      "        [ 4.3675, -2.5407, -2.6896]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6640, -3.5171, -1.8796],\n",
      "        [ 4.7705, -3.8586, -2.2326]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6640, -3.5171, -1.8796],\n",
      "        [ 4.7705, -3.8586, -2.2326]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3784, -3.1246, -2.6248],\n",
      "        [ 4.2874, -2.8901, -2.7278]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3784, -3.1246, -2.6248],\n",
      "        [ 4.2874, -2.8901, -2.7278]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4601, -3.1357, -2.5216],\n",
      "        [ 4.6513, -3.4344, -2.4825]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4601, -3.1357, -2.5216],\n",
      "        [ 4.6513, -3.4344, -2.4825]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.5554, -1.1024,  1.6879],\n",
      "        [ 4.7134, -3.6121, -2.2454]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.5554, -1.1024,  1.6879],\n",
      "        [ 4.7134, -3.6121, -2.2454]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4635, -3.6376, -2.1737],\n",
      "        [ 4.7007, -3.2043, -2.6416]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4635, -3.6376, -2.1737],\n",
      "        [ 4.7007, -3.2043, -2.6416]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4994, -3.4805, -2.2578],\n",
      "        [ 4.5605, -2.7020, -2.6313]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4994, -3.4805, -2.2578],\n",
      "        [ 4.5605, -2.7020, -2.6313]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5427, -3.4526, -2.3568],\n",
      "        [ 4.6634, -3.4612, -2.3728]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5427, -3.4526, -2.3568],\n",
      "        [ 4.6634, -3.4612, -2.3728]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6551, -3.2118, -2.4461],\n",
      "        [ 4.5088, -3.2650, -2.3315]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6551, -3.2118, -2.4461],\n",
      "        [ 4.5088, -3.2650, -2.3315]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.3169,  3.8689, -1.3838],\n",
      "        [ 3.5293, -2.4015, -2.4322]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.3169,  3.8689, -1.3838],\n",
      "        [ 3.5293, -2.4015, -2.4322]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.1636,  4.1109, -1.5884],\n",
      "        [ 4.8326, -3.4925, -2.4082]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.1636,  4.1109, -1.5884],\n",
      "        [ 4.8326, -3.4925, -2.4082]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7586, -3.5157, -2.4430],\n",
      "        [ 4.5755, -3.6168, -1.9628]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7586, -3.5157, -2.4430],\n",
      "        [ 4.5755, -3.6168, -1.9628]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.0253,  3.5978, -1.5425],\n",
      "        [ 4.5357, -3.2969, -2.3451]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.0253,  3.5978, -1.5425],\n",
      "        [ 4.5357, -3.2969, -2.3451]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7280, -3.4016, -2.2831],\n",
      "        [ 4.4383, -3.7750, -1.7660]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7280, -3.4016, -2.2831],\n",
      "        [ 4.4383, -3.7750, -1.7660]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7096, -3.5267, -2.6340],\n",
      "        [ 4.6499, -3.7768, -2.5194]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7096, -3.5267, -2.6340],\n",
      "        [ 4.6499, -3.7768, -2.5194]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7502, -3.5163, -2.6648],\n",
      "        [ 4.1378, -3.8409, -1.6431]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7502, -3.5163, -2.6648],\n",
      "        [ 4.1378, -3.8409, -1.6431]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.6090,  4.0575, -1.3395],\n",
      "        [ 4.2686, -2.9555, -2.4885]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.6090,  4.0575, -1.3395],\n",
      "        [ 4.2686, -2.9555, -2.4885]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5013, -3.1516, -2.7158],\n",
      "        [-2.1547,  4.0027, -1.6624]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5013, -3.1516, -2.7158],\n",
      "        [-2.1547,  4.0027, -1.6624]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6847, -3.2390, -2.4730],\n",
      "        [ 4.8145, -3.4673, -2.4133]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6847, -3.2390, -2.4730],\n",
      "        [ 4.8145, -3.4673, -2.4133]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.7364, -1.1256,  1.6802],\n",
      "        [-0.5134, -1.0028,  1.4283]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.7364, -1.1256,  1.6802],\n",
      "        [-0.5134, -1.0028,  1.4283]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6484, -3.4064, -2.3144],\n",
      "        [ 4.5559, -3.7104, -1.9198]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6484, -3.4064, -2.3144],\n",
      "        [ 4.5559, -3.7104, -1.9198]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6420, -3.1985, -2.5056],\n",
      "        [ 4.4037, -3.6984, -2.0921]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6420, -3.1985, -2.5056],\n",
      "        [ 4.4037, -3.6984, -2.0921]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7312, -3.4132, -2.4777],\n",
      "        [-0.8453, -0.8847,  1.9586]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7312, -3.4132, -2.4777],\n",
      "        [-0.8453, -0.8847,  1.9586]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7793, -3.3164, -2.6652],\n",
      "        [ 3.7978, -3.9145, -0.9382]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7793, -3.3164, -2.6652],\n",
      "        [ 3.7978, -3.9145, -0.9382]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7735, -3.6983, -2.2036],\n",
      "        [ 4.5819, -3.3852, -2.2365]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7735, -3.6983, -2.2036],\n",
      "        [ 4.5819, -3.3852, -2.2365]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4359, -3.4235, -2.3459],\n",
      "        [-1.9241,  3.9283, -1.5482]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4359, -3.4235, -2.3459],\n",
      "        [-1.9241,  3.9283, -1.5482]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5841, -3.5474, -2.2155],\n",
      "        [ 4.7819, -3.3730, -2.5678]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5841, -3.5474, -2.2155],\n",
      "        [ 4.7819, -3.3730, -2.5678]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4040, -2.9653, -2.4168],\n",
      "        [ 4.6456, -3.1847, -2.6443]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4040, -2.9653, -2.4168],\n",
      "        [ 4.6456, -3.1847, -2.6443]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9158, -3.4614, -2.3376],\n",
      "        [-2.4954,  3.8688, -1.3102]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9158, -3.4614, -2.3376],\n",
      "        [-2.4954,  3.8688, -1.3102]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.2375,  3.6668, -1.2894],\n",
      "        [ 4.4973, -3.5538, -2.4956]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.2375,  3.6668, -1.2894],\n",
      "        [ 4.4973, -3.5538, -2.4956]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5712, -3.6219, -2.1652],\n",
      "        [ 4.5484, -3.2221, -2.7410]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5712, -3.6219, -2.1652],\n",
      "        [ 4.5484, -3.2221, -2.7410]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4210, -2.8240, -2.7983],\n",
      "        [ 4.6857, -3.4226, -2.0485]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4210, -2.8240, -2.7983],\n",
      "        [ 4.6857, -3.4226, -2.0485]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3870, -3.4187, -2.9268],\n",
      "        [ 4.5586, -3.5849, -2.1842]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3870, -3.4187, -2.9268],\n",
      "        [ 4.5586, -3.5849, -2.1842]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6669, -3.6046, -2.3164],\n",
      "        [ 4.6870, -3.6487, -2.3643]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6669, -3.6046, -2.3164],\n",
      "        [ 4.6870, -3.6487, -2.3643]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7322, -3.3326, -2.5942],\n",
      "        [ 4.6683, -3.2031, -2.3936]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7322, -3.3326, -2.5942],\n",
      "        [ 4.6683, -3.2031, -2.3936]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4534, -3.7749, -2.0038],\n",
      "        [ 4.4529, -3.3781, -2.5199]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4534, -3.7749, -2.0038],\n",
      "        [ 4.4529, -3.3781, -2.5199]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7104, -3.4602, -2.5836],\n",
      "        [ 4.4496, -3.0719, -2.5225]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7104, -3.4602, -2.5836],\n",
      "        [ 4.4496, -3.0719, -2.5225]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4338, -2.4978, -2.7264],\n",
      "        [ 3.6200, -1.7359, -2.5816]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4338, -2.4978, -2.7264],\n",
      "        [ 3.6200, -1.7359, -2.5816]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5305, -3.4800, -2.2736],\n",
      "        [ 4.4187, -3.2170, -2.6440]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5305, -3.4800, -2.2736],\n",
      "        [ 4.4187, -3.2170, -2.6440]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.3441,  3.7878, -1.5295],\n",
      "        [ 4.5699, -3.2552, -2.4713]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.3441,  3.7878, -1.5295],\n",
      "        [ 4.5699, -3.2552, -2.4713]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7342, -3.1234, -2.5798],\n",
      "        [ 4.5579, -3.2682, -2.1755]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7342, -3.1234, -2.5798],\n",
      "        [ 4.5579, -3.2682, -2.1755]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6730, -3.3591, -2.4464],\n",
      "        [ 4.7102, -2.9399, -2.4500]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6730, -3.3591, -2.4464],\n",
      "        [ 4.7102, -2.9399, -2.4500]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.3394,  4.0419, -1.2806],\n",
      "        [ 3.6224, -2.5424, -2.4431]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.3394,  4.0419, -1.2806],\n",
      "        [ 3.6224, -2.5424, -2.4431]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5030, -2.8017, -2.4628],\n",
      "        [ 4.6714, -3.2196, -2.5874]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5030, -2.8017, -2.4628],\n",
      "        [ 4.6714, -3.2196, -2.5874]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2973, -2.9283, -2.5453],\n",
      "        [-2.4810,  4.1158, -1.3444]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2973, -2.9283, -2.5453],\n",
      "        [-2.4810,  4.1158, -1.3444]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.4982, -1.1849,  1.7737],\n",
      "        [ 4.6706, -3.1007, -2.5599]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.4982, -1.1849,  1.7737],\n",
      "        [ 4.6706, -3.1007, -2.5599]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8115, -3.4497, -2.4960],\n",
      "        [ 4.7171, -3.3765, -2.5476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8115, -3.4497, -2.4960],\n",
      "        [ 4.7171, -3.3765, -2.5476]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.6468, -1.1903,  1.9332],\n",
      "        [ 4.7082, -3.2405, -2.6241]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.6468, -1.1903,  1.9332],\n",
      "        [ 4.7082, -3.2405, -2.6241]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6792, -3.2820, -2.4913],\n",
      "        [ 4.6740, -3.3977, -2.5683]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6792, -3.2820, -2.4913],\n",
      "        [ 4.6740, -3.3977, -2.5683]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6398, -3.1659, -2.5230],\n",
      "        [ 4.2240, -3.5089, -2.4950]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6398, -3.1659, -2.5230],\n",
      "        [ 4.2240, -3.5089, -2.4950]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0741, -2.4630, -2.6642],\n",
      "        [ 4.6520, -3.3975, -2.3230]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0741, -2.4630, -2.6642],\n",
      "        [ 4.6520, -3.3975, -2.3230]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.0511,  2.7939, -1.8116],\n",
      "        [ 4.6083, -2.9055, -2.5711]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.0511,  2.7939, -1.8116],\n",
      "        [ 4.6083, -2.9055, -2.5711]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7509, -3.1667, -2.6163],\n",
      "        [ 4.8554, -3.4708, -2.4989]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7509, -3.1667, -2.6163],\n",
      "        [ 4.8554, -3.4708, -2.4989]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6856, -3.2655, -2.6892],\n",
      "        [ 4.3365, -2.9547, -2.6812]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6856, -3.2655, -2.6892],\n",
      "        [ 4.3365, -2.9547, -2.6812]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7144, -3.1006, -2.6182],\n",
      "        [ 4.3399, -3.0238, -2.4605]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7144, -3.1006, -2.6182],\n",
      "        [ 4.3399, -3.0238, -2.4605]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6982, -3.7679, -2.1238],\n",
      "        [ 4.2993, -3.0808, -2.0150]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6982, -3.7679, -2.1238],\n",
      "        [ 4.2993, -3.0808, -2.0150]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6989, -2.9228, -2.6490],\n",
      "        [-0.9792,  2.7861, -1.7667]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6989, -2.9228, -2.6490],\n",
      "        [-0.9792,  2.7861, -1.7667]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4558, -2.7200, -2.5445],\n",
      "        [ 4.8640, -2.8147, -2.4832]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4558, -2.7200, -2.5445],\n",
      "        [ 4.8640, -2.8147, -2.4832]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.4811,  4.0892, -1.3716],\n",
      "        [ 0.1489, -1.4192,  1.2893]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.4811,  4.0892, -1.3716],\n",
      "        [ 0.1489, -1.4192,  1.2893]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5384, -2.7076, -2.5721],\n",
      "        [ 4.0431, -3.6531, -2.0912]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5384, -2.7076, -2.5721],\n",
      "        [ 4.0431, -3.6531, -2.0912]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3919, -2.6974, -2.6399],\n",
      "        [ 4.4762, -3.2520, -2.5318]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3919, -2.6974, -2.6399],\n",
      "        [ 4.4762, -3.2520, -2.5318]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4584, -3.3894, -2.6208],\n",
      "        [ 4.4386, -2.9369, -2.2837]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4584, -3.3894, -2.6208],\n",
      "        [ 4.4386, -2.9369, -2.2837]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6393, -3.2196, -2.3192],\n",
      "        [ 4.4118, -2.9269, -2.8632]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6393, -3.2196, -2.3192],\n",
      "        [ 4.4118, -2.9269, -2.8632]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6276, -3.0174, -2.6287],\n",
      "        [ 4.7839, -3.0179, -2.8745]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6276, -3.0174, -2.6287],\n",
      "        [ 4.7839, -3.0179, -2.8745]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5446, -3.2206, -2.5083],\n",
      "        [ 4.5465, -3.5327, -2.3157]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5446, -3.2206, -2.5083],\n",
      "        [ 4.5465, -3.5327, -2.3157]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3587, -2.9028, -2.4625],\n",
      "        [ 4.8091, -3.0334, -2.6770]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3587, -2.9028, -2.4625],\n",
      "        [ 4.8091, -3.0334, -2.6770]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.9514, -1.0969,  2.0513],\n",
      "        [ 4.4757, -2.9762, -2.5957]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.9514, -1.0969,  2.0513],\n",
      "        [ 4.4757, -2.9762, -2.5957]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6813, -3.1464, -2.5402],\n",
      "        [ 4.4504, -2.5907, -2.9583]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6813, -3.1464, -2.5402],\n",
      "        [ 4.4504, -2.5907, -2.9583]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4962, -2.6046, -2.7267],\n",
      "        [ 4.7949, -3.1297, -2.7101]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4962, -2.6046, -2.7267],\n",
      "        [ 4.7949, -3.1297, -2.7101]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7499, -3.4127, -2.4784],\n",
      "        [ 4.6062, -3.3697, -1.9760]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7499, -3.4127, -2.4784],\n",
      "        [ 4.6062, -3.3697, -1.9760]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7207, -3.5178, -2.3913],\n",
      "        [ 4.7475, -3.3123, -2.5664]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7207, -3.5178, -2.3913],\n",
      "        [ 4.7475, -3.3123, -2.5664]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.3326,  3.9975, -1.5027],\n",
      "        [ 4.4693, -3.4049, -2.4280]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.3326,  3.9975, -1.5027],\n",
      "        [ 4.4693, -3.4049, -2.4280]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5350, -2.6984, -2.7873],\n",
      "        [-2.6363,  4.1506, -1.3102]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5350, -2.6984, -2.7873],\n",
      "        [-2.6363,  4.1506, -1.3102]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8279, -3.0478, -2.9338],\n",
      "        [ 0.3317, -1.8801,  1.5068]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8279, -3.0478, -2.9338],\n",
      "        [ 0.3317, -1.8801,  1.5068]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8031, -3.2378, -2.7150],\n",
      "        [ 4.1677, -3.7192, -1.4586]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8031, -3.2378, -2.7150],\n",
      "        [ 4.1677, -3.7192, -1.4586]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7371, -3.1150, -2.6760],\n",
      "        [ 4.6672, -3.0648, -2.6303]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7371, -3.1150, -2.6760],\n",
      "        [ 4.6672, -3.0648, -2.6303]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6282, -3.1847, -2.5497],\n",
      "        [ 4.7696, -3.2180, -2.5167]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6282, -3.1847, -2.5497],\n",
      "        [ 4.7696, -3.2180, -2.5167]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6522, -3.5059, -2.5130],\n",
      "        [ 4.7714, -3.4441, -2.3867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6522, -3.5059, -2.5130],\n",
      "        [ 4.7714, -3.4441, -2.3867]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3642, -3.3431, -1.9442],\n",
      "        [ 4.8310, -3.0136, -2.8350]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3642, -3.3431, -1.9442],\n",
      "        [ 4.8310, -3.0136, -2.8350]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4697, -2.9210, -2.6218],\n",
      "        [ 0.0069, -1.9750,  1.8757]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4697, -2.9210, -2.6218],\n",
      "        [ 0.0069, -1.9750,  1.8757]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3612, -2.8949, -2.8587],\n",
      "        [ 4.5077, -3.0795, -2.6053]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3612, -2.8949, -2.8587],\n",
      "        [ 4.5077, -3.0795, -2.6053]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4092, -3.3115, -2.4123],\n",
      "        [ 4.7822, -3.4548, -2.3937]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4092, -3.3115, -2.4123],\n",
      "        [ 4.7822, -3.4548, -2.3937]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6326, -3.0900, -2.8292],\n",
      "        [-2.5415,  4.1377, -1.0335]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6326, -3.0900, -2.8292],\n",
      "        [-2.5415,  4.1377, -1.0335]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.7355,  4.3129, -1.1928],\n",
      "        [ 4.7268, -3.1748, -2.8529]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.7355,  4.3129, -1.1928],\n",
      "        [ 4.7268, -3.1748, -2.8529]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7147, -3.2678, -2.6379],\n",
      "        [ 4.5602, -3.3350, -2.3152]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7147, -3.2678, -2.6379],\n",
      "        [ 4.5602, -3.3350, -2.3152]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4660, -2.8208, -2.9811],\n",
      "        [ 4.4035, -3.7546, -1.9592]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4660, -2.8208, -2.9811],\n",
      "        [ 4.4035, -3.7546, -1.9592]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.9539,  4.3727, -1.1151],\n",
      "        [ 4.7887, -3.2277, -2.5099]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.9539,  4.3727, -1.1151],\n",
      "        [ 4.7887, -3.2277, -2.5099]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6653, -3.4508, -2.6903],\n",
      "        [ 4.6100, -3.0729, -2.4361]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6653, -3.4508, -2.6903],\n",
      "        [ 4.6100, -3.0729, -2.4361]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9273, -3.3956, -2.2537],\n",
      "        [ 4.7541, -3.2102, -2.3026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9273, -3.3956, -2.2537],\n",
      "        [ 4.7541, -3.2102, -2.3026]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2674, -3.0572, -2.3352],\n",
      "        [ 4.6791, -3.5175, -2.5091]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2674, -3.0572, -2.3352],\n",
      "        [ 4.6791, -3.5175, -2.5091]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2869, -2.5581, -2.6680],\n",
      "        [-2.7362,  4.1727, -0.9829]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2869, -2.5581, -2.6680],\n",
      "        [-2.7362,  4.1727, -0.9829]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7661, -3.0544, -2.4383],\n",
      "        [ 4.4323, -3.0831, -2.4707]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7661, -3.0544, -2.4383],\n",
      "        [ 4.4323, -3.0831, -2.4707]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4187, -3.5107, -2.1786],\n",
      "        [ 3.6640, -3.7021, -0.6629]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4187, -3.5107, -2.1786],\n",
      "        [ 3.6640, -3.7021, -0.6629]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.4755,  4.0533, -1.4441],\n",
      "        [ 4.6045, -3.5036, -2.1751]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.4755,  4.0533, -1.4441],\n",
      "        [ 4.6045, -3.5036, -2.1751]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3350, -3.3433, -2.4469],\n",
      "        [ 4.3478, -2.5756, -2.4254]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3350, -3.3433, -2.4469],\n",
      "        [ 4.3478, -2.5756, -2.4254]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4882, -3.6993, -2.2807],\n",
      "        [ 4.6929, -3.0748, -2.8275]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4882, -3.6993, -2.2807],\n",
      "        [ 4.6929, -3.0748, -2.8275]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7712, -3.1917, -2.7326],\n",
      "        [ 4.5543, -3.7878, -2.2936]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7712, -3.1917, -2.7326],\n",
      "        [ 4.5543, -3.7878, -2.2936]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6247, -2.8066, -2.5027],\n",
      "        [ 4.0484, -2.9788, -1.9819]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6247, -2.8066, -2.5027],\n",
      "        [ 4.0484, -2.9788, -1.9819]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4440, -3.5731, -2.4021],\n",
      "        [ 4.8608, -3.3739, -2.5292]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4440, -3.5731, -2.4021],\n",
      "        [ 4.8608, -3.3739, -2.5292]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7320, -2.6881, -2.6477],\n",
      "        [ 4.7196, -3.4664, -2.3081]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7320, -2.6881, -2.6477],\n",
      "        [ 4.7196, -3.4664, -2.3081]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5142, -3.0214, -2.9476],\n",
      "        [-2.1927,  3.7569, -1.4107]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5142, -3.0214, -2.9476],\n",
      "        [-2.1927,  3.7569, -1.4107]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9402, -3.4502, -2.6220],\n",
      "        [ 4.8530, -3.2081, -2.2981]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9402, -3.4502, -2.6220],\n",
      "        [ 4.8530, -3.2081, -2.2981]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7037, -3.3302, -2.4638],\n",
      "        [ 4.4945, -3.8010, -2.0851]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7037, -3.3302, -2.4638],\n",
      "        [ 4.4945, -3.8010, -2.0851]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7520, -3.2850, -2.7803],\n",
      "        [ 4.8144, -3.2900, -2.5376]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7520, -3.2850, -2.7803],\n",
      "        [ 4.8144, -3.2900, -2.5376]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9504, -3.4967, -2.4876],\n",
      "        [ 4.6489, -3.7164, -2.2128]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9504, -3.4967, -2.4876],\n",
      "        [ 4.6489, -3.7164, -2.2128]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9685, -3.2360, -2.6690],\n",
      "        [ 4.9128, -3.2720, -2.5196]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9685, -3.2360, -2.6690],\n",
      "        [ 4.9128, -3.2720, -2.5196]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4934, -3.1947, -2.6805],\n",
      "        [ 4.6627, -3.1562, -2.3703]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4934, -3.1947, -2.6805],\n",
      "        [ 4.6627, -3.1562, -2.3703]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7433, -3.5363, -2.4531],\n",
      "        [ 4.4806, -3.5825, -2.5123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7433, -3.5363, -2.4531],\n",
      "        [ 4.4806, -3.5825, -2.5123]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5632, -3.5960, -2.0439],\n",
      "        [ 4.6816, -3.6176, -2.1627]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5632, -3.5960, -2.0439],\n",
      "        [ 4.6816, -3.6176, -2.1627]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8513, -3.6004, -2.5442],\n",
      "        [ 2.8999, -3.5466, -0.1393]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8513, -3.6004, -2.5442],\n",
      "        [ 2.8999, -3.5466, -0.1393]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7672, -3.2310, -2.8141],\n",
      "        [ 4.6056, -3.2143, -2.2716]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7672, -3.2310, -2.8141],\n",
      "        [ 4.6056, -3.2143, -2.2716]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5758, -3.7659, -2.2534],\n",
      "        [ 4.8154, -3.4818, -2.6252]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5758, -3.7659, -2.2534],\n",
      "        [ 4.8154, -3.4818, -2.6252]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8844, -3.2578, -2.7257],\n",
      "        [ 4.5627, -3.7511, -1.7241]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8844, -3.2578, -2.7257],\n",
      "        [ 4.5627, -3.7511, -1.7241]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6153, -3.5634, -2.4266],\n",
      "        [ 4.7764, -3.4978, -2.3138]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6153, -3.5634, -2.4266],\n",
      "        [ 4.7764, -3.4978, -2.3138]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7602, -3.2841, -2.5036],\n",
      "        [ 4.6511, -3.3866, -2.5668]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7602, -3.2841, -2.5036],\n",
      "        [ 4.6511, -3.3866, -2.5668]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.0942, -3.6383, -0.6588],\n",
      "        [ 4.7354, -3.3370, -2.5447]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.0942, -3.6383, -0.6588],\n",
      "        [ 4.7354, -3.3370, -2.5447]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6956, -3.1481, -2.4440],\n",
      "        [ 4.5620, -3.3819, -2.2214]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6956, -3.1481, -2.4440],\n",
      "        [ 4.5620, -3.3819, -2.2214]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6695, -3.3632, -2.6402],\n",
      "        [ 4.7504, -3.2478, -2.8239]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6695, -3.3632, -2.6402],\n",
      "        [ 4.7504, -3.2478, -2.8239]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7196, -3.2465, -2.4799],\n",
      "        [ 4.9309, -3.4296, -2.3933]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7196, -3.2465, -2.4799],\n",
      "        [ 4.9309, -3.4296, -2.3933]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5126, -3.3245, -2.4267],\n",
      "        [ 4.6384, -2.7876, -2.7239]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5126, -3.3245, -2.4267],\n",
      "        [ 4.6384, -2.7876, -2.7239]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5377, -3.1438, -2.1630],\n",
      "        [ 4.7971, -3.1072, -2.5889]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5377, -3.1438, -2.1630],\n",
      "        [ 4.7971, -3.1072, -2.5889]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6595, -3.6097, -2.4222],\n",
      "        [ 4.3955, -2.9584, -2.6890]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6595, -3.6097, -2.4222],\n",
      "        [ 4.3955, -2.9584, -2.6890]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7110, -3.1807, -2.5593],\n",
      "        [ 4.6951, -3.4034, -2.5116]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7110, -3.1807, -2.5593],\n",
      "        [ 4.6951, -3.4034, -2.5116]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6605, -3.5814, -2.4182],\n",
      "        [ 4.6627, -3.0750, -2.4872]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6605, -3.5814, -2.4182],\n",
      "        [ 4.6627, -3.0750, -2.4872]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6254, -3.3073, -2.4341],\n",
      "        [ 4.8172, -3.0528, -2.7368]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6254, -3.3073, -2.4341],\n",
      "        [ 4.8172, -3.0528, -2.7368]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8653, -3.3696, -2.3278],\n",
      "        [ 4.8765, -3.7646, -2.3518]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8653, -3.3696, -2.3278],\n",
      "        [ 4.8765, -3.7646, -2.3518]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8056, -3.2196, -2.6082],\n",
      "        [ 4.8802, -3.2748, -2.6984]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8056, -3.2196, -2.6082],\n",
      "        [ 4.8802, -3.2748, -2.6984]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7532, -3.3223, -2.7742],\n",
      "        [ 4.9254, -3.2327, -2.7802]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7532, -3.3223, -2.7742],\n",
      "        [ 4.9254, -3.2327, -2.7802]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9495, -3.3334, -2.5844],\n",
      "        [ 5.0403, -3.1475, -2.5886]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9495, -3.3334, -2.5844],\n",
      "        [ 5.0403, -3.1475, -2.5886]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.1942,  4.0085, -1.5266],\n",
      "        [ 4.7017, -3.1931, -2.6428]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.1942,  4.0085, -1.5266],\n",
      "        [ 4.7017, -3.1931, -2.6428]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9609, -3.3501, -2.3969],\n",
      "        [ 4.9887, -3.1993, -2.6397]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9609, -3.3501, -2.3969],\n",
      "        [ 4.9887, -3.1993, -2.6397]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.6206,  4.1275, -1.2725],\n",
      "        [ 4.8929, -3.3313, -2.1598]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.6206,  4.1275, -1.2725],\n",
      "        [ 4.8929, -3.3313, -2.1598]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 5.0145, -3.4930, -2.5696],\n",
      "        [ 4.8935, -3.6121, -2.3557]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 5.0145, -3.4930, -2.5696],\n",
      "        [ 4.8935, -3.6121, -2.3557]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7369, -3.0485, -2.5052],\n",
      "        [ 4.0861, -3.6623, -1.4717]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7369, -3.0485, -2.5052],\n",
      "        [ 4.0861, -3.6623, -1.4717]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6094, -3.5094, -2.5777],\n",
      "        [-0.6470, -1.4774,  1.9088]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6094, -3.5094, -2.5777],\n",
      "        [-0.6470, -1.4774,  1.9088]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8160, -3.1457, -2.7483],\n",
      "        [ 4.7031, -3.4035, -2.7223]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8160, -3.1457, -2.7483],\n",
      "        [ 4.7031, -3.4035, -2.7223]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.2306, -3.6948, -1.4877],\n",
      "        [ 4.7872, -2.7641, -2.7519]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.2306, -3.6948, -1.4877],\n",
      "        [ 4.7872, -2.7641, -2.7519]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8089, -2.8856, -2.7747],\n",
      "        [ 4.6737, -3.5975, -2.5236]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8089, -2.8856, -2.7747],\n",
      "        [ 4.6737, -3.5975, -2.5236]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8252, -3.0048, -2.8440],\n",
      "        [ 4.6488, -2.9424, -2.9926]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8252, -3.0048, -2.8440],\n",
      "        [ 4.6488, -2.9424, -2.9926]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7837, -3.0849, -2.5188],\n",
      "        [ 4.7563, -3.4517, -2.5942]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7837, -3.0849, -2.5188],\n",
      "        [ 4.7563, -3.4517, -2.5942]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8336, -3.6234, -2.5911],\n",
      "        [ 4.9063, -2.6918, -2.9182]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8336, -3.6234, -2.5911],\n",
      "        [ 4.9063, -2.6918, -2.9182]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7638, -2.7961, -2.7731],\n",
      "        [ 4.5554, -2.6557, -2.9233]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7638, -2.7961, -2.7731],\n",
      "        [ 4.5554, -2.6557, -2.9233]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 5.0738, -3.1260, -2.5909],\n",
      "        [ 4.6702, -2.7383, -2.6281]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 5.0738, -3.1260, -2.5909],\n",
      "        [ 4.6702, -2.7383, -2.6281]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7196, -3.2642, -2.8968],\n",
      "        [ 4.9052, -3.4181, -2.4725]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7196, -3.2642, -2.8968],\n",
      "        [ 4.9052, -3.4181, -2.4725]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8459, -3.2517, -2.5185],\n",
      "        [ 4.0812, -2.1581, -2.9015]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8459, -3.2517, -2.5185],\n",
      "        [ 4.0812, -2.1581, -2.9015]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4640, -2.7879, -2.7997],\n",
      "        [ 4.8183, -3.5785, -2.6371]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4640, -2.7879, -2.7997],\n",
      "        [ 4.8183, -3.5785, -2.6371]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7960, -3.0094, -2.8964],\n",
      "        [ 4.6470, -2.9433, -2.6002]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7960, -3.0094, -2.8964],\n",
      "        [ 4.6470, -2.9433, -2.6002]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8736, -3.2663, -2.6042],\n",
      "        [ 4.4569, -2.9887, -2.9003]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8736, -3.2663, -2.6042],\n",
      "        [ 4.4569, -2.9887, -2.9003]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5409, -2.9008, -2.8911],\n",
      "        [ 4.8701, -3.2402, -2.5424]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5409, -2.9008, -2.8911],\n",
      "        [ 4.8701, -3.2402, -2.5424]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6278, -3.1400, -2.7335],\n",
      "        [ 4.9383, -3.2478, -2.8434]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6278, -3.1400, -2.7335],\n",
      "        [ 4.9383, -3.2478, -2.8434]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.6721,  4.3109, -1.2209],\n",
      "        [ 4.5459, -2.7920, -2.6360]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.6721,  4.3109, -1.2209],\n",
      "        [ 4.5459, -2.7920, -2.6360]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8825, -3.1719, -2.6377],\n",
      "        [ 4.7294, -3.3475, -2.5877]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8825, -3.1719, -2.6377],\n",
      "        [ 4.7294, -3.3475, -2.5877]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5825, -3.4779, -2.3535],\n",
      "        [ 4.9191, -3.3404, -2.5916]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5825, -3.4779, -2.3535],\n",
      "        [ 4.9191, -3.3404, -2.5916]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6842, -3.0970, -2.7040],\n",
      "        [ 4.6234, -2.9148, -2.5901]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6842, -3.0970, -2.7040],\n",
      "        [ 4.6234, -2.9148, -2.5901]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6822, -2.5137, -2.8300],\n",
      "        [ 4.7190, -3.1059, -2.4768]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6822, -2.5137, -2.8300],\n",
      "        [ 4.7190, -3.1059, -2.4768]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 5.0077, -3.1582, -2.8175],\n",
      "        [-2.2540,  3.9559, -1.7308]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 5.0077, -3.1582, -2.8175],\n",
      "        [-2.2540,  3.9559, -1.7308]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0091, -2.0780, -2.6466],\n",
      "        [ 4.8583, -3.2968, -2.6709]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0091, -2.0780, -2.6466],\n",
      "        [ 4.8583, -3.2968, -2.6709]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4481, -2.9902, -2.7715],\n",
      "        [ 4.3161, -1.8567, -2.8006]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4481, -2.9902, -2.7715],\n",
      "        [ 4.3161, -1.8567, -2.8006]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6759, -3.2168, -2.7200],\n",
      "        [ 4.4294, -3.4671, -2.4759]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6759, -3.2168, -2.7200],\n",
      "        [ 4.4294, -3.4671, -2.4759]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6934, -3.0309, -3.0679],\n",
      "        [ 4.7137, -3.2512, -2.7743]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6934, -3.0309, -3.0679],\n",
      "        [ 4.7137, -3.2512, -2.7743]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7885, -3.6432, -2.5470],\n",
      "        [ 4.5059, -2.7568, -2.7383]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7885, -3.6432, -2.5470],\n",
      "        [ 4.5059, -2.7568, -2.7383]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5729, -2.9547, -2.8205],\n",
      "        [ 4.8698, -3.4020, -2.5047]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5729, -2.9547, -2.8205],\n",
      "        [ 4.8698, -3.4020, -2.5047]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7320, -2.8508, -2.9416],\n",
      "        [ 4.8208, -3.2387, -2.5600]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7320, -2.8508, -2.9416],\n",
      "        [ 4.8208, -3.2387, -2.5600]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7824, -2.6748, -2.7328],\n",
      "        [ 4.4008, -3.2637, -2.3749]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7824, -2.6748, -2.7328],\n",
      "        [ 4.4008, -3.2637, -2.3749]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6662, -3.2541, -2.7428],\n",
      "        [ 4.6892, -3.0012, -2.5870]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6662, -3.2541, -2.7428],\n",
      "        [ 4.6892, -3.0012, -2.5870]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9383, -3.0663, -2.8507],\n",
      "        [ 4.8040, -3.0098, -2.5118]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9383, -3.0663, -2.8507],\n",
      "        [ 4.8040, -3.0098, -2.5118]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6306, -2.5540, -2.6036],\n",
      "        [ 4.8308, -3.4824, -2.7350]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6306, -2.5540, -2.6036],\n",
      "        [ 4.8308, -3.4824, -2.7350]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7656, -3.0333, -2.7123],\n",
      "        [ 4.7441, -3.0938, -2.6274]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7656, -3.0333, -2.7123],\n",
      "        [ 4.7441, -3.0938, -2.6274]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3705, -2.5161, -2.8555],\n",
      "        [-1.9244,  3.8510, -1.7353]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3705, -2.5161, -2.8555],\n",
      "        [-1.9244,  3.8510, -1.7353]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8570, -3.3199, -2.5223],\n",
      "        [ 4.7280, -3.3104, -2.8257]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8570, -3.3199, -2.5223],\n",
      "        [ 4.7280, -3.3104, -2.8257]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5608, -2.9710, -2.6260],\n",
      "        [ 4.9174, -3.6456, -2.6525]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5608, -2.9710, -2.6260],\n",
      "        [ 4.9174, -3.6456, -2.6525]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6207, -3.1831, -2.4183],\n",
      "        [ 4.6235, -3.1181, -2.6318]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6207, -3.1831, -2.4183],\n",
      "        [ 4.6235, -3.1181, -2.6318]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4616, -2.8555, -2.7904],\n",
      "        [ 4.6348, -2.8180, -2.8959]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4616, -2.8555, -2.7904],\n",
      "        [ 4.6348, -2.8180, -2.8959]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6934, -2.8337, -2.8104],\n",
      "        [ 4.9144, -3.2328, -2.8689]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6934, -2.8337, -2.8104],\n",
      "        [ 4.9144, -3.2328, -2.8689]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.4128, -0.7570,  2.4373],\n",
      "        [ 4.8262, -2.7183, -2.9420]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.4128, -0.7570,  2.4373],\n",
      "        [ 4.8262, -2.7183, -2.9420]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.1996, -0.6468,  1.9864],\n",
      "        [ 4.7548, -3.3299, -2.7499]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.1996, -0.6468,  1.9864],\n",
      "        [ 4.7548, -3.3299, -2.7499]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9397, -3.2494, -2.7749],\n",
      "        [ 4.5208, -3.0456, -2.3650]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9397, -3.2494, -2.7749],\n",
      "        [ 4.5208, -3.0456, -2.3650]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7912, -3.0073, -2.9719],\n",
      "        [ 4.8077, -3.0720, -2.6982]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7912, -3.0073, -2.9719],\n",
      "        [ 4.8077, -3.0720, -2.6982]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7256, -3.4642, -2.7520],\n",
      "        [ 4.6749, -2.9851, -2.8743]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7256, -3.4642, -2.7520],\n",
      "        [ 4.6749, -2.9851, -2.8743]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7399, -3.0966, -2.5314],\n",
      "        [ 4.5131, -2.6757, -2.9887]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7399, -3.0966, -2.5314],\n",
      "        [ 4.5131, -2.6757, -2.9887]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5745, -2.7445, -3.0202],\n",
      "        [ 4.7462, -3.1117, -2.9486]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5745, -2.7445, -3.0202],\n",
      "        [ 4.7462, -3.1117, -2.9486]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.4068,  3.9688, -1.6076],\n",
      "        [ 4.9488, -3.0879, -2.8764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.4068,  3.9688, -1.6076],\n",
      "        [ 4.9488, -3.0879, -2.8764]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6681, -3.7259, -1.9851],\n",
      "        [ 4.8105, -3.3837, -2.5063]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6681, -3.7259, -1.9851],\n",
      "        [ 4.8105, -3.3837, -2.5063]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3555, -2.2270, -2.9392],\n",
      "        [ 4.3916, -3.1012, -2.7232]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3555, -2.2270, -2.9392],\n",
      "        [ 4.3916, -3.1012, -2.7232]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.6992, -0.4836,  2.4227],\n",
      "        [ 4.8597, -3.4146, -2.7355]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.6992, -0.4836,  2.4227],\n",
      "        [ 4.8597, -3.4146, -2.7355]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6135, -2.9502, -2.6666],\n",
      "        [-1.9483,  3.8232, -1.5870]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6135, -2.9502, -2.6666],\n",
      "        [-1.9483,  3.8232, -1.5870]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6809, -3.1417, -2.8165],\n",
      "        [-1.4506, -0.7067,  2.3692]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6809, -3.1417, -2.8165],\n",
      "        [-1.4506, -0.7067,  2.3692]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6276, -3.0854, -2.6816],\n",
      "        [ 4.6465, -2.6622, -2.7466]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6276, -3.0854, -2.6816],\n",
      "        [ 4.6465, -2.6622, -2.7466]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.8300, -1.0452,  2.0624],\n",
      "        [-1.2528, -0.9655,  2.2676]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.8300, -1.0452,  2.0624],\n",
      "        [-1.2528, -0.9655,  2.2676]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6368, -2.6046, -2.6325],\n",
      "        [ 4.7615, -3.3147, -2.6117]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6368, -2.6046, -2.6325],\n",
      "        [ 4.7615, -3.3147, -2.6117]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.3264, -0.8688,  2.3447],\n",
      "        [-1.5993, -0.4523,  2.3903]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.3264, -0.8688,  2.3447],\n",
      "        [-1.5993, -0.4523,  2.3903]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7428, -2.8524, -2.7034],\n",
      "        [ 4.7579, -3.4465, -2.6085]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7428, -2.8524, -2.7034],\n",
      "        [ 4.7579, -3.4465, -2.6085]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7486, -3.2838, -2.8249],\n",
      "        [-1.5894, -0.4767,  2.4744]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7486, -3.2838, -2.8249],\n",
      "        [-1.5894, -0.4767,  2.4744]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7103, -3.3463, -2.6684],\n",
      "        [ 4.0490, -3.8048, -1.5955]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7103, -3.3463, -2.6684],\n",
      "        [ 4.0490, -3.8048, -1.5955]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7516, -3.0368, -2.5429],\n",
      "        [ 4.6574, -3.1761, -2.4432]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7516, -3.0368, -2.5429],\n",
      "        [ 4.6574, -3.1761, -2.4432]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9166, -3.3044, -2.7077],\n",
      "        [ 4.9663, -3.2901, -2.8298]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9166, -3.3044, -2.7077],\n",
      "        [ 4.9663, -3.2901, -2.8298]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7603, -3.4790, -2.3165],\n",
      "        [ 4.8695, -3.3956, -2.7896]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7603, -3.4790, -2.3165],\n",
      "        [ 4.8695, -3.3956, -2.7896]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9622, -3.2666, -2.6426],\n",
      "        [ 4.8985, -3.4832, -2.4696]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9622, -3.2666, -2.6426],\n",
      "        [ 4.8985, -3.4832, -2.4696]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7042, -3.1039, -2.5426],\n",
      "        [ 4.6760, -2.9038, -2.7323]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7042, -3.1039, -2.5426],\n",
      "        [ 4.6760, -2.9038, -2.7323]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7496, -2.8744, -2.6355],\n",
      "        [ 4.7532, -3.3376, -2.5345]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7496, -2.8744, -2.6355],\n",
      "        [ 4.7532, -3.3376, -2.5345]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5227, -2.8736, -2.9557],\n",
      "        [ 4.8812, -3.2757, -2.5544]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5227, -2.8736, -2.9557],\n",
      "        [ 4.8812, -3.2757, -2.5544]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8779, -3.4465, -2.5628],\n",
      "        [ 4.8679, -3.6397, -2.4950]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8779, -3.4465, -2.5628],\n",
      "        [ 4.8679, -3.6397, -2.4950]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7405, -3.2683, -2.3052],\n",
      "        [ 4.7841, -3.2238, -2.7189]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7405, -3.2683, -2.3052],\n",
      "        [ 4.7841, -3.2238, -2.7189]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 5.0342, -3.4222, -2.6047],\n",
      "        [ 4.8975, -3.2659, -2.4518]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 5.0342, -3.4222, -2.6047],\n",
      "        [ 4.8975, -3.2659, -2.4518]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6755, -2.7543, -2.6646],\n",
      "        [ 4.5804, -3.1021, -2.6813]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6755, -2.7543, -2.6646],\n",
      "        [ 4.5804, -3.1021, -2.6813]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6717, -3.0199, -2.6853],\n",
      "        [ 4.5980, -3.0644, -2.4859]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6717, -3.0199, -2.6853],\n",
      "        [ 4.5980, -3.0644, -2.4859]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8225, -3.3108, -2.6436],\n",
      "        [-1.0305,  3.2875, -2.1997]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8225, -3.3108, -2.6436],\n",
      "        [-1.0305,  3.2875, -2.1997]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6353, -3.8443, -2.1292],\n",
      "        [ 4.6804, -3.4212, -2.6316]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6353, -3.8443, -2.1292],\n",
      "        [ 4.6804, -3.4212, -2.6316]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7128, -3.3367, -2.7333],\n",
      "        [ 4.7592, -3.1094, -2.6187]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7128, -3.3367, -2.7333],\n",
      "        [ 4.7592, -3.1094, -2.6187]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9006, -3.0745, -2.9299],\n",
      "        [ 4.6308, -2.6405, -2.8963]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9006, -3.0745, -2.9299],\n",
      "        [ 4.6308, -2.6405, -2.8963]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8217, -3.4405, -2.4231],\n",
      "        [ 4.5734, -2.7738, -3.0042]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8217, -3.4405, -2.4231],\n",
      "        [ 4.5734, -2.7738, -3.0042]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8530, -2.8554, -2.9064],\n",
      "        [ 4.4225, -2.6503, -2.7441]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8530, -2.8554, -2.9064],\n",
      "        [ 4.4225, -2.6503, -2.7441]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6499, -3.1707, -2.6487],\n",
      "        [ 4.8055, -3.3382, -2.7509]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6499, -3.1707, -2.6487],\n",
      "        [ 4.8055, -3.3382, -2.7509]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.0649,  3.9196, -1.6568],\n",
      "        [ 4.7965, -3.3145, -2.4561]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.0649,  3.9196, -1.6568],\n",
      "        [ 4.7965, -3.3145, -2.4561]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7065, -3.6743, -2.3413],\n",
      "        [ 4.7015, -3.0496, -2.7258]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7065, -3.6743, -2.3413],\n",
      "        [ 4.7015, -3.0496, -2.7258]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7984, -3.0631, -2.5939],\n",
      "        [-1.2600, -0.8287,  2.4060]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7984, -3.0631, -2.5939],\n",
      "        [-1.2600, -0.8287,  2.4060]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6790, -3.1811, -2.7747],\n",
      "        [ 4.7065, -2.9458, -2.6153]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6790, -3.1811, -2.7747],\n",
      "        [ 4.7065, -2.9458, -2.6153]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8426, -3.3121, -2.5593],\n",
      "        [ 4.7914, -3.1653, -2.6178]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8426, -3.3121, -2.5593],\n",
      "        [ 4.7914, -3.1653, -2.6178]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9826, -3.3007, -2.6372],\n",
      "        [ 4.7782, -3.0225, -2.6704]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9826, -3.3007, -2.6372],\n",
      "        [ 4.7782, -3.0225, -2.6704]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8169, -3.1411, -2.7254],\n",
      "        [ 4.7225, -3.1087, -2.8552]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8169, -3.1411, -2.7254],\n",
      "        [ 4.7225, -3.1087, -2.8552]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-0.7602, -1.4739,  2.0935],\n",
      "        [ 4.6855, -2.6768, -2.9784]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-0.7602, -1.4739,  2.0935],\n",
      "        [ 4.6855, -2.6768, -2.9784]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9084, -3.0660, -2.8587],\n",
      "        [ 4.3080, -3.1574, -2.4551]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9084, -3.0660, -2.8587],\n",
      "        [ 4.3080, -3.1574, -2.4551]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8146, -2.9341, -2.8690],\n",
      "        [ 4.6730, -3.5082, -2.5798]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8146, -2.9341, -2.8690],\n",
      "        [ 4.6730, -3.5082, -2.5798]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9550, -2.9675, -2.8536],\n",
      "        [ 4.9103, -3.2548, -2.7917]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9550, -2.9675, -2.8536],\n",
      "        [ 4.9103, -3.2548, -2.7917]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7478, -3.2111, -2.8266],\n",
      "        [ 1.4204, -2.3847,  0.4530]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7478, -3.2111, -2.8266],\n",
      "        [ 1.4204, -2.3847,  0.4530]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9790, -3.1743, -2.5227],\n",
      "        [ 4.8376, -3.0328, -2.6266]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9790, -3.1743, -2.5227],\n",
      "        [ 4.8376, -3.0328, -2.6266]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3373, -3.6932, -1.5316],\n",
      "        [ 4.7606, -3.2475, -2.4147]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.3373, -3.6932, -1.5316],\n",
      "        [ 4.7606, -3.2475, -2.4147]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5633, -2.2587,  0.5328],\n",
      "        [ 4.5974, -3.1552, -2.4518]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 1.5633, -2.2587,  0.5328],\n",
      "        [ 4.5974, -3.1552, -2.4518]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6615, -3.1536, -2.7393],\n",
      "        [ 4.8532, -3.3471, -2.7129]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6615, -3.1536, -2.7393],\n",
      "        [ 4.8532, -3.3471, -2.7129]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9919, -3.1881, -2.7596],\n",
      "        [-1.9908,  3.9935, -1.9044]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9919, -3.1881, -2.7596],\n",
      "        [-1.9908,  3.9935, -1.9044]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7330, -3.2454, -2.6567],\n",
      "        [ 4.8774, -2.9654, -2.5835]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7330, -3.2454, -2.6567],\n",
      "        [ 4.8774, -2.9654, -2.5835]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8019, -3.4541, -2.4485],\n",
      "        [ 4.7113, -3.3606, -2.6738]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8019, -3.4541, -2.4485],\n",
      "        [ 4.7113, -3.3606, -2.6738]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7734, -3.3889, -2.5400],\n",
      "        [ 4.0737, -1.8989, -2.9730]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7734, -3.3889, -2.5400],\n",
      "        [ 4.0737, -1.8989, -2.9730]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4492, -2.6142, -2.7885],\n",
      "        [ 4.6908, -3.2856, -2.5798]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4492, -2.6142, -2.7885],\n",
      "        [ 4.6908, -3.2856, -2.5798]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4659, -2.5610, -2.7152],\n",
      "        [-1.1780, -1.0434,  2.4984]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4659, -2.5610, -2.7152],\n",
      "        [-1.1780, -1.0434,  2.4984]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7252, -3.1703, -2.5089],\n",
      "        [-2.0948,  4.0006, -1.6203]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7252, -3.1703, -2.5089],\n",
      "        [-2.0948,  4.0006, -1.6203]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6681, -3.0569, -2.7147],\n",
      "        [ 4.4995, -2.8265, -2.8822]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6681, -3.0569, -2.7147],\n",
      "        [ 4.4995, -2.8265, -2.8822]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9554, -3.3425, -2.5699],\n",
      "        [ 4.8801, -3.1663, -2.4105]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9554, -3.3425, -2.5699],\n",
      "        [ 4.8801, -3.1663, -2.4105]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.3933,  3.9442, -1.5333],\n",
      "        [ 4.6199, -3.3855, -2.5931]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.3933,  3.9442, -1.5333],\n",
      "        [ 4.6199, -3.3855, -2.5931]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8424, -3.3206, -2.5088],\n",
      "        [ 4.8301, -3.5515, -2.4541]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8424, -3.3206, -2.5088],\n",
      "        [ 4.8301, -3.5515, -2.4541]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6194, -2.7051, -2.9317],\n",
      "        [ 4.7914, -2.8711, -2.7515]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6194, -2.7051, -2.9317],\n",
      "        [ 4.7914, -2.8711, -2.7515]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7135, -3.4061, -2.5329],\n",
      "        [ 4.7754, -3.2584, -2.6108]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7135, -3.4061, -2.5329],\n",
      "        [ 4.7754, -3.2584, -2.6108]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7053, -2.8113, -2.7701],\n",
      "        [ 4.8314, -3.1667, -2.6422]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7053, -2.8113, -2.7701],\n",
      "        [ 4.8314, -3.1667, -2.6422]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9742, -3.3583, -2.5129],\n",
      "        [ 4.5668, -2.5534, -3.0761]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9742, -3.3583, -2.5129],\n",
      "        [ 4.5668, -2.5534, -3.0761]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6589, -3.0020, -3.0260],\n",
      "        [-1.3078, -0.8376,  2.3303]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6589, -3.0020, -3.0260],\n",
      "        [-1.3078, -0.8376,  2.3303]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5591, -2.9360, -2.9741],\n",
      "        [ 4.7566, -3.1278, -2.7677]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5591, -2.9360, -2.9741],\n",
      "        [ 4.7566, -3.1278, -2.7677]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7285, -3.2175, -2.5209],\n",
      "        [ 4.6973, -3.2538, -2.4763]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7285, -3.2175, -2.5209],\n",
      "        [ 4.6973, -3.2538, -2.4763]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4555, -2.9418, -2.6363],\n",
      "        [ 4.5546, -3.2009, -2.7014]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4555, -2.9418, -2.6363],\n",
      "        [ 4.5546, -3.2009, -2.7014]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9657, -3.1572, -2.7511],\n",
      "        [ 4.5186, -2.4648, -2.7666]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9657, -3.1572, -2.7511],\n",
      "        [ 4.5186, -2.4648, -2.7666]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6588, -3.7765, -2.1274],\n",
      "        [-1.3996, -0.9988,  2.3449]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6588, -3.7765, -2.1274],\n",
      "        [-1.3996, -0.9988,  2.3449]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8330, -2.7915, -2.2122],\n",
      "        [ 4.7589, -3.0459, -2.6943]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.8330, -2.7915, -2.2122],\n",
      "        [ 4.7589, -3.0459, -2.6943]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7649, -3.2904, -2.6841],\n",
      "        [ 4.5882, -2.5748, -3.1011]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7649, -3.2904, -2.6841],\n",
      "        [ 4.5882, -2.5748, -3.1011]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 5.1242, -2.9624, -2.4887],\n",
      "        [ 4.4285, -3.0570, -2.2878]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 5.1242, -2.9624, -2.4887],\n",
      "        [ 4.4285, -3.0570, -2.2878]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9922, -3.2888, -2.5417],\n",
      "        [ 3.7114, -2.0416, -2.8413]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9922, -3.2888, -2.5417],\n",
      "        [ 3.7114, -2.0416, -2.8413]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8229, -2.9169, -2.6145],\n",
      "        [ 4.9021, -3.2345, -2.7563]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8229, -2.9169, -2.6145],\n",
      "        [ 4.9021, -3.2345, -2.7563]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9081, -3.0533, -2.7112],\n",
      "        [ 4.4844, -3.3189, -2.5654]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9081, -3.0533, -2.7112],\n",
      "        [ 4.4844, -3.3189, -2.5654]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8406, -3.1039, -2.6868],\n",
      "        [ 4.8803, -3.3895, -2.2097]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8406, -3.1039, -2.6868],\n",
      "        [ 4.8803, -3.3895, -2.2097]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7856, -3.3990, -2.6249],\n",
      "        [-2.3830,  4.1395, -1.4805]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7856, -3.3990, -2.6249],\n",
      "        [-2.3830,  4.1395, -1.4805]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6626, -2.9357, -2.7316],\n",
      "        [ 4.5920, -3.0564, -2.3415]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6626, -2.9357, -2.7316],\n",
      "        [ 4.5920, -3.0564, -2.3415]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8455, -3.4138, -2.7511],\n",
      "        [-2.3507,  4.0700, -1.3857]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8455, -3.4138, -2.7511],\n",
      "        [-2.3507,  4.0700, -1.3857]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7187, -3.3278, -2.6083],\n",
      "        [ 4.8260, -3.2673, -2.4146]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7187, -3.3278, -2.6083],\n",
      "        [ 4.8260, -3.2673, -2.4146]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4618, -3.0242, -2.8154],\n",
      "        [ 4.7334, -3.3097, -2.5252]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4618, -3.0242, -2.8154],\n",
      "        [ 4.7334, -3.3097, -2.5252]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.7390,  3.9780, -1.8161],\n",
      "        [ 4.7300, -3.4054, -2.6008]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.7390,  3.9780, -1.8161],\n",
      "        [ 4.7300, -3.4054, -2.6008]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6575, -3.0704, -2.9190],\n",
      "        [ 4.6616, -2.9810, -2.6459]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6575, -3.0704, -2.9190],\n",
      "        [ 4.6616, -2.9810, -2.6459]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8402, -3.1277, -2.9417],\n",
      "        [-1.5407, -1.0921,  2.5999]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8402, -3.1277, -2.9417],\n",
      "        [-1.5407, -1.0921,  2.5999]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5589, -3.2319,  0.0329],\n",
      "        [ 4.7237, -3.5093, -2.6067]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 2.5589, -3.2319,  0.0329],\n",
      "        [ 4.7237, -3.5093, -2.6067]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.4099, -1.1051,  2.5166],\n",
      "        [ 4.5173, -3.4638, -2.4450]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.4099, -1.1051,  2.5166],\n",
      "        [ 4.5173, -3.4638, -2.4450]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5923, -2.8961, -2.8294],\n",
      "        [ 4.7627, -3.2614, -2.6338]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5923, -2.8961, -2.8294],\n",
      "        [ 4.7627, -3.2614, -2.6338]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9164, -3.3553, -2.6835],\n",
      "        [-1.1202, -1.2727,  2.2305]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9164, -3.3553, -2.6835],\n",
      "        [-1.1202, -1.2727,  2.2305]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8260, -3.1529, -2.6950],\n",
      "        [ 4.7220, -3.1964, -2.6613]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8260, -3.1529, -2.6950],\n",
      "        [ 4.7220, -3.1964, -2.6613]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9512, -3.3945, -2.5517],\n",
      "        [ 4.8238, -3.6468, -2.3606]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9512, -3.3945, -2.5517],\n",
      "        [ 4.8238, -3.6468, -2.3606]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8999, -3.4447, -2.7445],\n",
      "        [ 4.8886, -3.2002, -2.3354]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8999, -3.4447, -2.7445],\n",
      "        [ 4.8886, -3.2002, -2.3354]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7595, -3.5604, -2.4816],\n",
      "        [ 4.7308, -2.9536, -2.7476]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7595, -3.5604, -2.4816],\n",
      "        [ 4.7308, -2.9536, -2.7476]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7179, -3.1598, -2.6361],\n",
      "        [ 4.6659, -3.0594, -2.8060]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7179, -3.1598, -2.6361],\n",
      "        [ 4.6659, -3.0594, -2.8060]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7480, -3.2702, -2.4593],\n",
      "        [ 4.5918, -2.9269, -2.8181]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7480, -3.2702, -2.4593],\n",
      "        [ 4.5918, -2.9269, -2.8181]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5782, -3.2440, -2.5239],\n",
      "        [ 4.4584, -2.7055, -2.9623]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5782, -3.2440, -2.5239],\n",
      "        [ 4.4584, -2.7055, -2.9623]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7034, -3.2955, -2.6716],\n",
      "        [ 4.9060, -3.3337, -2.8547]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7034, -3.2955, -2.6716],\n",
      "        [ 4.9060, -3.3337, -2.8547]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5875, -2.8987, -2.7194],\n",
      "        [ 4.9183, -3.5054, -2.5185]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5875, -2.8987, -2.7194],\n",
      "        [ 4.9183, -3.5054, -2.5185]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7346, -3.2454, -2.5769],\n",
      "        [-1.3580, -1.0141,  2.3532]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7346, -3.2454, -2.5769],\n",
      "        [-1.3580, -1.0141,  2.3532]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6746, -2.9621, -2.4689],\n",
      "        [ 5.0049, -2.9720, -2.8335]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6746, -2.9621, -2.4689],\n",
      "        [ 5.0049, -2.9720, -2.8335]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7987, -3.2184, -2.6335],\n",
      "        [ 4.8294, -3.2368, -2.7788]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7987, -3.2184, -2.6335],\n",
      "        [ 4.8294, -3.2368, -2.7788]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5581, -3.0378, -2.6532],\n",
      "        [ 4.8849, -2.9939, -2.8514]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5581, -3.0378, -2.6532],\n",
      "        [ 4.8849, -2.9939, -2.8514]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.1124,  4.1113, -1.7165],\n",
      "        [-2.2857,  4.0729, -1.6079]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.1124,  4.1113, -1.7165],\n",
      "        [-2.2857,  4.0729, -1.6079]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7689, -3.1671, -2.8174],\n",
      "        [ 4.7722, -2.9896, -2.6394]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7689, -3.1671, -2.8174],\n",
      "        [ 4.7722, -2.9896, -2.6394]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7186, -3.1773, -2.3365],\n",
      "        [ 4.9433, -3.5413, -2.5770]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7186, -3.1773, -2.3365],\n",
      "        [ 4.9433, -3.5413, -2.5770]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7258, -2.9875, -2.8459],\n",
      "        [-1.6191, -0.6677,  2.4675]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7258, -2.9875, -2.8459],\n",
      "        [-1.6191, -0.6677,  2.4675]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9160, -3.3054, -2.5530],\n",
      "        [-2.4368,  4.1314, -1.5108]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9160, -3.3054, -2.5530],\n",
      "        [-2.4368,  4.1314, -1.5108]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6409, -2.9395, -2.5134],\n",
      "        [ 4.8437, -3.3792, -2.5202]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6409, -2.9395, -2.5134],\n",
      "        [ 4.8437, -3.3792, -2.5202]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7252, -3.4721, -2.6621],\n",
      "        [ 4.5148, -2.8586, -2.6184]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7252, -3.4721, -2.6621],\n",
      "        [ 4.5148, -2.8586, -2.6184]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9042, -3.3425, -2.6912],\n",
      "        [ 4.6384, -2.9391, -2.9546]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9042, -3.3425, -2.6912],\n",
      "        [ 4.6384, -2.9391, -2.9546]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9207, -3.4479, -2.3310],\n",
      "        [ 4.7574, -3.2310, -2.6328]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9207, -3.4479, -2.3310],\n",
      "        [ 4.7574, -3.2310, -2.6328]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6927, -3.2642, -2.4594],\n",
      "        [ 4.4828, -2.2390, -2.9994]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6927, -3.2642, -2.4594],\n",
      "        [ 4.4828, -2.2390, -2.9994]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7350, -2.9169, -3.0489],\n",
      "        [ 4.5672, -3.2111, -2.7096]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7350, -2.9169, -3.0489],\n",
      "        [ 4.5672, -3.2111, -2.7096]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7952, -3.0481, -2.8475],\n",
      "        [ 4.7877, -3.5532, -2.7128]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7952, -3.0481, -2.8475],\n",
      "        [ 4.7877, -3.5532, -2.7128]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8324, -3.5892, -2.3698],\n",
      "        [ 4.6634, -3.6135, -2.4847]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8324, -3.5892, -2.3698],\n",
      "        [ 4.6634, -3.6135, -2.4847]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7449, -2.9686, -2.8397],\n",
      "        [ 4.5639, -2.8774, -2.7185]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7449, -2.9686, -2.8397],\n",
      "        [ 4.5639, -2.8774, -2.7185]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-2.3467,  4.0783, -1.6763],\n",
      "        [ 4.7452, -3.1499, -2.8817]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-2.3467,  4.0783, -1.6763],\n",
      "        [ 4.7452, -3.1499, -2.8817]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7048, -3.4110, -2.7118],\n",
      "        [ 4.7835, -3.3930, -2.6491]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7048, -3.4110, -2.7118],\n",
      "        [ 4.7835, -3.3930, -2.6491]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4264, -3.9034, -2.0252],\n",
      "        [ 4.8245, -3.3740, -2.7416]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4264, -3.9034, -2.0252],\n",
      "        [ 4.8245, -3.3740, -2.7416]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7256, -2.9039, -2.7854],\n",
      "        [-1.9620,  3.8752, -1.8048]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7256, -2.9039, -2.7854],\n",
      "        [-1.9620,  3.8752, -1.8048]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6326, -3.3134, -2.7075],\n",
      "        [ 4.8788, -3.1840, -2.6818]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6326, -3.3134, -2.7075],\n",
      "        [ 4.8788, -3.1840, -2.6818]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6464, -2.8502, -2.6652],\n",
      "        [-1.4089, -0.8376,  2.2584]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6464, -2.8502, -2.6652],\n",
      "        [-1.4089, -0.8376,  2.2584]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.6322, -0.8635,  2.3629],\n",
      "        [ 4.5036, -2.5107, -2.8106]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.6322, -0.8635,  2.3629],\n",
      "        [ 4.5036, -2.5107, -2.8106]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7840, -3.3240, -2.6130],\n",
      "        [ 4.7696, -2.9582, -2.7629]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7840, -3.3240, -2.6130],\n",
      "        [ 4.7696, -2.9582, -2.7629]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6222, -2.7320, -2.8209],\n",
      "        [ 4.7492, -3.3493, -2.3393]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6222, -2.7320, -2.8209],\n",
      "        [ 4.7492, -3.3493, -2.3393]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5056, -2.9745, -2.7077],\n",
      "        [ 4.9339, -3.2368, -2.6365]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5056, -2.9745, -2.7077],\n",
      "        [ 4.9339, -3.2368, -2.6365]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.3115, -0.9699,  2.4769],\n",
      "        [-2.3118,  4.1624, -1.7692]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.3115, -0.9699,  2.4769],\n",
      "        [-2.3118,  4.1624, -1.7692]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6792, -2.9714, -2.7772],\n",
      "        [ 4.8489, -2.9463, -2.7758]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6792, -2.9714, -2.7772],\n",
      "        [ 4.8489, -2.9463, -2.7758]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7325, -3.2376, -2.6193],\n",
      "        [ 4.9101, -3.5534, -2.4995]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7325, -3.2376, -2.6193],\n",
      "        [ 4.9101, -3.5534, -2.4995]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5182, -2.7171, -2.7131],\n",
      "        [-1.6564, -1.1047,  2.5969]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5182, -2.7171, -2.7131],\n",
      "        [-1.6564, -1.1047,  2.5969]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7020, -3.1962, -2.7218],\n",
      "        [-2.0473,  4.1658, -1.7457]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7020, -3.1962, -2.7218],\n",
      "        [-2.0473,  4.1658, -1.7457]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4510, -2.3435, -2.9987],\n",
      "        [ 4.5417, -2.3392, -3.0304]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4510, -2.3435, -2.9987],\n",
      "        [ 4.5417, -2.3392, -3.0304]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7257, -2.9280, -2.8376],\n",
      "        [ 4.9977, -3.4170, -2.7146]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7257, -2.9280, -2.8376],\n",
      "        [ 4.9977, -3.4170, -2.7146]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7505, -3.2892, -2.3741],\n",
      "        [-2.3342,  4.1812, -1.6439]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7505, -3.2892, -2.3741],\n",
      "        [-2.3342,  4.1812, -1.6439]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8307, -3.3411, -2.6510],\n",
      "        [ 4.5523, -2.4411, -2.8997]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8307, -3.3411, -2.6510],\n",
      "        [ 4.5523, -2.4411, -2.8997]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0346, -3.8986, -1.6799],\n",
      "        [-2.2781,  3.9678, -1.5873]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.0346, -3.8986, -1.6799],\n",
      "        [-2.2781,  3.9678, -1.5873]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.6204, -2.9956, -2.3212],\n",
      "        [ 4.6676, -3.1579, -2.6039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.6204, -2.9956, -2.3212],\n",
      "        [ 4.6676, -3.1579, -2.6039]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7116, -3.4115, -2.4031],\n",
      "        [-1.1164, -1.1910,  2.4104]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7116, -3.4115, -2.4031],\n",
      "        [-1.1164, -1.1910,  2.4104]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7014, -3.1817, -2.6471],\n",
      "        [ 4.7188, -3.1737, -2.7562]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7014, -3.1817, -2.6471],\n",
      "        [ 4.7188, -3.1737, -2.7562]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[-1.4068, -1.0396,  2.3739],\n",
      "        [-2.2712,  3.9601, -1.6207]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[-1.4068, -1.0396,  2.3739],\n",
      "        [-2.2712,  3.9601, -1.6207]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7795, -3.2339, -2.7070],\n",
      "        [ 4.7506, -2.9001, -2.6849]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7795, -3.2339, -2.7070],\n",
      "        [ 4.7506, -2.9001, -2.6849]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5428, -2.7336, -2.7100],\n",
      "        [ 4.6108, -2.6876, -2.9039]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5428, -2.7336, -2.7100],\n",
      "        [ 4.6108, -2.6876, -2.9039]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5961, -3.6648, -2.2336],\n",
      "        [ 4.5463, -2.9493, -2.5933]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5961, -3.6648, -2.2336],\n",
      "        [ 4.5463, -2.9493, -2.5933]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7671, -3.1915, -2.6920],\n",
      "        [ 4.7799, -3.1927, -2.5615]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7671, -3.1915, -2.6920],\n",
      "        [ 4.7799, -3.1927, -2.5615]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4743, -3.0505, -2.2479],\n",
      "        [ 4.5480, -2.7612, -2.6085]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4743, -3.0505, -2.2479],\n",
      "        [ 4.5480, -2.7612, -2.6085]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4882, -3.4485, -2.5001],\n",
      "        [ 4.7726, -3.3408, -2.7496]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.4882, -3.4485, -2.5001],\n",
      "        [ 4.7726, -3.3408, -2.7496]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.9123, -3.2578, -2.5708],\n",
      "        [ 4.6121, -2.6989, -2.5201]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.9123, -3.2578, -2.5708],\n",
      "        [ 4.6121, -2.6989, -2.5201]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8862, -3.4182, -2.7966],\n",
      "        [ 4.3479, -2.7663, -2.8800]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8862, -3.4182, -2.7966],\n",
      "        [ 4.3479, -2.7663, -2.8800]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.7658, -3.3040, -2.5254],\n",
      "        [ 4.8802, -3.4134, -2.7739]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.7658, -3.3040, -2.5254],\n",
      "        [ 4.8802, -3.4134, -2.7739]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.8086, -3.6120, -2.5244],\n",
      "        [ 4.5968, -2.9872, -2.8849]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.8086, -3.6120, -2.5244],\n",
      "        [ 4.5968, -2.9872, -2.8849]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5536, -3.2553, -2.3437],\n",
      "        [ 4.6901, -2.8019, -2.9071]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 4.5536, -3.2553, -2.3437],\n",
      "        [ 4.6901, -2.8019, -2.9071]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4323, -3.1342, -1.2171],\n",
      "        [ 4.4938, -2.8732, -2.5991]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "logits: tensor([[ 3.4323, -3.1342, -1.2171],\n",
      "        [ 4.4938, -2.8732, -2.5991]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"bert-phase1-default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Load dataset ======\n",
    "df = pd.read_csv(\"../Student_Training_Data/GPT.csv\").tail(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_path = \"./bert-phase1-default\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"background\", 1: \"method\", 2: \"result\"}\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: method\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: result\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: method\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n",
      "Predicted classification: background\n"
     ]
    }
   ],
   "source": [
    "# Loop through each row and predict\n",
    "for _, row in df.iterrows():\n",
    "    prompt = (\n",
    "        \"Classify the following scientific text as one of [background, method, result].\\n\\n\"\n",
    "        f\"Text: {row['string']}\\n\"\n",
    "        \"Provide your classification.\"\n",
    "    )\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred_id = logits.argmax(dim=1).item()\n",
    "        classification = label_map[pred_id]\n",
    "\n",
    "    print(\"Predicted classification:\", classification)\n",
    "    # Save prediction\n",
    "    predictions.append(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions to bert_classification_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Add to DataFrame\n",
    "df[\"predicted_classification\"] = predictions\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"bert_classification_predictions.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved predictions to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
