# ğŸ‰ Citation Intent Classification with Model Distillation ğŸ‰

**âœ¨ Shrinking Giants & Making Citations Smarter! âœ¨**

Ever wondered if a tiny model could outsmart a giant LLM? **We proved it can!** ğŸš€ This project distills the power of massive language models (like GPT-4 and LLaMA3) into a lightweight **T5-base model** (223M params) to classify citation intentâ€”because research should be **fast, efficient, and fun**!

## ğŸ”¥ Why You'll Love This

- **âš¡ Lightning-Fast**: Ditch the 70B-parameter monstersâ€”our distilled model is small but mighty!
- **ğŸ¯ Accurate AF**: Hits ~0.7 F1 on the [SciCite dataset](https://github.com/allenai/scicite) (just like the big guys).
- **ğŸ¤¯ Surprise Finding**: Adding *reasoning* during distillation **didn't help** (plot twist! ğŸ¿).
- **ğŸ“¦ Plug-and-Play**: Easy-to-run code for training, testing, and playing with citation magic.

---
## ğŸ“Š What's Inside?

```plaintext
â”œâ”€â”€ Cleaned Work/     # Main project work directory
â”‚   â”œâ”€â”€ utils/                    # Utility functions and helpers, e.g. Charts, Data Augmentor, Model Cards, Post and Preprocessing
â”‚   â”œâ”€â”€ teacher rationale generation/  # Teacher model rationale generation
â”‚   â”œâ”€â”€ student training data/    # Data for student model training
â”‚   â”œâ”€â”€ results (student, teacher outputs)/  # Model outputs and results
â”‚   â”œâ”€â”€ phase1_v2/               # Phase 1 implementation
â”‚   â”œâ”€â”€ data/                    # Dataset and processed data
â”‚   â”œâ”€â”€ titan-job.slurm         # Slurm job script for Titan
â”‚   â”œâ”€â”€ a100-job.slurm          # Slurm job script for A100
â”‚   â””â”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ Archive/          # Archived files and previous work
â”œâ”€â”€ pyproject.toml    # Python project configuration
â”œâ”€â”€ poetry.lock       # Poetry dependency lock file
â”œâ”€â”€ .gitignore        # Git ignore rules
â””â”€â”€ README.md         # You're here! ğŸ‘‹
```

## ğŸ” Key Findings
âœ… **Off-the-shelf LLMs perform well**  
   - Achieved F1 ~0.7 on SciCite dataset  
   - Model size â‰  performance (e.g., LLaMA3 1B outperformed Mistral 7B)  

âœ… **Distilled model succeeds**  
   - T5-base (223M params) matched giant LLMs (70B+ params)  
   - Dramatically reduced compute requirements  

âŒ **Rationales didn't help** *(Surprise!)*  
   - Unlike [prior work](https://arxiv.org/abs/2305.02301), reasoning distillation:  
     - Didn't improve SciCite performance  
     - May not work for nuanced, domain-specific tasks  

## ğŸ‘ Credits
Team Superstars: Cheng Jin Hao, Chung Yunseong, Ethan Wei, Evelyn Lai, Kok Joon Eu Shawn, Nikhil Sultania
Mentor: Zhang Bowen

**ğŸ“š Key Reference**:  
Hsieh, C.-Y., Li, C.-L., Yeh, C.-K., et al. (2023). *"Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"*.  
[arXiv:2305.02301](https://arxiv.org/abs/2305.02301)