{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced SciCite Distillation with Cosine Similarity\n",
    "\n",
    "**Objective**: Fine-tune student model using teacher reasoning with cosine similarity alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets accelerate peft bitsandbytes sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.10.4\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "print(nbformat.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shawn Kok\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_with_context(partition_path: str) -> Dataset:\n",
    "    df = pd.read_csv(partition_path)\n",
    "    return Dataset.from_pandas(df[[\"sectionName\", \"string\", \"id\", \"model_classification\", \"reasoning\"]])\n",
    "\n",
    "train_dataset = load_dataset_with_context(\"./merged_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\"classification\": \"background\", \"reasoning\": \"The text establishes a gap in knowledge regarding the interaction between frataxin and the Fe-S cluster biosynthesis components. It highlights the lack of direct, one-to-one interactions, suggesting this is an area requiring further investigation.\"}\n",
      "```\n",
      "('background', 'The text establishes a gap in knowledge regarding the interaction between frataxin and the Fe-S cluster biosynthesis components. It highlights the lack of direct, one-to-one interactions, suggesting this is an area requiring further investigation.')\n"
     ]
    }
   ],
   "source": [
    "%run student_eval.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "hf_token = os.getenv('HUGGINGFACE_API_KEY')\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "The collate function converts raw data into model-ready tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    metadata = {\n",
    "        \"sectionName\": [],\n",
    "        \"string\": [],\n",
    "        \"teacher_reasoning\": []\n",
    "    }\n",
    "    \n",
    "    for example in examples:\n",
    "        prompt = f\"Classify this citation:\\nSection: {example['sectionName']}\\nText: {example['string']}\\nClassification:\"\n",
    "        tokenized = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        batch[\"input_ids\"].append(tokenized[\"input_ids\"])\n",
    "        batch[\"attention_mask\"].append(tokenized[\"attention_mask\"])\n",
    "        batch[\"labels\"].append(tokenized[\"input_ids\"].clone())\n",
    "        \n",
    "        metadata[\"sectionName\"].append(example[\"sectionName\"])\n",
    "        metadata[\"string\"].append(example[\"string\"])\n",
    "        metadata[\"teacher_reasoning\"].append(example[\"reasoning\"])\n",
    "    \n",
    "    batch[\"input_ids\"] = torch.cat(batch[\"input_ids\"], dim=0)\n",
    "    batch[\"attention_mask\"] = torch.cat(batch[\"attention_mask\"], dim=0)\n",
    "    batch[\"labels\"] = torch.cat(batch[\"labels\"], dim=0)\n",
    "    batch.update(metadata)\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSimilarityDistiller(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.student_pipe = None\n",
    "        self.step_counter = 0\n",
    "        \n",
    "    def setup_pipeline(self):\n",
    "        if not self.student_pipe:\n",
    "            self.student_pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                device=self.model.device\n",
    "            )\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Standard language modeling loss\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Cosine similarity alignment every 5 steps\n",
    "        if self.step_counter % 5 == 0:\n",
    "            self.setup_pipeline()\n",
    "            \n",
    "            # Generate student outputs\n",
    "            student_reasonings = []\n",
    "            for section, text in zip(inputs[\"sectionName\"], inputs[\"string\"]):\n",
    "                _, reasoning = call_pipe(self.student_pipe, section, text)\n",
    "                student_reasonings.append(reasoning)\n",
    "            \n",
    "            # Get embeddings\n",
    "            teacher_embeds = self.get_embeddings(inputs[\"teacher_reasoning\"])\n",
    "            student_embeds = self.get_embeddings(student_reasonings)\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            cos_sim = torch.nn.functional.cosine_similarity(teacher_embeds, student_embeds)\n",
    "            similarity_loss = 1 - cos_sim.mean()\n",
    "            \n",
    "            # Combine losses\n",
    "            total_loss = loss + (0.5 * similarity_loss)\n",
    "            \n",
    "            if self.step_counter % 10 == 0:\n",
    "                print(f\"Step {self.step_counter} - \"\n",
    "                      f\"LM Loss: {loss:.4f} | \"\n",
    "                      f\"Similarity Loss: {similarity_loss:.4f} | \"\n",
    "                      f\"Total Loss: {total_loss:.4f}\")\n",
    "        else:\n",
    "            total_loss = loss\n",
    "        \n",
    "        self.step_counter += 1\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "    \n",
    "    def get_embeddings(self, texts):\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "            return outputs.hidden_states[-1][:, 0, :]  # CLS token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"cosine-distilled\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CosineSimilarityDistiller(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
