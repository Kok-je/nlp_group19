{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference LLM Distillation notebook: https://github.com/simranjeet97/LLM_Distillation/blob/main/LLM_Distillation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "Install the latest version of `transformers` library. This is to ensure that it is compatible with the latest Hugging Face models and tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import neccessary package\n",
    "\n",
    "This section loads all essential Python packages for the training workflow. We also load the Hugging Face token stored in the `.env` file here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer & Model Setup\n",
    "Set up the tokenizer and model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer & Model Setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", token=hf_token, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Special Tokens\n",
    "Add task-specific tokens (`[label]`, `[rationale]`) to the tokenizer and resize the model's embedding layer to accommodate them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure tokenizer has special tokens:\n",
    "tokenizer.add_special_tokens({\n",
    "    'additional_special_tokens': ['[label]', '[rationale]']\n",
    "})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "Load training dataset (train dataset + teachers generated classification + teacher generated rationale) from a CSV file and convert it into a Hugging Face `Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_partition(path: str) -> Dataset:\n",
    "    df = pd.read_csv(path)\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "dataset = load_partition(\"../Student_Training_Data/The_King.csv\") \n",
    "print(f\"Loaded {len(dataset)} samples from dataset.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function: Add Special Tokens If Missing\n",
    "Check whether `[label]` and `[rationale]` tokens are already in the tokenizer vocab. If the tokens are missing, then it will add them into the tokenizer vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens_if_missing(tokenizer):\n",
    "    # Add task-specific tokens if not present\n",
    "    special_tokens = []\n",
    "    if \"[label]\" not in tokenizer.get_vocab():\n",
    "        special_tokens.append(\"[label]\")\n",
    "    if \"[rationale]\" not in tokenizer.get_vocab():\n",
    "        special_tokens.append(\"[rationale]\")\n",
    "    \n",
    "    if special_tokens:\n",
    "        tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
    "    return tokenizer\n",
    "\n",
    "# Update tokenizer with special tokens\n",
    "tokenizer = add_special_tokens_if_missing(tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset\n",
    "Tokenizes the input data for both label classification and rationale generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Create base text inputs\n",
    "    base_texts = [\n",
    "        f\"Section Name: {sn}\\nText: {txt}\" \n",
    "        for sn, txt in zip(examples[\"sectionName\"], examples[\"string\"])\n",
    "    ]\n",
    "\n",
    "    # Create task-specific inputs\n",
    "    label_inputs = [f\"[label] {text} \\nLabel (either background, method or result):\" for text in base_texts]\n",
    "    rationale_inputs = [f\"[rationale] {text} \\nRationale:\" for text in base_texts]\n",
    "\n",
    "    # Tokenize base inputs (for potential shared encoder)\n",
    "    base_encoded = tokenizer(\n",
    "        base_texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,  # Reserve space for prefixes\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Tokenize label task inputs and targets\n",
    "    label_encoded = tokenizer(\n",
    "        label_inputs,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize label targets (text labels, not indices)\n",
    "    label_targets = tokenizer(\n",
    "        examples[\"model_classification\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True, \n",
    "        max_length=32,  # Short length for class labels\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Tokenize rationale task inputs\n",
    "    rationale_encoded = tokenizer(\n",
    "        rationale_inputs,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Tokenize rationale targets\n",
    "    rationale_targets = tokenizer(\n",
    "        examples[\"reasoning\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        # Base inputs (shared between tasks)\n",
    "        \"base_input_ids\": base_encoded.input_ids,\n",
    "        \"base_attention_mask\": base_encoded.attention_mask,\n",
    "\n",
    "        # Label prediction task\n",
    "        \"label_input_ids\": label_encoded.input_ids,\n",
    "        \"label_attention_mask\": label_encoded.attention_mask,\n",
    "        \"label_target_ids\": label_targets.input_ids,\n",
    "\n",
    "        # Rationale generation task\n",
    "        \"rationale_input_ids\": rationale_encoded.input_ids,\n",
    "        \"rationale_attention_mask\": rationale_encoded.attention_mask,\n",
    "        \"rationale_target_ids\": rationale_targets.input_ids,\n",
    "    }\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    remove_columns=dataset.column_names  # Remove original columns\n",
    ")\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\n",
    "    \"base_input_ids\",\n",
    "    \"base_attention_mask\",\n",
    "    \"label_input_ids\",\n",
    "    \"label_attention_mask\", \n",
    "    \"label_target_ids\",\n",
    "    \"rationale_input_ids\",\n",
    "    \"rationale_attention_mask\",\n",
    "    \"rationale_target_ids\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainingArguments Configuration\n",
    "\n",
    "- `output_dir`: Directory where model checkpoints and training outputs will be saved\n",
    "- `logging_dir`: Directory where training logs will be stored\n",
    "- `fp16`: 16-bit floating-point precision training (less memory-efficient but more precise)\n",
    "- `bf16`: bfloat16 precision format, which provides better numerical stability than fp16 while still offering memory savings (requires compatible hardware)\n",
    "- `per_device_train_batch_size`: Number of training examples processed per device (GPU/TPU) in each forward pass\n",
    "- `per_device_eval_batch_size`: Number of evaluation examples processed per device in each forward pass\n",
    "- `gradient_accumulation_steps`: Accumulates gradients over 2 batches before performing a parameter update\n",
    "- `learning_rate`: Initial learning rate for the optimizer\n",
    "- `num_train_epochs`: Number of complete passes through the training dataset\n",
    "- `report_to`: Integration with tracking platforms\n",
    "- `save_strategy`: Automatic model checkpointing during training\n",
    "- `remove_unused_columns`: Preserves all columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    fp16=False,  \n",
    "    bf16=True,   # You can try enabling this if you have newer hardware\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"no\",\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define trained model name\n",
    "Sets the name used for saving or tracking the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trained_model_name = \"distilled_t5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Multi-Task Trainer: `MultiTaskTrainer`\n",
    "\n",
    "This section defines a custom subclass of Hugging Face's `Trainer` to handle **multi-task learning** with both label classification and rationale generation.\n",
    "\n",
    "---\n",
    "\n",
    "#### MultiTaskTrainer Class:\n",
    "\n",
    "- Inherits from `transformers.Trainer` and overrides the below functions.\n",
    "  \n",
    "#### 1. `compute_loss(...)`\n",
    "\n",
    "This method overrides the base `Trainer`’s loss computation to enable multi-task training. This method calculates the combined loss for both the label prediction and rationale generation tasks.\n",
    "\n",
    "##### `Process`\n",
    "- `Configuration`: Sets alpha = 0.3 as the weighting hyperparameter (λ) to balance between label and rationale tasks\n",
    "  Initializes the CrossEntropyLoss function, ignoring padding tokens (-100)\n",
    "\n",
    "- `Label Task Processing`: Shifts the target ids right to create decoder inputs for the label task\n",
    "  Passes label inputs through the model to generate label predictions\n",
    "  Calculates cross-entropy loss between predicted and target label distributions\n",
    "\n",
    "\n",
    "- `Rationale Task Processing`: Passes rationale inputs through the model\n",
    "Retrieves the rationale loss directly from model outputs\n",
    "\n",
    "- `Final Weighted Loss`: The two losses are combined using a task-balancing hyperparameter $ \\alpha $.\n",
    "\n",
    "    $$\n",
    "    \\text{total\\_loss} = (1 - \\alpha) \\cdot \\text{label\\_loss} + \\alpha \\cdot \\text{rationale\\_loss}\n",
    "    $$\n",
    "\n",
    "#### `Return`\n",
    "\n",
    "- If return_outputs is `True`: Returns a tuple containing the total loss and model outputs <br>\n",
    "- If return_outputs is `False`: Returns only the total loss\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. `prediction_step(...)`\n",
    "\n",
    "This method handles model predictions during evaluation and testing phases.\n",
    "\n",
    "##### `Process`\n",
    "\n",
    "- `Input Preparation`: Separates inputs into label-specific and rationale-specific dictionaries\n",
    "\n",
    "\n",
    "- `Individual Predictions`: Uses the parent Trainer.prediction_step method to get predictions for each task\n",
    "Extracts loss, logits, and labels from both prediction outputs\n",
    "\n",
    "\n",
    "- `Combined Loss Calculation`: Applies the weighting formula\n",
    "\n",
    "    $$\n",
    "    \\text{total\\_loss} = \\alpha \\cdot \\text{label\\_loss} + (1 - \\alpha) \\cdot \\text{rationale\\_loss}\n",
    "    $$\n",
    "  Note: The weighting formula here appears to be reversed from the one in compute_loss\n",
    "\n",
    "#### `Return`\n",
    "\n",
    "- If prediction_loss_only is `True`: Returns only the combined loss <br>\n",
    "- If prediction_loss_only is `False`: Returns a tuple containing: The combined loss, a list containing logits from both tasks, a list containing labels from both tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, **kwargs):\n",
    "        alpha = 0.3  # λ hyperparameter from the paper\n",
    "        ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "        # Label Task ----------------------------------------------------------\n",
    "        # Create decoder inputs by shifting labels\n",
    "        label_decoder_input_ids = model._shift_right(inputs[\"label_target_ids\"])\n",
    "        \n",
    "        # Process Label Task --------------------------------------------------\n",
    "        label_outputs = model(\n",
    "            input_ids=inputs[\"label_input_ids\"],\n",
    "            attention_mask=inputs[\"label_attention_mask\"],\n",
    "            decoder_input_ids=label_decoder_input_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        # Calculate loss for label prediction\n",
    "        label_loss = ce_loss(\n",
    "            label_outputs.logits.view(-1, model.config.vocab_size),\n",
    "            inputs[\"label_target_ids\"].view(-1)\n",
    "        )\n",
    "\n",
    "        # Process Rationale Task ----------------------------------------------\n",
    "        rationale_outputs = model(\n",
    "            input_ids=inputs[\"rationale_input_ids\"],\n",
    "            attention_mask=inputs[\"rationale_attention_mask\"],\n",
    "            labels=inputs[\"rationale_target_ids\"]\n",
    "        )\n",
    "        rationale_loss = rationale_outputs.loss\n",
    "\n",
    "        # Combine Losses ------------------------------------------------------\n",
    "        total_loss = (1 - alpha) * label_loss + alpha * rationale_loss\n",
    "\n",
    "        return (total_loss, (label_outputs, rationale_outputs)) if return_outputs else total_loss\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        alpha = 0.3\n",
    "\n",
    "        label_inputs = {\n",
    "            \"input_ids\": inputs[\"label_input_ids\"],\n",
    "            \"attention_mask\": inputs[\"label_attention_mask\"],\n",
    "            \"labels\": inputs[\"label_target_ids\"]\n",
    "        }\n",
    "\n",
    "        rationale_inputs = {\n",
    "            \"input_ids\": inputs[\"rationale_input_ids\"],\n",
    "            \"attention_mask\": inputs[\"rationale_attention_mask\"],\n",
    "            \"labels\": inputs[\"rationale_target_ids\"]\n",
    "        }\n",
    "\n",
    "        # super calls the parent class Trainer's prediction_step method \n",
    "        label_outputs = super().prediction_step(model, label_inputs, prediction_loss_only=False, ignore_keys=ignore_keys)\n",
    "        rationale_outputs = super().prediction_step(model, rationale_inputs, prediction_loss_only=False, ignore_keys=ignore_keys)\n",
    "        # this will reutrn (loss, logits, labels) which we can unpack\n",
    "        label_loss, label_logits, label_labels = label_outputs\n",
    "        rationale_loss, rationale_logits, rationale_labels = rationale_outputs\n",
    "        # combine the losses now\n",
    "        loss = (alpha * label_loss) + (1 - alpha) * rationale_loss\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        return (\n",
    "            loss,\n",
    "            [label_logits, rationale_logits],\n",
    "            [label_labels, rationale_labels]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Initialization:\n",
    "\n",
    "This code initializes and executes the training process for a multi-task citation intent classification model using the custom MultiTaskTrainer class.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- `model`: The pre-trained transformer model being fine-tuned\n",
    "- `args`: Training arguments defining hyperparameters and settings\n",
    "- `train_dataset`: The tokenized dataset used for training\n",
    "- `eval_dataset`: The tokenized dataset used for evaluation (same as training dataset in this case)\n",
    "- `data_collator`: A function that collates individual data points into batches. It extracts and stacks tensors for both tasks (label and rationale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = MultiTaskTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    data_collator=lambda data: {\n",
    "        \"label_input_ids\": torch.stack([d[\"label_input_ids\"] for d in data]),\n",
    "        \"label_attention_mask\": torch.stack([d[\"label_attention_mask\"] for d in data]),\n",
    "        \"label_target_ids\": torch.stack([d[\"label_target_ids\"] for d in data]),\n",
    "        \"rationale_input_ids\": torch.stack([d[\"rationale_input_ids\"] for d in data]),\n",
    "        \"rationale_attention_mask\": torch.stack([d[\"rationale_attention_mask\"] for d in data]),\n",
    "        \"rationale_target_ids\": torch.stack([d[\"rationale_target_ids\"] for d in data])\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Execution\n",
    "This part of code initiates the training process using the configured trainer. After training the model, it saves the tokenizer, trained model weights and configuration to the specified directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(f\"./{new_trained_model_name}\")\n",
    "tokenizer.save_pretrained(f\"./{new_trained_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "This part of the code loads the trained model for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Configuration\n",
    "model_path = f\"./{new_trained_model_name}\" ## TODO: Note that this model is trained only on a 1000 samples! Because the paper says 25% of full training ata was alr good enough, so i wanted to just test with a smaller number of samples first.\n",
    "test_data_path = \"../data/test.jsonl\"\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load the distilled model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# After loading tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.float32).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "def load_test_data(file_path):\n",
    "    \"\"\"Load and parse test data\"\"\"\n",
    "    test_data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            test_data.append({\n",
    "                \"section\": entry[\"sectionName\"],\n",
    "                \"text\": entry[\"string\"],\n",
    "                \"true_label\": entry[\"label\"]\n",
    "            })\n",
    "    return test_data\n",
    "\n",
    "def preprocess_input(section, text):\n",
    "    \"\"\"Format input with task prefix\"\"\"\n",
    "    input_text = f\"[label] Section: {section}\\nText: {text} \\nLabel (either background, method or result):\" ## TODO: NOTE THAT THIS IS KEYyyyy\n",
    "    return tokenizer(\n",
    "        input_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "def predict_label(model, inputs):\n",
    "    \"\"\"Generate label prediction\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=10,\n",
    "            # For deterministic results (default):\n",
    "            do_sample=False,  # Disables sampling\n",
    "            num_beams=3,     # Beam search works better for Seq2Seq\n",
    "            early_stopping=False,\n",
    "            # Remove temperature parameter when do_sample=False\n",
    "            decoder_start_token_id=tokenizer.pad_token_id, #critical for T5\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            # forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"method\"),\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Debug raw outputs\n",
    "    print(\"Raw output IDs:\", outputs[0])\n",
    "    # print(\"Decoded output:\", tokenizer.decode(outputs[0]))\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def clean_prediction(raw_prediction):\n",
    "    \"\"\"Extract label from model output\"\"\"\n",
    "    # Split on \"Label:\" and take the first word after it\n",
    "    print(f\"Raw: {raw_prediction}\")\n",
    "    # parts = raw_prediction.split(\"Label:\")\n",
    "    if len(raw_prediction) > 1:\n",
    "        prediction = raw_prediction.strip().split()[0].lower()\n",
    "        # Map to valid labels\n",
    "        valid_labels = {\"background\", \"method\", \"result\"}\n",
    "        print(f\"Prediction: {prediction}\")\n",
    "        return prediction if prediction in valid_labels else \"unknown\"\n",
    "    return \"unknown\"\n",
    "\n",
    "# Load test data\n",
    "test_data = load_test_data(test_data_path)\n",
    "\n",
    "# Run predictions\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for example in test_data:\n",
    "    # Preprocess input\n",
    "    inputs = preprocess_input(example[\"section\"], example[\"text\"])\n",
    "    \n",
    "    # Get prediction\n",
    "    raw_pred = predict_label(model, inputs)\n",
    "    cleaned_label = clean_prediction(raw_pred)\n",
    "    \n",
    "    # Store results\n",
    "    true_labels.append(example[\"true_label\"])\n",
    "    pred_labels.append(cleaned_label)\n",
    "    \n",
    "    # Print example (optional)\n",
    "    print(f\"Section: {example['section']}\")\n",
    "    print(f\"Text: {example['text'][:100]}...\")\n",
    "    print(f\"True: {example['true_label']} | Pred: {cleaned_label}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save results\n",
    "with open(\"predictions_t5_trained.csv\", \"w\") as f:\n",
    "    f.write(\"true_label,predicted_label\\n\")\n",
    "    for true, pred in zip(true_labels, pred_labels):\n",
    "        f.write(f\"{true},{pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_and_save(model, tokenizer, test_data, model_name=\"Model\", save_path=None):\n",
    "    \"\"\"Evaluate model and save predictions to CSV\"\"\"\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    raw_preds = []\n",
    "    sections = []\n",
    "    texts = []\n",
    "    \n",
    "    for example in tqdm(test_data, desc=f\"Evaluating {model_name}\"):\n",
    "        inputs = preprocess_input(example[\"section\"], example[\"text\"])\n",
    "        raw_pred = predict_label(model, inputs)\n",
    "        cleaned_label = clean_prediction(raw_pred)\n",
    "        \n",
    "        # Collect data for CSV\n",
    "        sections.append(example[\"section\"])\n",
    "        texts.append(example[\"text\"])\n",
    "        true_labels.append(example[\"true_label\"])\n",
    "        pred_labels.append(cleaned_label)\n",
    "        raw_preds.append(raw_pred)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        \"section\": sections,\n",
    "        \"text\": texts,\n",
    "        \"true_label\": true_labels,\n",
    "        \"predicted_label\": pred_labels,\n",
    "        \"raw_prediction\": raw_preds\n",
    "    })\n",
    "    \n",
    "    # Save to CSV if path specified\n",
    "    if save_path:\n",
    "        results_df.to_csv(save_path, index=False)\n",
    "        print(f\"Saved predictions to {save_path}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    class_report = classification_report(true_labels, pred_labels, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_background\": class_report[\"background\"][\"precision\"],\n",
    "        \"recall_background\": class_report[\"background\"][\"recall\"],\n",
    "        \"precision_method\": class_report[\"method\"][\"precision\"],\n",
    "        \"recall_method\": class_report[\"method\"][\"recall\"],\n",
    "        \"precision_result\": class_report[\"result\"][\"precision\"],\n",
    "        \"recall_result\": class_report[\"result\"][\"recall\"],\n",
    "    }\n",
    "#%% [markdown]\n",
    "#### 1. Load Base Model (Pre-trained)\n",
    "#%%\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\").to(device)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"t5-base\") \n",
    "\n",
    "# Add special tokens if missing\n",
    "special_tokens = [\"[label]\", \"[rationale]\"]\n",
    "base_tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
    "base_model.resize_token_embeddings(len(base_tokenizer))\n",
    "\n",
    "#%% [markdown]\n",
    "#### 2. Load Distilled Model (Fine-tuned)\n",
    "#%%\n",
    "distilled_model = AutoModelForSeq2SeqLM.from_pretrained(f\"./{new_trained_model_name}\").to(device)\n",
    "distilled_tokenizer = AutoTokenizer.from_pretrained(f\"./{new_trained_model_name}\")\n",
    "\n",
    "#%% [markdown]\n",
    "#### 3. Evaluate Both Models\n",
    "#%%\n",
    "test_data = load_test_data(test_data_path)[:5]  # Use subset for faster evaluation\n",
    "\n",
    "# Evaluate base model and save\n",
    "base_results = evaluate_and_save(\n",
    "    base_model, \n",
    "    base_tokenizer,\n",
    "    test_data,\n",
    "    model_name=\"Base Model\",\n",
    "    save_path=\"base_model_predictions.csv\"\n",
    ")\n",
    "\n",
    "# Evaluate distilled model and save\n",
    "distilled_results = evaluate_and_save(\n",
    "    distilled_model,\n",
    "    distilled_tokenizer,\n",
    "    test_data,\n",
    "    model_name=\"Distilled Model\",\n",
    "    save_path=\"distilled_model_predictions.csv\"\n",
    ")\n",
    "\n",
    "#%% [markdown]\n",
    "#### 4. Display Comparison\n",
    "#%%\n",
    "results_df = pd.DataFrame([base_results, distilled_results])\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "display(results_df.style\n",
    "       .format(\"{:.2%}\", subset=[\"accuracy\", \"precision_background\", \"recall_background\", \n",
    "                                \"precision_method\", \"recall_method\", \n",
    "                                \"precision_result\", \"recall_result\"])\n",
    "       .background_gradient(cmap=\"Blues\", subset=[\"accuracy\"]))\n",
    "\n",
    "#%% [markdown]\n",
    "#### 5. Sample Predictions Comparison\n",
    "#%%\n",
    "print(\"\\nSample Prediction Comparison:\")\n",
    "sample_data = test_data[:3]  # First 3 examples\n",
    "\n",
    "for example in sample_data:\n",
    "    # Base model prediction\n",
    "    inputs = preprocess_input(example[\"section\"], example[\"text\"])\n",
    "    base_pred = clean_prediction(predict_label(base_model, inputs))\n",
    "    \n",
    "    # Distilled model prediction\n",
    "    inputs = preprocess_input(example[\"section\"], example[\"text\"])\n",
    "    distilled_pred = clean_prediction(predict_label(distilled_model, inputs))\n",
    "    \n",
    "    print(f\"\\nSection: {example['section']}\")\n",
    "    print(f\"Text: {example['text'][:100]}...\")\n",
    "    print(f\"True Label: {example['true_label']}\")\n",
    "    print(f\"Base Model: {base_pred} | Distilled Model: {distilled_pred}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
