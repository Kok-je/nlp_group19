{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = \"./gemma3-phase1-best\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(trained_model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Load dataset ======\n",
    "def load_partition(path: str) -> Dataset:\n",
    "    df = pd.read_csv(path).head(10)\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "dataset = load_partition(\"./Student_Training_Data/GPT.csv\")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} samples from dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for example in dataset:\n",
    "    prompt = (\n",
    "        \"Classify the following scientific text as one of [background, method, result].\\n\\n\"\n",
    "        f\"Text: {example['string']}\\n\"\n",
    "        \"Provide your classification.\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            num_beams=1\n",
    "        )\n",
    "\n",
    "    generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    classification = \"parse_error\"\n",
    "    for label in [\"background\", \"method\", \"result\"]:\n",
    "        if label in generated.lower():\n",
    "            classification = label\n",
    "            break\n",
    "\n",
    "    predictions.append({\n",
    "        \"predicted_classification\": classification,\n",
    "        \"predicted_reasoning\": \"None\"\n",
    "    })\n",
    "    print(f\"Predicted: {classification} Generated: {generated} \")\n",
    "\n",
    "# ====== Save to CSV ======\n",
    "df = dataset.to_pandas()\n",
    "df[\"predicted_classification\"] = [p[\"predicted_classification\"] for p in predictions]\n",
    "df[\"predicted_reasoning\"] = [p[\"predicted_reasoning\"] for p in predictions]\n",
    "df.to_csv(\"gemma3_classification_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference step here\n",
    "def generate_output(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    text_input = row[\"string\"]\n",
    "    predicted_label = generate_output(text_input)\n",
    "    predictions.append(predicted_label)\n",
    "    print(\"classification: \", predicted_label)\n",
    "\n",
    "# Save to DataFrame\n",
    "dataset[\"predicted_classification\"] = predictions\n",
    "dataset.to_csv(\"gemma3_classification_results.csv\", index=False)\n",
    "print(\"Inference for first 10 data points completed.\")\n",
    "# Save results\n",
    "# dataset = dataset.add_column(\"generated_reasoning\", predictions)\n",
    "# output_csv_path = \"./llama-student-phase2.csv\"\n",
    "# dataset.to_pandas().to_csv(output_csv_path, index=False)\n",
    "\n",
    "# print(f\"Inference completed. Results saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, prediction in enumerate(predictions):\n",
    "    print(f\"Index {i}: {prediction} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
