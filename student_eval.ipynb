{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-1B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-1B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "model_id = \"google/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"'}]}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B-Instruct\", max_new_tokens=2048)\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_pipe(pipe, section_name, content):\n",
    "    messages = [{\"role\": \"system\", \"content\": \n",
    "                 \"You are an AI assistant who will assess whether a given text is used as a background, result or method section in a scientific paper.\"\n",
    "                 \"You will be given a section name and the text, and you will need to classify the text as one of [background, result, method].\"\n",
    "                 \"You will also need to provide a reasoning for your classification.\"\n",
    "                 \"The output should strictly be a json with the following two keys: classification, reasoning.\"\n",
    "                 \"Example output would look like: {\\\"classification\\\": \\\"result\\\", \\\"reasoning\\\": \\\"This is the reasoning\\\"}\"\n",
    "    }, \n",
    "    {\"role\": \"user\", \"content\": f\"section name: {section_name}, text: {content}\"}]\n",
    "    response = pipe(messages)[0]\n",
    "    try:\n",
    "        if response[\"generated_text\"][2][\"content\"]:\n",
    "            print(response[\"generated_text\"][2][\"content\"])\n",
    "            content = response[\"generated_text\"][2][\"content\"]\n",
    "            start = content.find(\"{\")\n",
    "            end = content.rfind(\"}\") + 1\n",
    "            json_str = content[start:end]\n",
    "            answers = json.loads(json_str)\n",
    "            classification = answers[\"classification\"]\n",
    "            reasoning = answers[\"reasoning\"]\n",
    "            return classification, reasoning\n",
    "        else:\n",
    "            return \"Invalid\", \"No reasoning provided\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"Invalid\", \"No reasoning provided\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"classification\": \"result\", \"reasoning\": \"This is the reasoning. The given text mentions specific components (IscS, IscU/Isu1, ISD11/Isd11) that are known to interact with the Fe-S cluster biosynthesis, but does not provide a direct description of how frataxin interacts with these components. The text suggests that the interaction is complex and may involve multiple components, but does not provide any specific details about the mechanism or outcome of this interaction.\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = call_pipe(pipe, \"Introduction\", \"However, how frataxin interacts with the Fe-S cluster biosynthesis components remains unclear as direct one-to-one interactions with each component were reported (IscS [12,22], IscU/Isu1 [6,11,16] or ISD11/Isd11 [14,15]).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"classification\": \"method\", \"reasoning\": \"This text is describing a specific experimental or analytical\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(file_path):\n",
    "    data = pd.read_json(path_or_buf=file_path, lines=True)\n",
    "    ids = set()\n",
    "    rows_to_be_dropped = []\n",
    "    for i in range(len(data)):\n",
    "        row = data.iloc[i]\n",
    "        if row.unique_id in ids:\n",
    "            rows_to_be_dropped.append(i)\n",
    "        else:\n",
    "            ids.add(row.unique_id)\n",
    "    data = data.drop(rows_to_be_dropped)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess(\"./data/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "reasonings = []\n",
    "raw_output = []\n",
    "ids = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    current_data = data.iloc[i]\n",
    "    ids.append(current_data.unique_id)\n",
    "    classification, reasoning = call_pipe(pipe, current_data.sectionName, current_data.string)\n",
    "    raw_output.append(classification)\n",
    "    labels.append(classification)\n",
    "    reasonings.append(reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(zip(ids, labels, reasonings), columns=[\"id\", \"model_classification\", \"reasoning\"])\n",
    "df.to_csv(\"llama-1b.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
